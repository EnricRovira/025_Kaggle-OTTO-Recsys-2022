{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 14:20:10.631969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-03 14:20:10.691378: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-03 14:20:10.706908: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-03 14:20:11.026328: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-12-03 14:20:11.026353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-12-03 14:20:11.026356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 14:20:11.434227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:20:11.448170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:20:11.448254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "from models import build_model_bert4Rec\n",
    "from dataloader import Bert4RecDataLoader, SASRecDataLoader\n",
    "\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1855603/1855603 [00:00<00:00, 5388688.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_ITEMS: 1311743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_mapping = pd.read_csv('../tfrecords/tfrecords_v0.7/df_mapping.csv')\n",
    "# NUM_ITEMS = 1_855_603+1\n",
    "NUM_ITEMS = len(df_mapping['aid_map'].unique())\n",
    "\n",
    "dict_map, dict_map_inv = {}, {}\n",
    "for x in tqdm(df_mapping.to_dict('records')):\n",
    "    dict_map[x['aid_map']] = x['aid']\n",
    "    dict_map_inv[x['aid']] = x['aid_map']\n",
    "\n",
    "dict_map_type = {\n",
    "    'clicks' : 1,\n",
    "    'carts' : 2,\n",
    "    'orders' : 3\n",
    "}\n",
    "\n",
    "dict_map_type_inv = {\n",
    "    1 : 'clicks',\n",
    "    2 : 'carts',\n",
    "    3 : 'orders'\n",
    "  }\n",
    "\n",
    "print(f'NUM_ITEMS: {NUM_ITEMS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 14:20:13.148567: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-03 14:20:13.149281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:20:13.149364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:20:13.149404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:20:13.420863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:20:13.420945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:20:13.420991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:20:13.421037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21437 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "10092it [01:02, 162.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5742"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_paths = ['../tfrecords/tfrecords_v0.7/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.7/na_split=val')]\n",
    "# 5,45, 1,09\n",
    "dataloader = Bert4RecDataLoader(list_paths, \n",
    "                                num_items=NUM_ITEMS, \n",
    "                                seq_len=80, \n",
    "                                seq_len_target=None,\n",
    "                                batch_size=32, \n",
    "                                mask_prob=0.0, \n",
    "                                reverse_prob=0.0, \n",
    "                                get_session=True,\n",
    "                                is_val=True,\n",
    "                                is_test=False,\n",
    "                                shuffle=False).get_generator()\n",
    "# Val\n",
    "list_sessions_val, list_items_val, list_types_val = [], [], []\n",
    "for batch in tqdm(dataloader):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    target, type_target, idx_mask = targets\n",
    "    idxs = idx_mask\n",
    "    list_sessions_val = list_sessions_val + session.numpy().tolist()\n",
    "    list_items_val += [[dict_map[item]-1 for item in seq[:idxs[i]]] for i, seq in enumerate(seq_items[:, :, 0].numpy())]\n",
    "    list_types_val += [[type_ for type_ in seq[:idxs[i]]] for i, seq in enumerate(seq_type[:, :, 0].numpy())]\n",
    "    # list_items_val += [[item-1 for item in seq[:idxs[i]] if item!=0] for i, seq in enumerate(seq_items[:, :, 0].numpy())]\n",
    "    # list_types_val += [[type_ for type_ in seq[:idxs[i]] if type_!=0] for i, seq in enumerate(seq_type[:, :, 0].numpy())]\n",
    "    # break\n",
    "print(len(list_sessions_val))\n",
    "# 10092 [01:02, 160.29it/s]\n",
    "\n",
    "del dataloader, batch, features, targets, session\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Model based predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 14:21:16.021810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "class BERT4REC_CONFIG:\n",
    "    seed = 42 \n",
    "    num_items = NUM_ITEMS\n",
    "    model_arch = 'bert4rec'\n",
    "    path_tfrecords = '../tfrecords/tfrecords_v0.7/'\n",
    "    restore_last_chekpoint = (True, 'model_sasrec_complete_0.14/checkpoints/', 'ckpt-25')\n",
    "    model_name = 'model_bert4rec_complete_0.10'\n",
    "    checkpoint_filepath = f'../2_Models/'\n",
    "    num_records_dataset = 10_000_000\n",
    "    batch_size = 32\n",
    "    tup_scheduler_grad_accum = (5, 10, 1_500_000) #(start_grad_accum, max_grad_accum, ramp_up_samples)\n",
    "    seq_len = 20\n",
    "    mask_prob = 0.4\n",
    "    reverse_prob = 0.5\n",
    "    emb_dim = 128\n",
    "    trf_dim = 128\n",
    "    num_heads = 4\n",
    "    num_layers = 1\n",
    "    ff_dim = trf_dim*4\n",
    "    drop_rate = 0.1\n",
    "    att_drop_rate = 0.1\n",
    "    epochs = 3\n",
    "    early_stopping = 5\n",
    "    batch_num_printer_train = 500\n",
    "    batch_num_printer_val = 250\n",
    "    clipnorm = 1.0\n",
    "    num_iters_save_checkpoint = 25_000\n",
    "    scheduler_scaler = 128 \n",
    "    warmup_steps = 10_000\n",
    "    weight_decay = 1e-1\n",
    "    log_wandb = True\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model_bert4Rec(num_items=NUM_ITEMS, model_cfg=BERT4REC_CONFIG)\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.14/checkpoints'))\n",
    "\n",
    "def get_score_metric(y_true, y_pred, type_target, k=20):\n",
    "    y_pred = list(set(y_pred))[:k]\n",
    "    y_true = list(set(y_true))[:k]\n",
    "    score = 0 \n",
    "    if len(y_true)==0:\n",
    "        return None\n",
    "    if type_target=='clicks':\n",
    "        num_targets = 1\n",
    "        hits = len([x for x in y_pred if x==y_true[0]])\n",
    "    else:\n",
    "        num_targets = min(k, len(y_true))\n",
    "        hits = len([x for x in y_pred if x in y_true])\n",
    "    score = hits / num_targets\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m num_batch\u001b[39m==\u001b[39m\u001b[39m500\u001b[39m:\n\u001b[1;32m     44\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m df_val_model \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\n\u001b[1;32m     47\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39msession\u001b[39;49m\u001b[39m'\u001b[39;49m : np\u001b[39m.\u001b[39;49mconcatenate(list_sessions),\n\u001b[1;32m     48\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mpredictions\u001b[39;49m\u001b[39m'\u001b[39;49m : list_predictions,\u001b[39m#np.concatenate(list_predictions).tolist(),\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mtrues\u001b[39;49m\u001b[39m'\u001b[39;49m : list_trues,\n\u001b[1;32m     50\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m'\u001b[39;49m : np\u001b[39m.\u001b[39;49mconcatenate(list_types)\n\u001b[1;32m     51\u001b[0m })\n\u001b[1;32m     52\u001b[0m \u001b[39m# df_val_model['past_items'] = df_val_model['past_items'].apply(lambda seq : [dict_map[x]-1 for x in seq if x!=0])\u001b[39;00m\n\u001b[1;32m     53\u001b[0m df_val_model[\u001b[39m'\u001b[39m\u001b[39mqt_trues\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_val_model[\u001b[39m'\u001b[39m\u001b[39mtrues\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x : \u001b[39mlen\u001b[39m(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/pandas/core/frame.py:662\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    656\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    657\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    660\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    661\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 662\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    663\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    664\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/pandas/core/internals/construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    665\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    668\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    669\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    671\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "list_paths_val = ['../tfrecords/tfrecords_v0.7/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.7/na_split=val')]\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20, \n",
    "                                     seq_len_target=50, \n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0, \n",
    "                                     is_val=True,\n",
    "                                     get_session=True, \n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "num_candidates = 20\n",
    "list_sessions, list_past_items, list_predictions, list_trues, list_types = [], [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(val_dataloader)):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    target, type_target, idx_mask = targets\n",
    "    idxs = idx_mask.numpy()\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=num_candidates)\n",
    "        topk_idxs = np.asarray([[dict_map[x]-1 for x in topk_idxs.numpy()[i, :-1]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        last_item = np.asarray([[dict_map[x[idx-1, 0]]-1] for x, idx in zip(seq_items.numpy(), idxs)])\n",
    "        new_predictions = np.concatenate([last_item, topk_idxs], axis=1, dtype=np.int64)\n",
    "        labels = [list(set([dict_map[_target]-1 for _type, _target in zip(type_target.numpy()[i], target.numpy()[i]) if dict_map_type[type_]==_type and _target!=0])) for i in range(target.shape[0])]\n",
    "        # labels = [list(set([_target-1 for _type, _target in zip(type_target.numpy()[i], target.numpy()[i]) if dict_map_type[type_]==_type and _target!=0])) for i in range(target.shape[0])]\n",
    "        ###\n",
    "        list_sessions.append(session.numpy())\n",
    "        list_predictions.append(new_predictions.tolist())\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_trues += labels\n",
    "        list_past_items.append(seq_items.numpy()[:, :, 0])\n",
    "    break\n",
    "    if num_batch==500:\n",
    "        break\n",
    "\n",
    "df_val_model = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : list_predictions,#np.concatenate(list_predictions).tolist(),\n",
    "    'trues' : list_trues,\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "# df_val_model['past_items'] = df_val_model['past_items'].apply(lambda seq : [dict_map[x]-1 for x in seq if x!=0])\n",
    "df_val_model['qt_trues'] = df_val_model['trues'].apply(lambda x : len(x))\n",
    "\n",
    "del preds, features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1302210, 1288292,   77368, 1139298,   11486,  323556, 1056925,\n",
       "        835283,  465180, 1275985, 1744774, 1502737, 1020276, 1628816,\n",
       "        466357,  755675,  442721, 1794981, 1031602])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1302210,\n",
       " 1302210,\n",
       " 1288292,\n",
       " 77368,\n",
       " 1139298,\n",
       " 11486,\n",
       " 323556,\n",
       " 1056925,\n",
       " 835283,\n",
       " 465180,\n",
       " 1275985,\n",
       " 1744774,\n",
       " 1502737,\n",
       " 1020276,\n",
       " 1628816,\n",
       " 466357,\n",
       " 755675,\n",
       " 442721,\n",
       " 1794981,\n",
       " 1031602]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_predictions.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1302210,\n",
       " 1302210,\n",
       " 1288292,\n",
       " 1139298,\n",
       " 77368,\n",
       " 465180,\n",
       " 11486,\n",
       " 1502737,\n",
       " 1791789,\n",
       " 1056925,\n",
       " 804249,\n",
       " 1275985,\n",
       " 1628816,\n",
       " 323556,\n",
       " 1266071,\n",
       " 358268,\n",
       " 755675,\n",
       " 1031602,\n",
       " 442721,\n",
       " 709475]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_predictions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rule based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_embedding = model.layers[1].weights[0].numpy()\n",
    "index = AnnoyIndex(arr_embedding.shape[1], 'euclidean')\n",
    "\n",
    "for idx, aid in dict_map.items():\n",
    "    index.add_item(idx, arr_embedding[idx])\n",
    "\n",
    "# for idx in tqdm(range(1, NUM_ITEMS)):\n",
    "#     aid = idx - 1\n",
    "#     index.add_item(aid, arr_embedding[idx])\n",
    "    \n",
    "index.build(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322934/322934 [00:17<00:00, 18005.71it/s]\n"
     ]
    }
   ],
   "source": [
    "session_types = ['clicks', 'carts', 'orders']\n",
    "\n",
    "# val_session_items = df_val.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n",
    "# val_session_types = df_val.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n",
    "# val_sessions = val_session_items.index\n",
    "\n",
    "num_items_to_consider = 20\n",
    "num_candidates = 20\n",
    "type_weight_multipliers = {1: 1, 2: 6, 3: 3}\n",
    "list_labels, list_sessions, list_len_past_items = [], [], []\n",
    "for session, seq_items, seq_types in tqdm(zip(list_sessions_val, list_items_val, list_types_val), total=len(list_items_val)):\n",
    "    if len(seq_items) >= num_items_to_consider:\n",
    "        # if we have enough aids (over equals num_items_to_consider) we don't need to look for candidates! we just use the old logic\n",
    "        weights=np.logspace(0.1, 1, len(seq_items), base=2, endpoint=True)-1\n",
    "        aids_temp=defaultdict(lambda: 0)\n",
    "        for aid,w,t in zip(seq_items, weights, seq_types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "            \n",
    "        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n",
    "        list_labels.append(sorted_aids[:num_candidates])\n",
    "    else:\n",
    "        if len(seq_items)==0:\n",
    "            seq_items = [1]\n",
    "        # list_labels.append([0])\n",
    "        # here we don't have num_items_to_consider aids to output -- we will use embeddings to generate candidates!\n",
    "        seq_items_new = list(dict.fromkeys(seq_items[::-1]))\n",
    "        \n",
    "        # let's grab the most recent aid\n",
    "        most_recent_aid = seq_items_new[0]\n",
    "        \n",
    "        # and look for some neighbors!\n",
    "        # nns = [i-1 for i in index.get_nns_by_item(most_recent_aid, num_candidates+1, search_k=-1)[1:]]\n",
    "        nns = [dict_map[i]-1 for i in index.get_nns_by_item(dict_map_inv[most_recent_aid+1], num_candidates+1, search_k=-1)[1:]]\n",
    "                        \n",
    "        list_labels.append((seq_items_new+nns)[:num_candidates])\n",
    "\n",
    "    list_sessions.append(session)\n",
    "    list_len_past_items.append(len(seq_items))\n",
    "\n",
    "# del list_items_val, list_types_val\n",
    "# gc.collect()\n",
    "\n",
    "df_val_rule = pd.DataFrame({\n",
    "    'session' : list_sessions,\n",
    "    'past_items' : list_items_val,\n",
    "    'prediction' : list_labels,\n",
    "    'qt_past_items' : list_len_past_items\n",
    "})\n",
    "\n",
    "# 100%|██████████| 647081/647081 [00:22<00:00, 28792.13it/s]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def emsemble_predictions(qt_past_items, list_past_items, type, list_pred_model, list_pred_embs):\n",
    "    tmp_preds = Counter()\n",
    "    tmp_past_items = Counter()\n",
    "    list_past_items = [x for x in list_past_items if x!= 0]\n",
    "    for x in list_past_items:\n",
    "        tmp_past_items[x] += 1\n",
    "    list_past_items_unique = [k for k, v in tmp_past_items.most_common(20)]\n",
    "    ###            \n",
    "    # We add last item of past items\n",
    "    if len(list_past_items) >= 1:\n",
    "        tmp_preds[list_past_items[-1]] += 2\n",
    "    ###\n",
    "    if qt_past_items >= 20:\n",
    "        for x in list_pred_embs:\n",
    "            tmp_preds[x] += 1\n",
    "        if len(tmp_preds) < 20:\n",
    "            for x in list_pred_model:\n",
    "                tmp_preds[x] += 1\n",
    "    else:\n",
    "        if type in ['clicks', 'carts']:\n",
    "            for x in list_pred_model:\n",
    "                tmp_preds[x] += 1\n",
    "            for x in list_pred_embs:\n",
    "                tmp_preds[x] += 1\n",
    "        else:\n",
    "            for x in list_pred_embs:\n",
    "                tmp_preds[x] += 1\n",
    "            for x in list_pred_model:\n",
    "                tmp_preds[x] += 1\n",
    "    for x in list_past_items_unique:\n",
    "        tmp_preds[x] += 0.5\n",
    "        \n",
    "    final_preds = [k for k, v in tmp_preds.most_common(20)]\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp_preds = df_val_merge[~df_val_merge['score'].isna()].sort_values('score')\n",
    "# df_tmp_preds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48096/48096 [00:00<00:00, 53976.79it/s]\n",
      "100%|██████████| 48096/48096 [00:00<00:00, 173050.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>qt_past_items</th>\n",
       "      <th>qt_trues</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.809600e+04</td>\n",
       "      <td>48096.000000</td>\n",
       "      <td>48096.000000</td>\n",
       "      <td>20335.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.405316e+06</td>\n",
       "      <td>7.533870</td>\n",
       "      <td>1.726755</td>\n",
       "      <td>0.318049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.722111e+06</td>\n",
       "      <td>13.543594</td>\n",
       "      <td>4.425970</td>\n",
       "      <td>0.454682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.480000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.179468e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.371556e+06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.620998e+06</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.289906e+07</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            session  qt_past_items      qt_trues         score\n",
       "count  4.809600e+04   48096.000000  48096.000000  20335.000000\n",
       "mean   6.405316e+06       7.533870      1.726755      0.318049\n",
       "std    3.722111e+06      13.543594      4.425970      0.454682\n",
       "min    5.480000e+02       1.000000      0.000000      0.000000\n",
       "25%    3.179468e+06       1.000000      0.000000      0.000000\n",
       "50%    6.371556e+06       3.000000      0.000000      0.000000\n",
       "75%    9.620998e+06       7.000000      1.000000      1.000000\n",
       "max    1.289906e+07      79.000000     47.000000      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'carts': 0.3260695213186236,\n",
       " 'clicks': 0.30532681420483787,\n",
       " 'orders': 0.4313093420726104}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Metric: 0.3871\n"
     ]
    }
   ],
   "source": [
    "df_val_merge = df_val_rule.merge(df_val_model, how='inner', on='session')\n",
    "df_val_merge['predictions_final'] = df_val_merge.progress_apply(lambda x : emsemble_predictions(x['qt_past_items'], x['past_items'], x['type'], x['predictions'], x['prediction']), axis=1)\n",
    "df_val_merge['score'] = df_val_merge.progress_apply(lambda x: get_score_metric(x['trues'], x['predictions'], x['type'], k=20), axis=1)\n",
    "\n",
    "display(df_val_merge.describe())\n",
    "dict_scores = df_val_merge.groupby('type')['score'].mean().to_dict()\n",
    "display(dict_scores)\n",
    "kaggle_metric = 0.1*dict_scores['clicks'] + 0.3*dict_scores['carts'] + 0.6*dict_scores['orders']\n",
    "print(f'Kaggle Metric: {kaggle_metric:.4f}')\n",
    "\n",
    "# # Model\n",
    "# {'carts': 0.36967699795805764,\n",
    "#  'clicks': 0.3317656031524088,\n",
    "#  'orders': 0.49884798425002636}\n",
    "# Kaggle Metric: 0.4434\n",
    "\n",
    "# {'carts': 0.37824778417738686,\n",
    "#  'clicks': 0.34341224909933094,\n",
    "#  'orders': 0.5106465174467738}\n",
    "# Kaggle Metric: 0.4542\n",
    "\n",
    "# rule + embeddings\n",
    "# {'carts': 0.33723911424885844,\n",
    "#  'clicks': 0.2439938985636202,\n",
    "#  'orders': 0.5639535939043883}\n",
    "# Kaggle Metric: 0.4639\n",
    "\n",
    "# {'carts': 0.3287085750659387,\n",
    "#  'clicks': 0.2584920226453937,\n",
    "#  'orders': 0.5698522081263553}\n",
    "# Kaggle Metric: 0.4664\n",
    "\n",
    "# final - ensemble\n",
    "# {'carts': 0.39870859533618364,\n",
    "#  'clicks': 0.34835388331002926,\n",
    "#  'orders': 0.5649640172185767}\n",
    "# Kaggle Metric: 0.4934\n",
    "\n",
    "# {'carts': 0.39924042328738585,\n",
    "#  'clicks': 0.3641276376737005,\n",
    "#  'orders': 0.5725932913496049}\n",
    "# Kaggle Metric: 0.4997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>prediction</th>\n",
       "      <th>qt_past_items</th>\n",
       "      <th>past_items</th>\n",
       "      <th>predictions</th>\n",
       "      <th>trues</th>\n",
       "      <th>type</th>\n",
       "      <th>qt_trues</th>\n",
       "      <th>predictions_final</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7988810</td>\n",
       "      <td>[1302210, 1735258, 607231, 1721635, 884174, 54...</td>\n",
       "      <td>20</td>\n",
       "      <td>[1735258, 1735258, 1735258, 607231, 607231, 88...</td>\n",
       "      <td>[1302210, 1302210, 1288292, 1139298, 77368, 46...</td>\n",
       "      <td>[464216]</td>\n",
       "      <td>clicks</td>\n",
       "      <td>1</td>\n",
       "      <td>[1302210, 1735258, 607231, 1721635, 884174, 54...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7988810</td>\n",
       "      <td>[1302210, 1735258, 607231, 1721635, 884174, 54...</td>\n",
       "      <td>20</td>\n",
       "      <td>[1735258, 1735258, 1735258, 607231, 607231, 88...</td>\n",
       "      <td>[1302210, 1302210, 77368, 465180, 1139298, 128...</td>\n",
       "      <td>[1302210]</td>\n",
       "      <td>carts</td>\n",
       "      <td>1</td>\n",
       "      <td>[1302210, 1735258, 607231, 1721635, 884174, 54...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4504462</td>\n",
       "      <td>[1547201, 1679951, 736260, 608812, 1372217, 10...</td>\n",
       "      <td>3</td>\n",
       "      <td>[1679951, 1679951, 1547201, 215561]</td>\n",
       "      <td>[215561, 1679951, 413827, 1547201, 1499394, 19...</td>\n",
       "      <td>[1717153, 1767620, 382117, 1383529, 889686]</td>\n",
       "      <td>clicks</td>\n",
       "      <td>5</td>\n",
       "      <td>[1679951, 215561, 1547201, 413827, 1499394, 19...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11243337</td>\n",
       "      <td>[561352, 541254, 361867, 1797671, 1614737, 161...</td>\n",
       "      <td>13</td>\n",
       "      <td>[1200447, 361867, 1797671, 1614737, 1616342, 8...</td>\n",
       "      <td>[541254, 541254, 561352, 1197114, 1086629, 361...</td>\n",
       "      <td>[1094947, 1266508, 772856, 1842010, 1374205, 4...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>6</td>\n",
       "      <td>[541254, 1797671, 361867, 561352, 355790, 1614...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10498189</td>\n",
       "      <td>[1513653, 1767646, 526357, 572456, 508354, 136...</td>\n",
       "      <td>7</td>\n",
       "      <td>[526357, 1767646, 1767646, 1767646, 1767646, 1...</td>\n",
       "      <td>[1841371, 1513653, 1841371, 1077678, 1109308, ...</td>\n",
       "      <td>[682370, 1215753, 1533675, 1027728, 252337, 16...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>8</td>\n",
       "      <td>[1841371, 1513653, 1767646, 526357, 1077678, 1...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48084</th>\n",
       "      <td>244006</td>\n",
       "      <td>[1073464, 835002, 1219143, 1707890, 130486, 64...</td>\n",
       "      <td>8</td>\n",
       "      <td>[649030, 835002, 835002, 130486, 1707890, 1219...</td>\n",
       "      <td>[205182, 205182, 835002, 1851567, 1059972, 155...</td>\n",
       "      <td>[1146755, 1616778, 1346715, 1119262, 1153314, ...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>28</td>\n",
       "      <td>[205182, 835002, 1073464, 1219143, 1707890, 13...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48085</th>\n",
       "      <td>244006</td>\n",
       "      <td>[1073464, 835002, 1219143, 1707890, 130486, 64...</td>\n",
       "      <td>8</td>\n",
       "      <td>[649030, 835002, 835002, 130486, 1707890, 1219...</td>\n",
       "      <td>[205182, 205182, 835002, 1851567, 1559703, 160...</td>\n",
       "      <td>[1676639, 77543, 637498, 1119262, 1382394, 851...</td>\n",
       "      <td>carts</td>\n",
       "      <td>8</td>\n",
       "      <td>[205182, 835002, 1219143, 1073464, 1707890, 13...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48087</th>\n",
       "      <td>10442323</td>\n",
       "      <td>[155954, 1798464, 1651724, 774152, 783827, 633...</td>\n",
       "      <td>1</td>\n",
       "      <td>[155954, 1760714]</td>\n",
       "      <td>[1760714, 155954, 1760714, 1743151, 1239687, 7...</td>\n",
       "      <td>[1760714]</td>\n",
       "      <td>clicks</td>\n",
       "      <td>1</td>\n",
       "      <td>[1760714, 155954, 783827, 1651724, 1798464, 63...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48090</th>\n",
       "      <td>11419214</td>\n",
       "      <td>[1137945, 680719, 1824916, 562169, 1405871, 31...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1137945, 1137945]</td>\n",
       "      <td>[1137945, 1137945, 420572, 1301535, 1015397, 3...</td>\n",
       "      <td>[1137945, 308051, 1283678]</td>\n",
       "      <td>clicks</td>\n",
       "      <td>3</td>\n",
       "      <td>[1137945, 420572, 1301535, 1015397, 331655, 10...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48093</th>\n",
       "      <td>7295880</td>\n",
       "      <td>[1830730, 1221281, 1291045, 190284, 492815, 89...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1830730, 1830730]</td>\n",
       "      <td>[1830730, 1830730, 546395, 630279, 633566, 124...</td>\n",
       "      <td>[1597032, 990171, 419948, 578046]</td>\n",
       "      <td>clicks</td>\n",
       "      <td>4</td>\n",
       "      <td>[1830730, 546395, 630279, 633566, 1242770, 157...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20335 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        session                                         prediction  \\\n",
       "0       7988810  [1302210, 1735258, 607231, 1721635, 884174, 54...   \n",
       "1       7988810  [1302210, 1735258, 607231, 1721635, 884174, 54...   \n",
       "3       4504462  [1547201, 1679951, 736260, 608812, 1372217, 10...   \n",
       "6      11243337  [561352, 541254, 361867, 1797671, 1614737, 161...   \n",
       "9      10498189  [1513653, 1767646, 526357, 572456, 508354, 136...   \n",
       "...         ...                                                ...   \n",
       "48084    244006  [1073464, 835002, 1219143, 1707890, 130486, 64...   \n",
       "48085    244006  [1073464, 835002, 1219143, 1707890, 130486, 64...   \n",
       "48087  10442323  [155954, 1798464, 1651724, 774152, 783827, 633...   \n",
       "48090  11419214  [1137945, 680719, 1824916, 562169, 1405871, 31...   \n",
       "48093   7295880  [1830730, 1221281, 1291045, 190284, 492815, 89...   \n",
       "\n",
       "       qt_past_items                                         past_items  \\\n",
       "0                 20  [1735258, 1735258, 1735258, 607231, 607231, 88...   \n",
       "1                 20  [1735258, 1735258, 1735258, 607231, 607231, 88...   \n",
       "3                  3                [1679951, 1679951, 1547201, 215561]   \n",
       "6                 13  [1200447, 361867, 1797671, 1614737, 1616342, 8...   \n",
       "9                  7  [526357, 1767646, 1767646, 1767646, 1767646, 1...   \n",
       "...              ...                                                ...   \n",
       "48084              8  [649030, 835002, 835002, 130486, 1707890, 1219...   \n",
       "48085              8  [649030, 835002, 835002, 130486, 1707890, 1219...   \n",
       "48087              1                                  [155954, 1760714]   \n",
       "48090              1                                 [1137945, 1137945]   \n",
       "48093              1                                 [1830730, 1830730]   \n",
       "\n",
       "                                             predictions  \\\n",
       "0      [1302210, 1302210, 1288292, 1139298, 77368, 46...   \n",
       "1      [1302210, 1302210, 77368, 465180, 1139298, 128...   \n",
       "3      [215561, 1679951, 413827, 1547201, 1499394, 19...   \n",
       "6      [541254, 541254, 561352, 1197114, 1086629, 361...   \n",
       "9      [1841371, 1513653, 1841371, 1077678, 1109308, ...   \n",
       "...                                                  ...   \n",
       "48084  [205182, 205182, 835002, 1851567, 1059972, 155...   \n",
       "48085  [205182, 205182, 835002, 1851567, 1559703, 160...   \n",
       "48087  [1760714, 155954, 1760714, 1743151, 1239687, 7...   \n",
       "48090  [1137945, 1137945, 420572, 1301535, 1015397, 3...   \n",
       "48093  [1830730, 1830730, 546395, 630279, 633566, 124...   \n",
       "\n",
       "                                                   trues    type  qt_trues  \\\n",
       "0                                               [464216]  clicks         1   \n",
       "1                                              [1302210]   carts         1   \n",
       "3            [1717153, 1767620, 382117, 1383529, 889686]  clicks         5   \n",
       "6      [1094947, 1266508, 772856, 1842010, 1374205, 4...  clicks         6   \n",
       "9      [682370, 1215753, 1533675, 1027728, 252337, 16...  clicks         8   \n",
       "...                                                  ...     ...       ...   \n",
       "48084  [1146755, 1616778, 1346715, 1119262, 1153314, ...  clicks        28   \n",
       "48085  [1676639, 77543, 637498, 1119262, 1382394, 851...   carts         8   \n",
       "48087                                          [1760714]  clicks         1   \n",
       "48090                         [1137945, 308051, 1283678]  clicks         3   \n",
       "48093                  [1597032, 990171, 419948, 578046]  clicks         4   \n",
       "\n",
       "                                       predictions_final  score  \n",
       "0      [1302210, 1735258, 607231, 1721635, 884174, 54...    0.0  \n",
       "1      [1302210, 1735258, 607231, 1721635, 884174, 54...    1.0  \n",
       "3      [1679951, 215561, 1547201, 413827, 1499394, 19...    0.0  \n",
       "6      [541254, 1797671, 361867, 561352, 355790, 1614...    0.0  \n",
       "9      [1841371, 1513653, 1767646, 526357, 1077678, 1...    0.0  \n",
       "...                                                  ...    ...  \n",
       "48084  [205182, 835002, 1073464, 1219143, 1707890, 13...    0.0  \n",
       "48085  [205182, 835002, 1219143, 1073464, 1707890, 13...    0.0  \n",
       "48087  [1760714, 155954, 783827, 1651724, 1798464, 63...    1.0  \n",
       "48090  [1137945, 420572, 1301535, 1015397, 331655, 10...    1.0  \n",
       "48093  [1830730, 546395, 630279, 633566, 1242770, 157...    0.0  \n",
       "\n",
       "[20335 rows x 10 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_val_merge[~df_val_merge['score'].isna()]\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_val_merge, df_val_model, df_val_rule, list_sessions_val, list_items_val, list_types_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pl.read_parquet(\n",
    "    '../0_Data/data_optimized/test.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1671803/1671803 [00:02<00:00, 620252.66it/s]\n"
     ]
    }
   ],
   "source": [
    "session_types = ['clicks', 'carts', 'orders']\n",
    "\n",
    "test_session_items = df_test.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n",
    "test_session_types = df_test.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n",
    "test_sessions = test_session_items.index\n",
    "\n",
    "num_items_to_consider = 20\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "list_labels, list_sessions = [], []\n",
    "for session, seq_items, seq_types in tqdm(zip(test_sessions, test_session_items, test_session_types), total=len(test_session_items)):\n",
    "    if len(seq_items) >= num_items_to_consider:\n",
    "        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n",
    "        weights=np.logspace(0.1, 1, len(seq_items), base=2, endpoint=True)-1\n",
    "        aids_temp=defaultdict(lambda: 0)\n",
    "        for aid,w,t in zip(seq_items, weights, seq_types): \n",
    "            if dict_map_inv[aid+1] == 0:\n",
    "                aids_temp[aid] = 0\n",
    "            else:  \n",
    "                aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "            \n",
    "        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n",
    "        list_labels.append(sorted_aids[:num_items_to_consider])\n",
    "    else:\n",
    "        list_labels.append([0])\n",
    "    list_sessions.append(session)\n",
    "\n",
    "# del test_session_items, test_session_types\n",
    "# gc.collect()\n",
    "\n",
    "df_test_rule = pd.DataFrame({\n",
    "    'session' : list_sessions,\n",
    "    'prediction' : list_labels,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1323it [02:00, 10.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m features \u001b[39m=\u001b[39m (seq_items, tf\u001b[39m.\u001b[39mstack(seq_type_new, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), seq_time, seq_recency)\n\u001b[1;32m     27\u001b[0m preds \u001b[39m=\u001b[39m model(features, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m preds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mgather(preds, indices\u001b[39m=\u001b[39;49midxs, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, batch_dims\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m topk_scores, topk_idxs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mtop_k(preds, k\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m     30\u001b[0m type_ \u001b[39m=\u001b[39m type_target[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5297\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   5289\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgather\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   5290\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   5291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather_v2\u001b[39m(params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5295\u001b[0m               batch_dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   5296\u001b[0m               name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 5297\u001b[0m   \u001b[39mreturn\u001b[39;00m gather(\n\u001b[1;32m   5298\u001b[0m       params,\n\u001b[1;32m   5299\u001b[0m       indices,\n\u001b[1;32m   5300\u001b[0m       validate_indices\u001b[39m=\u001b[39;49mvalidate_indices,\n\u001b[1;32m   5301\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   5302\u001b[0m       axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5303\u001b[0m       batch_dims\u001b[39m=\u001b[39;49mbatch_dims)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:561\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    554\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    555\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    556\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[1;32m    560\u001b[0m           instructions)\n\u001b[0;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5279\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5277\u001b[0m   axis \u001b[39m=\u001b[39m batch_dims\n\u001b[1;32m   5278\u001b[0m \u001b[39mif\u001b[39;00m tensor_util\u001b[39m.\u001b[39mconstant_value(axis) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 5279\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mgather_v2(\n\u001b[1;32m   5280\u001b[0m       params, indices, axis, batch_dims\u001b[39m=\u001b[39;49mbatch_dims, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   5281\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   5282\u001b[0m   \u001b[39m# TODO(apassos) find a less bad way of detecting resource variables\u001b[39;00m\n\u001b[1;32m   5283\u001b[0m   \u001b[39m# without introducing a circular dependency.\u001b[39;00m\n\u001b[1;32m   5284\u001b[0m   \u001b[39mreturn\u001b[39;00m params\u001b[39m.\u001b[39msparse_read(indices, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:3940\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   3938\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3939\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3940\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3941\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mGatherV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, params, indices, axis, \u001b[39m\"\u001b[39;49m\u001b[39mbatch_dims\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   3942\u001b[0m       batch_dims)\n\u001b[1;32m   3943\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3944\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_paths_val = ['../tfrecords/tfrecords_v0.7/na_split=test/' + x for x in os.listdir('../tfrecords/tfrecords_v0.7/na_split=test')]\n",
    "test_dataloader = SASRecDataLoader(list_paths_val, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20, \n",
    "                                     seq_len_target=50, \n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0, \n",
    "                                     is_val=False,\n",
    "                                     get_session=True, \n",
    "                                     is_test=True,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "list_predictions, list_sessions, list_types, list_scores = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(test_dataloader)):\n",
    "    features, idxs, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    idxs = idxs.numpy() - 1\n",
    "    ###\n",
    "    preds = model(features, training=False)\n",
    "    preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "    topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "    topk_idxs = np.asarray([[dict_map[x]-1 for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "    ### \n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_sessions.append(session.numpy())\n",
    "\n",
    "# 41796it [1:22:29,  8.44it/s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5015409, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>predictions</th>\n",
       "      <th>type</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictions_final</th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14309103</td>\n",
       "      <td>[1048964, 539417, 744067, 1182248, 654388, 813...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1048964, 539417, 744067, 1182248, 654388, 813...</td>\n",
       "      <td>14309103_clicks</td>\n",
       "      <td>1048964 539417 744067 1182248 654388 813656 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14309103</td>\n",
       "      <td>[1048964, 744067, 539417, 1182248, 813816, 813...</td>\n",
       "      <td>carts</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1048964, 744067, 539417, 1182248, 813816, 813...</td>\n",
       "      <td>14309103_carts</td>\n",
       "      <td>1048964 744067 539417 1182248 813816 813656 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14309103</td>\n",
       "      <td>[1048964, 744067, 539417, 1182248, 813816, 813...</td>\n",
       "      <td>orders</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1048964, 744067, 539417, 1182248, 813816, 813...</td>\n",
       "      <td>14309103_orders</td>\n",
       "      <td>1048964 744067 539417 1182248 813816 813656 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13784979</td>\n",
       "      <td>[393449, 831848, 228982, 390399, 56279, 219222...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[393449, 831848, 228982, 390399, 56279, 219222...</td>\n",
       "      <td>13784979_clicks</td>\n",
       "      <td>393449 831848 228982 390399 56279 219222 18429...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13784979</td>\n",
       "      <td>[393449, 831848, 228982, 219222, 748414, 18429...</td>\n",
       "      <td>carts</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[393449, 831848, 228982, 219222, 748414, 18429...</td>\n",
       "      <td>13784979_carts</td>\n",
       "      <td>393449 831848 228982 219222 748414 1842909 891...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015404</th>\n",
       "      <td>13408102</td>\n",
       "      <td>[1358904, 396974, 898986, 1485262, 74336, 7555...</td>\n",
       "      <td>carts</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1358904, 396974, 898986, 1485262, 74336, 7555...</td>\n",
       "      <td>13408102_carts</td>\n",
       "      <td>1358904 396974 898986 1485262 74336 755541 132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015405</th>\n",
       "      <td>13408102</td>\n",
       "      <td>[1358904, 396974, 898986, 1485262, 755541, 541...</td>\n",
       "      <td>orders</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1358904, 396974, 898986, 1485262, 755541, 541...</td>\n",
       "      <td>13408102_orders</td>\n",
       "      <td>1358904 396974 898986 1485262 755541 541728 62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015406</th>\n",
       "      <td>13857463</td>\n",
       "      <td>[1838234, 276012, 32808, 1636724, 654388, 1027...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1838234, 276012, 32808, 1636724, 654388, 1027...</td>\n",
       "      <td>13857463_clicks</td>\n",
       "      <td>1838234 276012 32808 1636724 654388 102778 730...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015407</th>\n",
       "      <td>13857463</td>\n",
       "      <td>[1838234, 276012, 32808, 774027, 1733659, 1115...</td>\n",
       "      <td>carts</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1838234, 276012, 32808, 774027, 1733659, 1115...</td>\n",
       "      <td>13857463_carts</td>\n",
       "      <td>1838234 276012 32808 774027 1733659 1115883 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015408</th>\n",
       "      <td>13857463</td>\n",
       "      <td>[1838234, 276012, 32808, 1636724, 774027, 1027...</td>\n",
       "      <td>orders</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1838234, 276012, 32808, 1636724, 774027, 1027...</td>\n",
       "      <td>13857463_orders</td>\n",
       "      <td>1838234 276012 32808 1636724 774027 102778 173...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5015409 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          session                                        predictions    type  \\\n",
       "0        14309103  [1048964, 539417, 744067, 1182248, 654388, 813...  clicks   \n",
       "1        14309103  [1048964, 744067, 539417, 1182248, 813816, 813...   carts   \n",
       "2        14309103  [1048964, 744067, 539417, 1182248, 813816, 813...  orders   \n",
       "3        13784979  [393449, 831848, 228982, 390399, 56279, 219222...  clicks   \n",
       "4        13784979  [393449, 831848, 228982, 219222, 748414, 18429...   carts   \n",
       "...           ...                                                ...     ...   \n",
       "5015404  13408102  [1358904, 396974, 898986, 1485262, 74336, 7555...   carts   \n",
       "5015405  13408102  [1358904, 396974, 898986, 1485262, 755541, 541...  orders   \n",
       "5015406  13857463  [1838234, 276012, 32808, 1636724, 654388, 1027...  clicks   \n",
       "5015407  13857463  [1838234, 276012, 32808, 774027, 1733659, 1115...   carts   \n",
       "5015408  13857463  [1838234, 276012, 32808, 1636724, 774027, 1027...  orders   \n",
       "\n",
       "        prediction                                  predictions_final  \\\n",
       "0              [0]  [1048964, 539417, 744067, 1182248, 654388, 813...   \n",
       "1              [0]  [1048964, 744067, 539417, 1182248, 813816, 813...   \n",
       "2              [0]  [1048964, 744067, 539417, 1182248, 813816, 813...   \n",
       "3              [0]  [393449, 831848, 228982, 390399, 56279, 219222...   \n",
       "4              [0]  [393449, 831848, 228982, 219222, 748414, 18429...   \n",
       "...            ...                                                ...   \n",
       "5015404        [0]  [1358904, 396974, 898986, 1485262, 74336, 7555...   \n",
       "5015405        [0]  [1358904, 396974, 898986, 1485262, 755541, 541...   \n",
       "5015406        [0]  [1838234, 276012, 32808, 1636724, 654388, 1027...   \n",
       "5015407        [0]  [1838234, 276012, 32808, 774027, 1733659, 1115...   \n",
       "5015408        [0]  [1838234, 276012, 32808, 1636724, 774027, 1027...   \n",
       "\n",
       "            session_type                                             labels  \n",
       "0        14309103_clicks  1048964 539417 744067 1182248 654388 813656 81...  \n",
       "1         14309103_carts  1048964 744067 539417 1182248 813816 813656 65...  \n",
       "2        14309103_orders  1048964 744067 539417 1182248 813816 813656 16...  \n",
       "3        13784979_clicks  393449 831848 228982 390399 56279 219222 18429...  \n",
       "4         13784979_carts  393449 831848 228982 219222 748414 1842909 891...  \n",
       "...                  ...                                                ...  \n",
       "5015404   13408102_carts  1358904 396974 898986 1485262 74336 755541 132...  \n",
       "5015405  13408102_orders  1358904 396974 898986 1485262 755541 541728 62...  \n",
       "5015406  13857463_clicks  1838234 276012 32808 1636724 654388 102778 730...  \n",
       "5015407   13857463_carts  1838234 276012 32808 774027 1733659 1115883 10...  \n",
       "5015408  13857463_orders  1838234 276012 32808 1636724 774027 102778 173...  \n",
       "\n",
       "[5015409 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_submission = f\"submission_{datetime.now().__str__().split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_')}\"\n",
    "\n",
    "df_inference = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_inference = df_inference.merge(df_test_rule, how='inner', on='session')\n",
    "df_inference['predictions_final'] = df_inference.progress_apply(lambda x : x['predictions'] if len(x['prediction'])==1 else x['prediction'], axis=1)\n",
    "\n",
    "df_inference['session_type'] = df_inference['session'].astype(str) + '_' + df_inference['type']\n",
    "df_inference['labels'] = df_inference['predictions_final'].apply(lambda x : ' '.join([str(y) for y in x]))\n",
    "df_inference[['session_type', 'labels']].to_csv(f'../3_Submissions/{name_submission}.csv', index=False)\n",
    "\n",
    "print(df_inference.shape)\n",
    "display(\n",
    "    df_inference\n",
    ")\n",
    "\n",
    "import gzip\n",
    "with open(f'../3_Submissions/{name_submission}.csv', 'rb') as f_in, gzip.open(f'../3_Submissions/{name_submission}.csv.gz', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0432fa0070c5c9f7d9e158f590013ccc765eb84f02e6f69521746370c3bf6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
