{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 08:13:37.689400: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 08:13:37.814419: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-01 08:13:37.865792: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-01 08:13:38.399531: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-12-01 08:13:38.399565: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-12-01 08:13:38.399580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 08:13:39.162889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-01 08:13:39.241382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-01 08:13:39.241480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "from models import build_model_bert4Rec\n",
    "from dataloader import Bert4RecDataLoader\n",
    "\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_ITEMS: 1855604\n"
     ]
    }
   ],
   "source": [
    "# df_mapping = pd.read_csv('../tfrecords/tfrecords_v0.5/df_mapping.csv')\n",
    "NUM_ITEMS = 1_855_603+1\n",
    "\n",
    "# dict_map, dict_map_inv = {}, {}\n",
    "# for x in tqdm(df_mapping.to_dict('records')):\n",
    "#     dict_map[x['aid_map']] = x['aid']\n",
    "#     dict_map_inv[x['aid']] = x['aid_map']\n",
    "\n",
    "dict_map_type = {\n",
    "    'clicks' : 1,\n",
    "    'carts' : 2,\n",
    "    'orders' : 3\n",
    "  }\n",
    "\n",
    "print(f'NUM_ITEMS: {NUM_ITEMS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:36:58.946608: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 20:36:58.947625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 20:36:58.947718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 20:36:58.947758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 20:36:59.230898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 20:36:59.231012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 20:36:59.231071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 20:36:59.231116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21861 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "20222it [02:27, 136.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_paths = ['../tfrecords/tfrecords_v0.6/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.6/na_split=val')]\n",
    "# 5,45, 1,09\n",
    "dataloader = Bert4RecDataLoader(list_paths, \n",
    "                                num_items=NUM_ITEMS, \n",
    "                                seq_len=80, \n",
    "                                seq_len_target=None,\n",
    "                                batch_size=32, \n",
    "                                mask_prob=0.0, \n",
    "                                reverse_prob=0.0, \n",
    "                                get_session=True,\n",
    "                                is_val=True,\n",
    "                                is_test=False,\n",
    "                                shuffle=False).get_generator()\n",
    "# Val\n",
    "list_sessions_val, list_items_val, list_types_val = [], [], []\n",
    "for batch in tqdm(dataloader):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    target, type_target, idx_mask = targets\n",
    "    idxs = idx_mask-1\n",
    "    list_sessions_val = list_sessions_val + session.numpy().tolist()\n",
    "    list_items_val += [[item-1 for item in seq[:idxs[i]] if item!=0] for i, seq in enumerate(seq_items[:, :, 0].numpy())]\n",
    "    list_types_val += [[type_ for type_ in seq[:idxs[i]] if type_!=0] for i, seq in enumerate(seq_type[:, :, 0].numpy())]\n",
    "    # break\n",
    "print(len(list_sessions_val))\n",
    "# 20110it [02:36, 128.29it/s]\n",
    "\n",
    "# del dataloader, batch, features, targets, session\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Model based predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:39:27.635783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "class BERT4REC_CONFIG:\n",
    "    seed = 42 \n",
    "    num_items = NUM_ITEMS\n",
    "    model_arch = 'bert4rec'\n",
    "    path_tfrecords = '../tfrecords/tfrecords_v0.6/'\n",
    "    restore_last_chekpoint = (True, 'model_bert4rec_complete_0.11/checkpoints/', 'ckpt-25')\n",
    "    model_name = 'model_bert4rec_complete_0.10'\n",
    "    checkpoint_filepath = f'../2_Models/'\n",
    "    num_records_dataset = 10_000_000\n",
    "    batch_size = 32\n",
    "    tup_scheduler_grad_accum = (5, 10, 1_500_000) #(start_grad_accum, max_grad_accum, ramp_up_samples)\n",
    "    seq_len = 40\n",
    "    mask_prob = 0.4\n",
    "    reverse_prob = 0.5\n",
    "    emb_dim = 128\n",
    "    trf_dim = 128\n",
    "    num_heads = 4\n",
    "    num_layers = 1\n",
    "    ff_dim = trf_dim*4\n",
    "    drop_rate = 0.1\n",
    "    att_drop_rate = 0.1\n",
    "    epochs = 3\n",
    "    early_stopping = 5\n",
    "    batch_num_printer_train = 500\n",
    "    batch_num_printer_val = 250\n",
    "    clipnorm = 1.0\n",
    "    num_iters_save_checkpoint = 25_000\n",
    "    scheduler_scaler = 128 \n",
    "    warmup_steps = 10_000\n",
    "    weight_decay = 1e-1\n",
    "    log_wandb = True\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model_bert4Rec(num_items=NUM_ITEMS, model_cfg=BERT4REC_CONFIG)\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.11/checkpoints'))\n",
    "# _ = model(features, training=False)\n",
    "# del _\n",
    "# gc.collect()\n",
    "\n",
    "def get_score_metric(y_true, y_pred, type_target, k=20):\n",
    "    y_pred = list(set(y_pred))[:k]\n",
    "    score = 0 \n",
    "    if len(y_true)==0:\n",
    "        return None\n",
    "    if type_target=='clicks':\n",
    "        num_targets = 1\n",
    "        hits = len([x for x in y_pred if x==y_true[0]])\n",
    "    else:\n",
    "        num_targets = min(k, len(y_true))\n",
    "        hits = len([x for x in y_pred if x in y_true])\n",
    "    score = hits / num_targets\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:04,  8.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5726"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_paths_val = ['../tfrecords/tfrecords_v0.6/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.6/na_split=val')]\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=40, \n",
    "                                     seq_len_target=50, \n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0, \n",
    "                                     is_val=True,\n",
    "                                     get_session=True, \n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "num_candidates = 20\n",
    "list_sessions, list_past_items, list_predictions, list_trues, list_types = [], [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(val_dataloader)):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    target, type_target, idx_mask = targets\n",
    "    idxs = idx_mask.numpy() - 1\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=num_candidates)\n",
    "        topk_idxs = np.asarray([[x-1 for x in topk_idxs.numpy()[i, :-1]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        last_item = np.asarray([[x[idx-1, 0]-1] for x, idx in zip(seq_items.numpy(), idxs)])\n",
    "        new_predictions = np.concatenate([last_item, topk_idxs], axis=1)\n",
    "        labels = [list(set([_target-1 for _type, _target in zip(type_target.numpy()[i], target.numpy()[i]) if dict_map_type[type_]==_type and _target!=0])) for i in range(target.shape[0])]\n",
    "        ###\n",
    "        list_sessions.append(session.numpy())\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_trues = list_trues + labels\n",
    "        list_past_items.append(seq_items.numpy()[:, :, 0])\n",
    "    if num_batch==1000:\n",
    "        break\n",
    "\n",
    "df_val_model = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'past_items' : np.concatenate(list_past_items).tolist(),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'trues' : list_trues,\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_val_model['qt_trues'] = df_val_model['trues'].apply(lambda x : len(x))\n",
    "\n",
    "del preds, features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rule based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1855603/1855603 [00:07<00:00, 248748.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_embedding = model.layers[1].weights[0].numpy()\n",
    "index = AnnoyIndex(arr_embedding.shape[1], 'euclidean')\n",
    "\n",
    "# for idx, aid in dict_map.items():\n",
    "#     index.add_item(idx, arr_embedding[idx])\n",
    "\n",
    "for idx in tqdm(range(1, NUM_ITEMS)):\n",
    "    aid = idx - 1\n",
    "    index.add_item(aid, arr_embedding[idx])\n",
    "    \n",
    "index.build(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647081/647081 [00:19<00:00, 33543.43it/s]\n"
     ]
    }
   ],
   "source": [
    "session_types = ['clicks', 'carts', 'orders']\n",
    "\n",
    "# val_session_items = df_val.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n",
    "# val_session_types = df_val.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n",
    "# val_sessions = val_session_items.index\n",
    "\n",
    "num_items_to_consider = 20\n",
    "num_candidates = 20\n",
    "type_weight_multipliers = {1: 1, 2: 6, 3: 3}\n",
    "list_labels, list_sessions, list_len_past_items = [], [], []\n",
    "for session, seq_items, seq_types in tqdm(zip(list_sessions_val, list_items_val, list_types_val), total=len(list_items_val)):\n",
    "    if len(seq_items) >= num_items_to_consider:\n",
    "        # if we have enough aids (over equals num_items_to_consider) we don't need to look for candidates! we just use the old logic\n",
    "        weights=np.logspace(0.1, 1, len(seq_items), base=2, endpoint=True)-1\n",
    "        aids_temp=defaultdict(lambda: 0)\n",
    "        for aid,w,t in zip(seq_items, weights, seq_types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "            \n",
    "        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n",
    "        list_labels.append(sorted_aids[:num_candidates])\n",
    "    else:\n",
    "        if len(seq_items)==0:\n",
    "            seq_items = [1]\n",
    "        # list_labels.append([0])\n",
    "        # here we don't have num_items_to_consider aids to output -- we will use embeddings to generate candidates!\n",
    "        seq_items_new = list(dict.fromkeys(seq_items[::-1]))\n",
    "        \n",
    "        # let's grab the most recent aid\n",
    "        most_recent_aid = seq_items_new[0]\n",
    "        \n",
    "        # and look for some neighbors!\n",
    "        nns = [i-1 for i in index.get_nns_by_item(most_recent_aid, num_candidates+1, search_k=-1)[1:]]\n",
    "                        \n",
    "        list_labels.append((seq_items_new+nns)[:num_candidates])\n",
    "\n",
    "    list_sessions.append(session)\n",
    "    list_len_past_items.append(len(seq_items))\n",
    "\n",
    "# del list_items_val, list_types_val\n",
    "# gc.collect()\n",
    "\n",
    "df_val_rule = pd.DataFrame({\n",
    "    'session' : list_sessions,\n",
    "    'prediction' : list_labels,\n",
    "    'qt_past_items' : list_len_past_items\n",
    "})\n",
    "\n",
    "# 100%|██████████| 647081/647081 [00:22<00:00, 28792.13it/s]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def emsemble_predictions(qt_past_items, type, list_pred_model, list_pred_embs):\n",
    "    tmp_preds = Counter()\n",
    "    if qt_past_items >= 20:\n",
    "        for x in list_pred_embs:\n",
    "            tmp_preds[x] += 1\n",
    "        if len(tmp_preds) < 20:\n",
    "            for x in list_pred_model:\n",
    "                tmp_preds[x] += 1\n",
    "    else:\n",
    "        if type in ['clicks', 'carts']:\n",
    "            for x in list_pred_model:\n",
    "                tmp_preds[x] += 1\n",
    "            for x in list_pred_embs:\n",
    "                tmp_preds[x] += 1\n",
    "        else:\n",
    "            for x in list_pred_embs:\n",
    "                tmp_preds[x] += 1\n",
    "            for x in list_pred_model:\n",
    "                tmp_preds[x] += 1\n",
    "    final_preds = [k for k, v in tmp_preds.most_common(20)]\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96096/96096 [00:01<00:00, 71695.45it/s]\n",
      "100%|██████████| 96096/96096 [00:00<00:00, 177994.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>qt_past_items</th>\n",
       "      <th>qt_trues</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.609600e+04</td>\n",
       "      <td>96096.000000</td>\n",
       "      <td>96096.000000</td>\n",
       "      <td>41133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.513860e+06</td>\n",
       "      <td>7.543300</td>\n",
       "      <td>1.763268</td>\n",
       "      <td>0.369885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.737217e+06</td>\n",
       "      <td>13.521467</td>\n",
       "      <td>4.543446</td>\n",
       "      <td>0.471300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.220000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.292913e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.516554e+06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.797466e+06</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.289960e+07</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            session  qt_past_items      qt_trues         score\n",
       "count  9.609600e+04   96096.000000  96096.000000  41133.000000\n",
       "mean   6.513860e+06       7.543300      1.763268      0.369885\n",
       "std    3.737217e+06      13.521467      4.543446      0.471300\n",
       "min    2.220000e+02       1.000000      0.000000      0.000000\n",
       "25%    3.292913e+06       1.000000      0.000000      0.000000\n",
       "50%    6.516554e+06       2.000000      0.000000      0.000000\n",
       "75%    9.797466e+06       7.000000      1.000000      1.000000\n",
       "max    1.289960e+07      78.000000     50.000000      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'carts': 0.39558536656187565,\n",
       " 'clicks': 0.3451707255039105,\n",
       " 'orders': 0.5659949689258904}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Metric: 0.4928\n"
     ]
    }
   ],
   "source": [
    "df_val_merge = df_val_rule.merge(df_val_model, how='inner', on='session')\n",
    "# df_val_merge['predictions_final'] = df_val_merge.progress_apply(lambda x : list(dict.fromkeys(x['predictions'] + x['prediction'])), axis=1)\n",
    "df_val_merge['predictions_final'] = df_val_merge.progress_apply(lambda x : emsemble_predictions(x['qt_past_items'], x['type'], x['predictions'], x['prediction']), axis=1)\n",
    "df_val_merge['score'] = df_val_merge.progress_apply(lambda x: get_score_metric(x['trues'], x['predictions_final'], x['type']), axis=1)\n",
    "\n",
    "display(df_val_merge.describe())\n",
    "dict_scores = df_val_merge.groupby('type')['score'].mean().to_dict()\n",
    "display(dict_scores)\n",
    "kaggle_metric = 0.1*dict_scores['clicks'] + 0.3*dict_scores['carts'] + 0.6*dict_scores['orders']\n",
    "print(f'Kaggle Metric: {kaggle_metric:.4f}')\n",
    "\n",
    "# # Model\n",
    "# {'carts': 0.36967699795805764,\n",
    "#  'clicks': 0.3317656031524088,\n",
    "#  'orders': 0.49884798425002636}\n",
    "# Kaggle Metric: 0.4434\n",
    "\n",
    "# rule + embeddings\n",
    "# {'carts': 0.33723911424885844,\n",
    "#  'clicks': 0.2439938985636202,\n",
    "#  'orders': 0.5639535939043883}\n",
    "# Kaggle Metric: 0.4639\n",
    "\n",
    "# final - ensemble\n",
    "# {'carts': 0.39870859533618364,\n",
    "#  'clicks': 0.34835388331002926,\n",
    "#  'orders': 0.5649640172185767}\n",
    "# Kaggle Metric: 0.4934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_val_merge, df_val_model, df_val_rule, list_sessions_val, list_items_val, list_types_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pl.read_parquet(\n",
    "    '../0_Data/data_optimized/test.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1671803/1671803 [00:02<00:00, 782559.23it/s] \n"
     ]
    }
   ],
   "source": [
    "session_types = ['clicks', 'carts', 'orders']\n",
    "\n",
    "test_session_items = df_test.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n",
    "test_session_types = df_test.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n",
    "test_sessions = test_session_items.index\n",
    "\n",
    "num_items_to_consider = 20\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "list_labels, list_sessions = [], []\n",
    "for session, seq_items, seq_types in tqdm(zip(test_sessions, test_session_items, test_session_types), total=len(test_session_items)):\n",
    "    if len(seq_items) >= num_items_to_consider:\n",
    "        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n",
    "        weights=np.logspace(0.1, 1, len(seq_items), base=2, endpoint=True)-1\n",
    "        aids_temp=defaultdict(lambda: 0)\n",
    "        for aid,w,t in zip(seq_items, weights, seq_types): \n",
    "            if dict_map_inv[aid+1] == 0:\n",
    "                aids_temp[aid] = 0\n",
    "            else:  \n",
    "                aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "            \n",
    "        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n",
    "        list_labels.append(sorted_aids[:num_items_to_consider])\n",
    "    else:\n",
    "        list_labels.append([0])\n",
    "    list_sessions.append(session)\n",
    "\n",
    "# del test_session_items, test_session_types\n",
    "# gc.collect()\n",
    "\n",
    "df_test_rule = pd.DataFrame({\n",
    "    'session' : list_sessions,\n",
    "    'prediction' : list_labels,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30960it [1:15:19,  6.85it/s]\n"
     ]
    }
   ],
   "source": [
    "list_paths_test = ['../tfrecords/tfrecords_v0.5/na_split=test/' + x for x in os.listdir('../tfrecords/tfrecords_v0.5/na_split=test')]\n",
    "test_dataloader = Bert4RecDataLoader(list_paths_test, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=30,  \n",
    "                                     batch_size=54, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     is_val=False,\n",
    "                                     is_test=True,\n",
    "                                     get_session=True,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "list_predictions, list_sessions, list_types, list_scores = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(test_dataloader)):\n",
    "    features, idxs, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    idxs = idxs.numpy() - 1\n",
    "    ###\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[dict_map[x]-1 for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        last_item = np.asarray([[dict_map[x[idx-1, 0]]-1] for x, idx in zip(seq_items.numpy(), idxs)])\n",
    "        new_predictions = np.concatenate([last_item, topk_idxs], axis=1)\n",
    "        list_predictions.append(new_predictions)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_sessions.append(session.numpy())\n",
    "\n",
    "# 41796it [1:22:29,  8.44it/s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5015409, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>predictions</th>\n",
       "      <th>type</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictions_final</th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14309103</td>\n",
       "      <td>[1048964, 539417, 744067, 1182248, 654388, 813...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1048964, 539417, 744067, 1182248, 654388, 813...</td>\n",
       "      <td>14309103_clicks</td>\n",
       "      <td>1048964 539417 744067 1182248 654388 813656 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14309103</td>\n",
       "      <td>[1048964, 744067, 539417, 1182248, 813816, 813...</td>\n",
       "      <td>carts</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1048964, 744067, 539417, 1182248, 813816, 813...</td>\n",
       "      <td>14309103_carts</td>\n",
       "      <td>1048964 744067 539417 1182248 813816 813656 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14309103</td>\n",
       "      <td>[1048964, 744067, 539417, 1182248, 813816, 813...</td>\n",
       "      <td>orders</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1048964, 744067, 539417, 1182248, 813816, 813...</td>\n",
       "      <td>14309103_orders</td>\n",
       "      <td>1048964 744067 539417 1182248 813816 813656 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13784979</td>\n",
       "      <td>[393449, 831848, 228982, 390399, 56279, 219222...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[393449, 831848, 228982, 390399, 56279, 219222...</td>\n",
       "      <td>13784979_clicks</td>\n",
       "      <td>393449 831848 228982 390399 56279 219222 18429...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13784979</td>\n",
       "      <td>[393449, 831848, 228982, 219222, 748414, 18429...</td>\n",
       "      <td>carts</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[393449, 831848, 228982, 219222, 748414, 18429...</td>\n",
       "      <td>13784979_carts</td>\n",
       "      <td>393449 831848 228982 219222 748414 1842909 891...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015404</th>\n",
       "      <td>13408102</td>\n",
       "      <td>[1358904, 396974, 898986, 1485262, 74336, 7555...</td>\n",
       "      <td>carts</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1358904, 396974, 898986, 1485262, 74336, 7555...</td>\n",
       "      <td>13408102_carts</td>\n",
       "      <td>1358904 396974 898986 1485262 74336 755541 132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015405</th>\n",
       "      <td>13408102</td>\n",
       "      <td>[1358904, 396974, 898986, 1485262, 755541, 541...</td>\n",
       "      <td>orders</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1358904, 396974, 898986, 1485262, 755541, 541...</td>\n",
       "      <td>13408102_orders</td>\n",
       "      <td>1358904 396974 898986 1485262 755541 541728 62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015406</th>\n",
       "      <td>13857463</td>\n",
       "      <td>[1838234, 276012, 32808, 1636724, 654388, 1027...</td>\n",
       "      <td>clicks</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1838234, 276012, 32808, 1636724, 654388, 1027...</td>\n",
       "      <td>13857463_clicks</td>\n",
       "      <td>1838234 276012 32808 1636724 654388 102778 730...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015407</th>\n",
       "      <td>13857463</td>\n",
       "      <td>[1838234, 276012, 32808, 774027, 1733659, 1115...</td>\n",
       "      <td>carts</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1838234, 276012, 32808, 774027, 1733659, 1115...</td>\n",
       "      <td>13857463_carts</td>\n",
       "      <td>1838234 276012 32808 774027 1733659 1115883 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015408</th>\n",
       "      <td>13857463</td>\n",
       "      <td>[1838234, 276012, 32808, 1636724, 774027, 1027...</td>\n",
       "      <td>orders</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1838234, 276012, 32808, 1636724, 774027, 1027...</td>\n",
       "      <td>13857463_orders</td>\n",
       "      <td>1838234 276012 32808 1636724 774027 102778 173...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5015409 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          session                                        predictions    type  \\\n",
       "0        14309103  [1048964, 539417, 744067, 1182248, 654388, 813...  clicks   \n",
       "1        14309103  [1048964, 744067, 539417, 1182248, 813816, 813...   carts   \n",
       "2        14309103  [1048964, 744067, 539417, 1182248, 813816, 813...  orders   \n",
       "3        13784979  [393449, 831848, 228982, 390399, 56279, 219222...  clicks   \n",
       "4        13784979  [393449, 831848, 228982, 219222, 748414, 18429...   carts   \n",
       "...           ...                                                ...     ...   \n",
       "5015404  13408102  [1358904, 396974, 898986, 1485262, 74336, 7555...   carts   \n",
       "5015405  13408102  [1358904, 396974, 898986, 1485262, 755541, 541...  orders   \n",
       "5015406  13857463  [1838234, 276012, 32808, 1636724, 654388, 1027...  clicks   \n",
       "5015407  13857463  [1838234, 276012, 32808, 774027, 1733659, 1115...   carts   \n",
       "5015408  13857463  [1838234, 276012, 32808, 1636724, 774027, 1027...  orders   \n",
       "\n",
       "        prediction                                  predictions_final  \\\n",
       "0              [0]  [1048964, 539417, 744067, 1182248, 654388, 813...   \n",
       "1              [0]  [1048964, 744067, 539417, 1182248, 813816, 813...   \n",
       "2              [0]  [1048964, 744067, 539417, 1182248, 813816, 813...   \n",
       "3              [0]  [393449, 831848, 228982, 390399, 56279, 219222...   \n",
       "4              [0]  [393449, 831848, 228982, 219222, 748414, 18429...   \n",
       "...            ...                                                ...   \n",
       "5015404        [0]  [1358904, 396974, 898986, 1485262, 74336, 7555...   \n",
       "5015405        [0]  [1358904, 396974, 898986, 1485262, 755541, 541...   \n",
       "5015406        [0]  [1838234, 276012, 32808, 1636724, 654388, 1027...   \n",
       "5015407        [0]  [1838234, 276012, 32808, 774027, 1733659, 1115...   \n",
       "5015408        [0]  [1838234, 276012, 32808, 1636724, 774027, 1027...   \n",
       "\n",
       "            session_type                                             labels  \n",
       "0        14309103_clicks  1048964 539417 744067 1182248 654388 813656 81...  \n",
       "1         14309103_carts  1048964 744067 539417 1182248 813816 813656 65...  \n",
       "2        14309103_orders  1048964 744067 539417 1182248 813816 813656 16...  \n",
       "3        13784979_clicks  393449 831848 228982 390399 56279 219222 18429...  \n",
       "4         13784979_carts  393449 831848 228982 219222 748414 1842909 891...  \n",
       "...                  ...                                                ...  \n",
       "5015404   13408102_carts  1358904 396974 898986 1485262 74336 755541 132...  \n",
       "5015405  13408102_orders  1358904 396974 898986 1485262 755541 541728 62...  \n",
       "5015406  13857463_clicks  1838234 276012 32808 1636724 654388 102778 730...  \n",
       "5015407   13857463_carts  1838234 276012 32808 774027 1733659 1115883 10...  \n",
       "5015408  13857463_orders  1838234 276012 32808 1636724 774027 102778 173...  \n",
       "\n",
       "[5015409 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_submission = f\"submission_{datetime.now().__str__().split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_')}\"\n",
    "\n",
    "df_inference = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_inference = df_inference.merge(df_test_rule, how='inner', on='session')\n",
    "df_inference['predictions_final'] = df_inference.progress_apply(lambda x : x['predictions'] if len(x['prediction'])==1 else x['prediction'], axis=1)\n",
    "\n",
    "df_inference['session_type'] = df_inference['session'].astype(str) + '_' + df_inference['type']\n",
    "df_inference['labels'] = df_inference['predictions_final'].apply(lambda x : ' '.join([str(y) for y in x]))\n",
    "df_inference[['session_type', 'labels']].to_csv(f'../3_Submissions/{name_submission}.csv', index=False)\n",
    "\n",
    "print(df_inference.shape)\n",
    "display(\n",
    "    df_inference\n",
    ")\n",
    "\n",
    "import gzip\n",
    "with open(f'../3_Submissions/{name_submission}.csv', 'rb') as f_in, gzip.open(f'../3_Submissions/{name_submission}.csv.gz', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0432fa0070c5c9f7d9e158f590013ccc765eb84f02e6f69521746370c3bf6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
