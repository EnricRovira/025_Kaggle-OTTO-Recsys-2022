{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, models, metrics, optimizers, constraints\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# tfrecords for kaggle\n",
    "\n",
    "# name_dataset = 'tfrecords_v0.3_kaggle'\n",
    "# path_out = f'../tfrecords/{name_dataset}/'\n",
    "\n",
    "# if not os.path.exists(path_out):\n",
    "#     os.mkdir(path_out)\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_train'):\n",
    "#     os.rename(path_out + 'na_split_train/' + file, \n",
    "#               path_out + 'na_split_train/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_val'):\n",
    "#     os.rename(path_out + 'na_split_val/' + file, \n",
    "#               path_out + 'na_split_val/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_test'):\n",
    "#     os.rename(path_out + 'na_split_test/' + file, \n",
    "#               path_out + 'na_split_test/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Paths & Global Variables\n",
    "\n",
    "# Train: (datetime.datetime(2022, 7, 31, 22, 0, 0, 25000), datetime.datetime(2022, 8, 28, 21, 59, 59, 984000))\n",
    "# Test: (datetime.datetime(2022, 8, 28, 22, 0, 0, 278000), datetime.datetime(2022, 9, 4, 21, 59, 51, 563000))\n",
    "\n",
    "path_data_raw = '../0_Data/'\n",
    "\n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "df_mapping = pd.read_csv('../tfrecords/tfrecords_v0.3/df_mapping.csv')\n",
    "NUM_ITEMS = len(df_mapping['aid_map'].unique())\n",
    "print(NUM_ITEMS)\n",
    "\n",
    "dict_map = {}\n",
    "for x in tqdm(df_mapping.to_dict('records')):\n",
    "    dict_map[x['aid_map']] = x['aid']\n",
    "\n",
    "dict_map_type = {\n",
    "    'clicks' : 1,\n",
    "    'carts' : 2,\n",
    "    'orders' : 3\n",
    "  }\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert4RecDataLoader:\n",
    "    \"\"\"\n",
    "    Class that iterates over tfrecords in order to get the sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_paths, num_items, seq_len, batch_size, num_targets=-1, mask_prob=0.4, \n",
    "                 reverse_prob=0.2, get_session=False, get_only_first_on_val=False, seq_len_target=None,\n",
    "                 min_size_seq_to_mask=2, is_val=False, is_test=False, avoid_repeats=False, shuffle=False, drop_remainder=False):\n",
    "        self.list_paths = list_paths\n",
    "        self.num_items = num_items\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_targets = num_targets\n",
    "        self.mask_prob = mask_prob\n",
    "        self.reverse_prob = tf.constant(reverse_prob)\n",
    "        self.shuffle = shuffle\n",
    "        self.min_size_seq_to_mask = min_size_seq_to_mask\n",
    "        self.avoid_repeats = avoid_repeats\n",
    "        self.get_session = get_session\n",
    "        self.seq_len_target = seq_len if not seq_len_target else seq_len_target\n",
    "        self.get_only_first_on_val = get_only_first_on_val\n",
    "        self.is_val = is_val\n",
    "        self.is_test = is_test\n",
    "        self.drop_remainder = drop_remainder\n",
    "\n",
    "    def get_generator(self):\n",
    "        dataset = tf.data.TFRecordDataset(self.list_paths, num_parallel_reads=AUTO, compression_type='GZIP')\n",
    "        dataset = dataset.map(self.parse_tf_record, num_parallel_calls=AUTO)\n",
    "        if self.is_val:\n",
    "            dataset = dataset.map(self.make_transforms_val, num_parallel_calls=AUTO)\n",
    "        elif self.is_test:\n",
    "            dataset = dataset.map(self.make_transforms_test, num_parallel_calls=AUTO)\n",
    "        else:\n",
    "            dataset = dataset.map(self.make_transforms_train, num_parallel_calls=AUTO)\n",
    "        dataset = dataset.map(self.set_shapes, num_parallel_calls=AUTO)\n",
    "        if self.shuffle:\n",
    "            dataset = dataset.shuffle(self.batch_size*50, reshuffle_each_iteration=True)\n",
    "\n",
    "        dataset = dataset.batch(self.batch_size, num_parallel_calls=AUTO, drop_remainder=self.drop_remainder).prefetch(AUTO)\n",
    "        return dataset\n",
    "\n",
    "    def parse_tf_record(self, data):\n",
    "        features_context = {\n",
    "             \"session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "             \"size_session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        if not self.is_val:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        else:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_aid_target\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type_target\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        data_context, data_sequence = tf.io.parse_single_sequence_example(data, context_features=features_context, sequence_features=features_seq)\n",
    "        return data_context, data_sequence\n",
    "\n",
    "    def pad_sequence(self, seq_to_pad, maxlen, return_pad_mask=False, dtype=tf.float32):\n",
    "        length, num_feats = tf.shape(seq_to_pad)[0], tf.shape(seq_to_pad)[-1]\n",
    "        ###\n",
    "        if length < maxlen:\n",
    "            pad = tf.zeros((maxlen - length, num_feats), dtype)\n",
    "            seq = tf.concat([seq_to_pad, pad], axis=0)\n",
    "            pad_mask = tf.concat([tf.ones(tf.shape(seq_to_pad), dtype=seq_to_pad.dtype), \n",
    "                                 pad], axis=0)\n",
    "        else:\n",
    "            seq = seq_to_pad[-maxlen:, :]\n",
    "            pad_mask = tf.ones((maxlen, tf.shape(seq_to_pad)[-1]), dtype=seq_to_pad.dtype)\n",
    "        if return_pad_mask:\n",
    "            return seq, pad_mask\n",
    "        return seq \n",
    "\n",
    "    def make_transforms_val(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding']\n",
    "        seq_items_target_raw, seq_type_target_raw =  dict_sequences['seq_aid_target'], dict_sequences['seq_type_target']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        ###\n",
    "        # Build target\n",
    "        seq_items, seq_target = seq_items, seq_items_target_raw[:1] if not self.get_session else seq_items_target_raw[:self.seq_len_target]\n",
    "        seq_type, seq_type_target = seq_type, seq_type_target_raw[:1] if not self.get_session else seq_type_target_raw[:self.seq_len_target]\n",
    "        seq_time_encoding, seq_time_encoding_target = seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)\n",
    "        seq_items_target = tf.concat([seq_items, seq_target], axis=0)\n",
    "        seq_type_target = tf.concat([seq_type, seq_type_target], axis=0)\n",
    "        ###\n",
    "        #Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, seq_type_target[:1]], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_items_target = self.pad_sequence(seq_items_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "        seq_type_target = self.pad_sequence(seq_type_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)\n",
    "        \n",
    "        if self.get_session:\n",
    "            seq_items_target_all = self.pad_sequence(seq_items_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "            seq_type_target_all = self.pad_sequence(seq_type_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64) \n",
    "            return (seq_items, seq_type, seq_time_encoding), (seq_items_target_all[:, 0], seq_type_target_all[:, 0]), session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding), seq_items_target[:, 0]\n",
    "\n",
    "    def make_transforms_test(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        ###\n",
    "        seq_items = seq_items[-self.seq_len:, :]\n",
    "        seq_type = seq_type[-self.seq_len:, :]\n",
    "        seq_time_encoding = seq_time_encoding[-self.seq_len:, :]\n",
    "        #Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, tf.zeros((1, tf.shape(seq_type)[1]), tf.int64)], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)   \n",
    "\n",
    "        if self.get_session:\n",
    "            return (seq_items, seq_type, seq_time_encoding), tf.zeros(tf.shape(seq_items)), session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding), tf.zeros(tf.shape(seq_items))\n",
    "\n",
    "  \n",
    "    def make_transforms_train(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding']\n",
    "        qt_size_seq = dict_context['size_session']\n",
    "        ### \n",
    "        # With prob reverse\n",
    "        if tf.random.uniform(shape=(1,1)) <= self.reverse_prob:\n",
    "            seq_items = tf.reverse(seq_items, axis=[0])\n",
    "            seq_type = tf.reverse(seq_type, axis=[0])\n",
    "            seq_time_encoding = tf.reverse(seq_time_encoding, axis=[0])\n",
    "            \n",
    "        # If our seq is longer than seq_len we can use it for data augmentation purpose \n",
    "        # and select a random idx to begin with.\n",
    "        if tf.shape(seq_items)[0] > self.seq_len:\n",
    "            idx_list = tf.range(tf.shape(seq_items)[0]-self.seq_len) \n",
    "            rand_idx = tf.random.shuffle(idx_list)[0]\n",
    "            seq_items = seq_items[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_type = seq_type[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_time_encoding = seq_time_encoding[rand_idx:(rand_idx+self.seq_len), :]\n",
    "        \n",
    "        qt_size_seq = tf.shape(seq_items)[0]\n",
    "\n",
    "        ## Get idxs to mask for inputs and targets\n",
    "        probs = tf.random.uniform(shape=(qt_size_seq,), minval=0, maxval=1)\n",
    "        idxs_inputs = tf.cast(tf.where(probs >= (1-self.mask_prob)), tf.int64) # -> we mask to zero the inputs as we dont want to leak \n",
    "        idxs_target = tf.cast(tf.where(probs < (1-self.mask_prob)), tf.int64) # -> we mask to zero the targets as the loss will only be applied on non zero\n",
    "\n",
    "        # If all items are masked we leave an item unmasked\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.cast(qt_size_seq, tf.int64):\n",
    "            idxs_target = idxs_inputs[-1:]\n",
    "            idxs_inputs = idxs_inputs[:-1]\n",
    "            \n",
    "        # If no item has been masked we leave at least one item masked(be careful of size=1 seqs)\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.constant(0, dtype=tf.int64):\n",
    "            all_idxs = tf.cast(tf.random.shuffle(tf.range(0, qt_size_seq)), dtype=tf.int64)\n",
    "            idxs_inputs = all_idxs[:1][:, tf.newaxis]\n",
    "            idxs_target = all_idxs[1:][:, tf.newaxis]\n",
    "\n",
    "        # Mask inputs and targets\n",
    "        seq_items_raw = seq_items\n",
    "        updates_items = tf.zeros((len(idxs_inputs), seq_items.shape[-1]), tf.int64)\n",
    "        # updates_type = tf.zeros((len(idxs_inputs), seq_type.shape[-1]), tf.int64)\n",
    "        updates_time_encoding = tf.zeros((len(idxs_inputs), seq_time_encoding.shape[-1]), tf.float32)\n",
    "        updates_target = tf.zeros((len(idxs_target), seq_items_raw.shape[-1]), tf.int64)\n",
    "        \n",
    "        seq_items = tf.tensor_scatter_nd_update(seq_items, idxs_inputs, updates_items)\n",
    "        # seq_type = tf.tensor_scatter_nd_update(seq_type, idxs_inputs, updates_type)\n",
    "        seq_time_encoding = tf.tensor_scatter_nd_update(seq_time_encoding, idxs_inputs, updates_time_encoding)\n",
    "        seq_target = tf.tensor_scatter_nd_update(seq_items_raw, idxs_target, updates_target)\n",
    "        \n",
    "        # Padding\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_target = self.pad_sequence(seq_target, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)  \n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding), seq_target[:, 0]\n",
    "  \n",
    "  \n",
    "    def set_shapes(self, features, targets=None, session=None):\n",
    "        features[0].set_shape((self.seq_len, 1))\n",
    "        features[1].set_shape((self.seq_len, 1))\n",
    "        features[2].set_shape((self.seq_len, 8))\n",
    "        if self.get_session:\n",
    "            return features, targets, session\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([32, 10, 1]), TensorShape([32, 10, 1]), TensorShape([32, 10, 8])]\n",
      "[ 863885 1166894  591718       0 1078084       0       0       0       0\n",
      "       0]\n",
      "[1 1 1 1 1 1 0 0 0 0]\n",
      "[     0      0      0 987942      0 432286      0      0      0      0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_paths = ['../tfrecords/tfrecords_v0.3/na_split=train/' + x for x in os.listdir('../tfrecords/tfrecords_v0.3/na_split=train')]\n",
    "\n",
    "dataloader = Bert4RecDataLoader(list_paths, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=10, \n",
    "                                     seq_len_target=None,\n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.4, \n",
    "                                     reverse_prob=0.2, \n",
    "                                     get_session=False,\n",
    "                                     is_val=False,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "# # Train\n",
    "for batch in tqdm(dataloader):\n",
    "    features, target = batch\n",
    "    seq_items, seq_type, seq_time = features\n",
    "    break\n",
    "\n",
    "# # # Test\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, target, session = batch\n",
    "#     seq_items, seq_type, seq_time = features\n",
    "#     break\n",
    "\n",
    "# Val\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, targets, session = batch\n",
    "#     seq_items, seq_type, seq_time = features\n",
    "#     target, type_target = targets\n",
    "#     break\n",
    "\n",
    "print([x.shape for x in features])\n",
    "\n",
    "idx = 15\n",
    "print(seq_items[idx].numpy().flatten())\n",
    "print(seq_type[idx].numpy().flatten())\n",
    "print(target[idx].numpy().flatten())\n",
    "# print(type_target[idx].numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransposed(tf.keras.layers.Layer):\n",
    "    def __init__(self, tied_to=None, activation=None, **kwargs):\n",
    "        super(EmbeddingTransposed, self).__init__(**kwargs)\n",
    "        self.tied_to = tied_to\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.custom_weights = self.tied_to.weights[0]\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.tied_to.weights[0].shape[0]\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        output = tf.keras.backend.dot(inputs, tf.keras.backend.transpose(self.custom_weights))\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'activation': tf.keras.activations.serialize(self.activation)}\n",
    "        base_config = super(EmbeddingTransposed, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class EncoderTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, attention_axes=None, drop_rate=0.1, att_drop_rate=0.1):\n",
    "        super(EncoderTransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes, dropout=att_drop_rate)\n",
    "        self.ffn = tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation='gelu'), \n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-7)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-7)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, query, key, training, attention_mask=None):\n",
    "        attn_output = self.att(query, key, attention_mask=attention_mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "      \n",
    "                 \n",
    "class ModelBert4Rec(tf.keras.models.Model):\n",
    "    def __init__(self, num_items, model_cfg):\n",
    "        super(ModelBert4Rec, self).__init__()\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        self.num_items = num_items\n",
    "        self.model_cfg = model_cfg\n",
    "        self.embed_items = tf.keras.layers.Embedding(\n",
    "            num_items, model_cfg.emb_dim, \n",
    "            # embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=0.02)\n",
    "        )\n",
    "        self.embed_type = tf.keras.layers.Embedding(3+1, model_cfg.emb_dim)\n",
    "        self.mlp_proj_encoding = tf.keras.models.Sequential([\n",
    "           tf.keras.layers.Dropout(model_cfg.drop_rate), \n",
    "           tf.keras.layers.Dense(model_cfg.trf_dim),\n",
    "           tf.keras.layers.LayerNormalization(epsilon=1e-7)\n",
    "        ])\n",
    "        self.list_transformer_block = [EncoderTransformerBlock(model_cfg.trf_dim, model_cfg.num_heads, \n",
    "                                                               model_cfg.ff_dim, attention_axes=None, \n",
    "                                                               drop_rate=model_cfg.drop_rate, \n",
    "                                                               att_drop_rate=model_cfg.att_drop_rate) \n",
    "                                       for _ in range(model_cfg.num_layers)]\n",
    "        # policy = mixed_precision.Policy('float32')\n",
    "        self.pred_layer = EmbeddingTransposed(tied_to=self.embed_items, activation='linear', dtype='float32')\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        x_seq_past, x_seq_type, x_seq_encoding = inputs\n",
    "        pad_mask = tf.cast(tf.where(tf.equal(x_seq_type, 0), 0, 1), tf.float32)\n",
    "        ###########\n",
    "        x_seq_past_items = self.embed_items(x_seq_past[:, :, 0])\n",
    "        x_seq_past_type = self.embed_type(x_seq_type[:, :, 0])\n",
    "        x_seq_time_encoding = self.mlp_proj_encoding(x_seq_encoding, training=training)\n",
    "        x_ones = tf.ones(tf.shape(x_seq_past_items))\n",
    "        ########### \n",
    "        x = x_seq_past_items * (x_ones + x_seq_time_encoding + x_seq_past_type)\n",
    "        for i in range(len(self.list_transformer_block)):\n",
    "            x = self.list_transformer_block[i](x, x, training=training, attention_mask=pad_mask)\n",
    "        probs = self.pred_layer(x)\n",
    "        return probs\n",
    "      \n",
    "\n",
    "def build_model_bert4Rec(num_items, model_cfg):\n",
    "    return ModelBert4Rec(num_items, model_cfg)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, weight_decay=None):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_decay_tensor = tf.cast(1. if not weight_decay else weight_decay, tf.float32)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          'd_model': self.d_model,\n",
    "          'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        if self.weight_decay:\n",
    "            return self.weight_decay_tensor * tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "        else:\n",
    "            return tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "    \n",
    "    \n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "def custom_loss_bert4rec(tensor_weights=None):\n",
    "    def loss(y_true, y_pred):\n",
    "        mask = tf.where(y_true >= 1, 1., 0.)\n",
    "        ones = tf.ones(tf.shape(y_true))\n",
    "        y_pred = y_pred\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_true, y_pred)\n",
    "        if tensor_weights is not None:\n",
    "            weights = tf.gather(params=tensor_weights, indices=y_true)\n",
    "            return tf.reduce_sum(loss * weights * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "        else:\n",
    "            return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "    loss.__name__ = f'loss_bert4rec'\n",
    "    return loss\n",
    "\n",
    "\n",
    "def mrr_topk_categorical(top_k):\n",
    "  \"\"\"\n",
    "  Mrr Topk Categorical metric\n",
    "  \"\"\"\n",
    "  def mrr(y_true, y_pred):                                      \n",
    "    n_samples = tf.shape(y_true)[0]\n",
    "    n_samples_mask = tf.where(tf.reduce_sum(y_true, -1) >= 1, 1., 0.)\n",
    "    _, top_index = tf.nn.top_k(y_pred, top_k)  \n",
    "    result = tf.constant(0.0)\n",
    "    top_index = tf.cast(top_index, tf.float32)\n",
    "    idxs_not_masked = tf.cast(tf.argmax(y_true, axis=-1), tf.int32)\n",
    "    for i in tf.range(n_samples):\n",
    "        ranked_indicies = tf.where(tf.equal(top_index[i, idxs_not_masked[i], :], y_true[i, :][:, tf.newaxis]))\n",
    "        if tf.shape(ranked_indicies)[0] > 0:\n",
    "            ranked_indicies = tf.cast(ranked_indicies[0], tf.int32)\n",
    "            #check that the prediction its not padding\n",
    "            if top_index[i, ranked_indicies[0], ranked_indicies[1]] != 0.0: \n",
    "                rr = tf.cast(1/(ranked_indicies[1]+1), tf.float32)\n",
    "            else:\n",
    "                rr = tf.constant(0.0)\n",
    "        else:\n",
    "            rr = tf.constant(0.0)\n",
    "        result+=rr\n",
    "    return result/(tf.reduce_sum(n_samples_mask) + 1e-8)\n",
    "  mrr.__name__ = f'mrr_{top_k}_categorical'\n",
    "  return mrr\n",
    "\n",
    "def recall_top_k(top_k=1):\n",
    "    def recall(y_true, y_pred):\n",
    "        n_samples = tf.shape(y_true)[0]\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), tf.float32)\n",
    "        _, top_index = tf.nn.top_k(y_pred, top_k) \n",
    "        top_index = tf.cast(top_index, tf.float32)\n",
    "        cum_sum = tf.zeros(n_samples)\n",
    "        for i in tf.range(top_k):\n",
    "            indexes_i = top_index[:, :, i]\n",
    "            is_true = tf.reduce_sum(tf.cast(tf.equal(y_true, indexes_i), tf.float32), axis=-1)/tf.reduce_sum(mask, -1)\n",
    "            cum_sum += (is_true/tf.cast(i+1, tf.float32))\n",
    "        return tf.reduce_mean(cum_sum)\n",
    "    recall.__name__ = f'recall_{top_k}'\n",
    "    return recall\n",
    "\n",
    "def create_folder_with_version(base_name, checkpoint_path):\n",
    "    if os.path.exists(os.path.join(checkpoint_path, base_name)):\n",
    "        version_ = base_name.split('_v')\n",
    "        if not version_ or len(version_)==1:\n",
    "            base_name_no_version = base_name\n",
    "            version_ = '_v1'\n",
    "        else:\n",
    "            base_name_no_version = '_'.join(base_name.split('_v')[:-1])\n",
    "            version_ = f'_v{int(version_[-1])+1}'\n",
    "        base_name = base_name_no_version + version_\n",
    "        return create_folder_with_version(base_name, checkpoint_path)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_path, base_name)\n",
    "        os.mkdir(checkpoint_path)\n",
    "        return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_lr = CustomSchedule(128, 20_000, weight_decay=None)\n",
    "plt.plot(tmp_lr(tf.range(12_000_000 // 64, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "\n",
    "def flat_gradients(grads_or_idx_slices: tf.Tensor) -> tf.Tensor:\n",
    "    '''Convert gradients if it's tf.IndexedSlices.\n",
    "    When computing gradients for operation concerning `tf.gather`, the type of gradients \n",
    "    '''\n",
    "    if type(grads_or_idx_slices) == tf.IndexedSlices:\n",
    "        return tf.scatter_nd(\n",
    "            tf.expand_dims(grads_or_idx_slices.indices, 1),\n",
    "            grads_or_idx_slices.values,\n",
    "            tf.cast(grads_or_idx_slices.dense_shape, tf.int64)\n",
    "        )\n",
    "    return grads_or_idx_slices\n",
    "\n",
    "def backward_optimization(num_grad_steps, global_gradients, step_gradients, step, model, optimizer):\n",
    "    for i, g in enumerate(step_gradients):\n",
    "        step_gradients[i] += flat_gradients(g) / num_grad_steps\n",
    "    global_gradients = global_gradients + step_gradients\n",
    "    if (step + 1) % num_grad_steps == 0:\n",
    "        global_gradients = zip(global_gradients, model.trainable_variables)\n",
    "        optimizer.apply_gradients(global_gradients)\n",
    "        global_gradients = []\n",
    "    return global_gradients\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(*inputs, target, **kwargs):\n",
    "    l_loss = kwargs['loss']\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(*inputs, training=True)\n",
    "        loss = loss_function(target, predictions)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "    # gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    l_loss(loss)\n",
    "    return gradients\n",
    "  \n",
    "@tf.function\n",
    "def test_step(*inputs, target, **kwargs):\n",
    "    l_loss = kwargs['loss']\n",
    "    predictions = model(*inputs, training=False)\n",
    "    loss = loss_function(target, predictions)\n",
    "    l_loss(loss)\n",
    "\n",
    "\n",
    "def metrics_reset_states(*metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "\n",
    "def fancy_printer(loss_tracker, epoch, batch_num, start, step='train', dict_metrics={}, num_epochs=1, **kwargs):\n",
    "    dict_print_metrics = {' '.join(f\"{key}:{value:.4f}\" for key, value in dict_metrics.items())}\n",
    "    if step!='epoch':\n",
    "        printer = f'[{step} Epoch]{epoch + 1}/{num_epochs} [Time]{time.time() - start:.2f} [Batch]{batch_num} [Speed]{((time.time() - start)/max(1, batch_num))*1000:.2f}ms/step '\n",
    "        printer += f'[Loss]{loss_tracker.result():.4f} ' + '[Metrics]' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "    else:\n",
    "        train_loss, val_loss = kwargs['train_loss'], kwargs['val_loss']\n",
    "        print(f'\\nTime taken for epoch {epoch+1}/{num_epochs}: {time.time() - start:.2f} secs')\n",
    "        printer = f'[Epoch]{epoch + 1}/{num_epochs} - [Train Loss]{train_loss.result():.4f} '\n",
    "        printer += f'- [Val Loss]{val_loss.result():.4f} ' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "\n",
    "\n",
    "def log_wandb_metrics(step='train', num_step=0, dict_metrics=None, gradients=None, plot_image=False, **kwargs):\n",
    "    # Scalar metrics\n",
    "    if step=='train' or step=='val':\n",
    "        wandb.log({name : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "    if step=='epoch':\n",
    "        wandb.log({f'epoch_{name}' : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "\n",
    "    # Gradients\n",
    "    if gradients:\n",
    "        wandb.log({\n",
    "            'mean_norm_gradients' : np.mean([tf.norm(x) for x in gradients]), \n",
    "            'max_norm_gradients': np.max([tf.norm(x) for x in gradients])\n",
    "        })\n",
    "\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2060, compute capability 7.5\n",
      "================================================================================\n",
      "Epoch 1\n",
      "[Train Epoch]1/5 [Time]2.26 [Batch]0 [Speed]2255.03ms/step [Loss]14.0784 [Metrics]{'train_loss:14.0784'}\n",
      "[Train Epoch]1/5 [Time]29.08 [Batch]100 [Speed]290.82ms/step [Loss]14.0927 [Metrics]{'train_loss:14.0927'}\n",
      "[Train Epoch]1/5 [Time]55.82 [Batch]200 [Speed]279.09ms/step [Loss]14.0927 [Metrics]{'train_loss:14.0927'}\n",
      "[Train Epoch]1/5 [Time]82.34 [Batch]300 [Speed]274.45ms/step [Loss]14.0918 [Metrics]{'train_loss:14.0918'}\n",
      "[Train Epoch]1/5 [Time]108.85 [Batch]400 [Speed]272.12ms/step [Loss]14.0903 [Metrics]{'train_loss:14.0903'}\n",
      "[Train Epoch]1/5 [Time]135.34 [Batch]500 [Speed]270.68ms/step [Loss]14.0875 [Metrics]{'train_loss:14.0875'}\n",
      "[Train Epoch]1/5 [Time]161.93 [Batch]600 [Speed]269.88ms/step [Loss]14.0839 [Metrics]{'train_loss:14.0839'}\n",
      "[Train Epoch]1/5 [Time]188.63 [Batch]700 [Speed]269.47ms/step [Loss]14.0792 [Metrics]{'train_loss:14.0792'}\n",
      "[Train Epoch]1/5 [Time]215.31 [Batch]800 [Speed]269.14ms/step [Loss]14.0735 [Metrics]{'train_loss:14.0735'}\n",
      "[Train Epoch]1/5 [Time]241.98 [Batch]900 [Speed]268.87ms/step [Loss]14.0672 [Metrics]{'train_loss:14.0672'}\n",
      "[Train Epoch]1/5 [Time]268.64 [Batch]1000 [Speed]268.64ms/step [Loss]14.0597 [Metrics]{'train_loss:14.0597'}\n",
      "[Train Epoch]1/5 [Time]295.32 [Batch]1100 [Speed]268.47ms/step [Loss]14.0510 [Metrics]{'train_loss:14.0510'}\n",
      "[Train Epoch]1/5 [Time]321.99 [Batch]1200 [Speed]268.33ms/step [Loss]14.0415 [Metrics]{'train_loss:14.0415'}\n",
      "[Train Epoch]1/5 [Time]348.59 [Batch]1300 [Speed]268.15ms/step [Loss]14.0307 [Metrics]{'train_loss:14.0307'}\n",
      "[Train Epoch]1/5 [Time]375.12 [Batch]1400 [Speed]267.94ms/step [Loss]14.0191 [Metrics]{'train_loss:14.0191'}\n",
      "[Train Epoch]1/5 [Time]401.61 [Batch]1500 [Speed]267.74ms/step [Loss]14.0066 [Metrics]{'train_loss:14.0066'}\n",
      "[Train Epoch]1/5 [Time]428.07 [Batch]1600 [Speed]267.55ms/step [Loss]13.9931 [Metrics]{'train_loss:13.9931'}\n",
      "[Train Epoch]1/5 [Time]454.52 [Batch]1700 [Speed]267.37ms/step [Loss]13.9783 [Metrics]{'train_loss:13.9783'}\n",
      "[Train Epoch]1/5 [Time]481.02 [Batch]1800 [Speed]267.24ms/step [Loss]13.9631 [Metrics]{'train_loss:13.9631'}\n",
      "[Train Epoch]1/5 [Time]507.69 [Batch]1900 [Speed]267.20ms/step [Loss]13.9463 [Metrics]{'train_loss:13.9463'}\n",
      "[Train Epoch]1/5 [Time]534.35 [Batch]2000 [Speed]267.18ms/step [Loss]13.9292 [Metrics]{'train_loss:13.9292'}\n",
      "[Train Epoch]1/5 [Time]561.02 [Batch]2100 [Speed]267.15ms/step [Loss]13.9115 [Metrics]{'train_loss:13.9115'}\n",
      "[Train Epoch]1/5 [Time]587.69 [Batch]2200 [Speed]267.13ms/step [Loss]13.8930 [Metrics]{'train_loss:13.8930'}\n",
      "[Train Epoch]1/5 [Time]614.35 [Batch]2300 [Speed]267.11ms/step [Loss]13.8733 [Metrics]{'train_loss:13.8733'}\n",
      "[Train Epoch]1/5 [Time]641.02 [Batch]2400 [Speed]267.09ms/step [Loss]13.8531 [Metrics]{'train_loss:13.8531'}\n",
      "[Train Epoch]1/5 [Time]667.68 [Batch]2500 [Speed]267.07ms/step [Loss]13.8324 [Metrics]{'train_loss:13.8324'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m inputs, target \u001b[39m=\u001b[39m batch_data\n\u001b[0;32m    100\u001b[0m step_gradients \u001b[39m=\u001b[39m train_step(inputs, target\u001b[39m=\u001b[39mtarget, loss\u001b[39m=\u001b[39mtrain_loss)\n\u001b[1;32m--> 101\u001b[0m global_gradients \u001b[39m=\u001b[39m backward_optimization(BERT4REC_CONFIG\u001b[39m.\u001b[39;49mnum_grad_accum_steps, global_gradients, step_gradients, total_step, model, optimizer)\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m batch_num \u001b[39m%\u001b[39m BERT4REC_CONFIG\u001b[39m.\u001b[39mbatch_num_printer_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    103\u001b[0m     train_dict_metrics \u001b[39m=\u001b[39m {x\u001b[39m.\u001b[39mname : x\u001b[39m.\u001b[39mresult() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [train_loss]}\n",
      "Cell \u001b[1;32mIn [8], line 17\u001b[0m, in \u001b[0;36mbackward_optimization\u001b[1;34m(num_grad_steps, global_gradients, step_gradients, step, model, optimizer)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_optimization\u001b[39m(num_grad_steps, global_gradients, step_gradients, step, model, optimizer):\n\u001b[0;32m     16\u001b[0m     \u001b[39mfor\u001b[39;00m i, g \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(step_gradients):\n\u001b[1;32m---> 17\u001b[0m         step_gradients[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m flat_gradients(g) \u001b[39m/\u001b[39m num_grad_steps\n\u001b[0;32m     18\u001b[0m     global_gradients \u001b[39m=\u001b[39m global_gradients \u001b[39m+\u001b[39m step_gradients\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m num_grad_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "class BERT4REC_CONFIG:\n",
    "    num_items = NUM_ITEMS\n",
    "    path_tfrecords = '../tfrecords/tfrecords_v0.3/'\n",
    "    restore_last_chekpoint = (False, 'model_bert4rec_complete_v0.4_finetuned')\n",
    "    model_name = 'model_bert4rec_complete_0.4.1'\n",
    "    checkpoint_filepath = f'../2_Models/'\n",
    "    num_records_dataset = 12_000_000\n",
    "    batch_size = 64\n",
    "    num_grad_accum_steps = 2\n",
    "    seq_len = 10\n",
    "    mask_prob = 0.35\n",
    "    reverse_prob = 0.25\n",
    "    emb_dim = 16\n",
    "    trf_dim = 16\n",
    "    num_heads = 2\n",
    "    num_layers = 1\n",
    "    ff_dim = trf_dim*4\n",
    "    drop_rate = 0.1\n",
    "    att_drop_rate = 0.1\n",
    "    epochs = 5\n",
    "    early_stopping = 5\n",
    "    batch_num_printer_train = 100\n",
    "    batch_num_printer_val = 100\n",
    "    clipnorm = 1\n",
    "    num_iters_save_checkpoint = 10_000\n",
    "    scheduler_scaler = 128 \n",
    "    warmup_steps = 20_000\n",
    "    log_wandb = False\n",
    "    \n",
    "\n",
    "list_paths_train = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=train/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=train')]\n",
    "np.random.shuffle(list_paths_train)\n",
    "list_paths_val = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=val/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=val')]\n",
    "\n",
    "train_dataloader = Bert4RecDataLoader(list_paths_train, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len, \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=BERT4REC_CONFIG.mask_prob, \n",
    "                                     reverse_prob=BERT4REC_CONFIG.reverse_prob, \n",
    "                                     is_test=False,\n",
    "                                     is_val=False,\n",
    "                                     shuffle=True,\n",
    "                                     drop_remainder=True).get_generator()\n",
    "\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len,  \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     get_session=False,\n",
    "                                     is_val=True,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "# model = tf.keras.models.load_model(f'../2_Models/seq_len{BERT4REC_CONFIG.seq_len}_{BERT4REC_CONFIG.load_model_name}/', compile=False)\n",
    "optimizer = optimizers.Adam(learning_rate=CustomSchedule(BERT4REC_CONFIG.scheduler_scaler, \n",
    "                            warmup_steps=BERT4REC_CONFIG.warmup_steps),\n",
    "                            clipnorm=BERT4REC_CONFIG.clipnorm)\n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)                           \n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "                            \n",
    "# Build utils\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "checkpoint_path = create_folder_with_version(BERT4REC_CONFIG.model_name, BERT4REC_CONFIG.checkpoint_filepath)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, os.path.join(BERT4REC_CONFIG.checkpoint_filepath, checkpoint_path, 'checkpoints'), \n",
    "                                          max_to_keep=10)\n",
    "if BERT4REC_CONFIG.restore_last_chekpoint[0]:\n",
    "    ckpt.restore(BERT4REC_CONFIG.restore_last_chekpoint[1])\n",
    "    print('Latest checkpoint restored!!')\n",
    "\n",
    "# Loss function\n",
    "loss_function = custom_loss_bert4rec()\n",
    "\n",
    "# Trackers\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "##############################################\n",
    "\n",
    "total_step, val_step = 0, 0\n",
    "global_gradients = []\n",
    "for epoch in range(BERT4REC_CONFIG.epochs):\n",
    "    start = time.time()\n",
    "    print('===='*20)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    metrics_reset_states(train_loss, val_loss)\n",
    "    \n",
    "    for batch_num, batch_data in enumerate(train_dataloader):\n",
    "        inputs, target = batch_data\n",
    "        step_gradients = train_step(inputs, target=target, loss=train_loss)\n",
    "        global_gradients = backward_optimization(BERT4REC_CONFIG.num_grad_accum_steps, global_gradients, step_gradients, total_step, model, optimizer)\n",
    "        if batch_num % BERT4REC_CONFIG.batch_num_printer_train == 0:\n",
    "            train_dict_metrics = {x.name : x.result() for x in [train_loss]}\n",
    "            fancy_printer(train_loss, epoch, batch_num, start, step='Train', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=train_dict_metrics)\n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                log_wandb_metrics(step='train', num_step=total_step, gradients=global_gradients, dict_metrics=train_dict_metrics) \n",
    "        \n",
    "        total_step += 1  \n",
    "\n",
    "        if total_step % BERT4REC_CONFIG.num_iters_save_checkpoint==0:\n",
    "            print(f'Saving checkpoint for epoch {epoch+1} at step {total_step} on path {checkpoint_path}')        \n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            \n",
    "    for val_batch_num, val_batch_data in enumerate(val_dataloader):\n",
    "        inputs, target = val_batch_data\n",
    "        test_step(inputs, target=target, loss=val_loss)\n",
    "        val_step += 1\n",
    "        if val_batch_num % BERT4REC_CONFIG.batch_num_printer_val == 0:\n",
    "            val_dict_metrics = {x.name : x.result() for x in [val_loss]}\n",
    "            fancy_printer(val_loss, epoch, val_batch_num, start, step='Val', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=val_dict_metrics)    \n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                log_wandb_metrics(step='val', num_step=val_step, dict_metrics=val_dict_metrics) \n",
    "                # if val_batch_num==0:\n",
    "                #     log_wandb_metrics(step=None, plot_image=True, \n",
    "                #                       model=model, inputs=inputs, epoch=epoch, target=target, stats=stats)\n",
    "\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {checkpoint_path}')        \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    \n",
    "    epoch_dict_metrics = {x.name : x.result() for x in [train_loss, val_loss]}\n",
    "    printer = fancy_printer(None, epoch, epoch, start, step='epoch', dict_metrics=epoch_dict_metrics, \n",
    "                            train_loss=train_loss, val_loss=val_loss)\n",
    "    if BERT4REC_CONFIG.log_wandb:\n",
    "        log_wandb_metrics(step='epoch', num_step=total_step, dict_metrics=epoch_dict_metrics)\n",
    "\n",
    "if BERT4REC_CONFIG.log_wandb:\n",
    "    # wandb.save(checkpoint_path)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [01:59,  8.34it/s]\n",
      "100%|| 96096/96096 [00:01<00:00, 71928.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.609600e+04</td>\n",
       "      <td>40454.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.358616e+06</td>\n",
       "      <td>0.189249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.721860e+06</td>\n",
       "      <td>0.383285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.200000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.118926e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.336554e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.541742e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.289966e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            session         score\n",
       "count  9.609600e+04  40454.000000\n",
       "mean   6.358616e+06      0.189249\n",
       "std    3.721860e+06      0.383285\n",
       "min    2.200000e+02      0.000000\n",
       "25%    3.118926e+06      0.000000\n",
       "50%    6.336554e+06      0.000000\n",
       "75%    9.541742e+06      0.000000\n",
       "max    1.289966e+07      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'carts': 0.23110458407992615,\n",
       " 'clicks': 0.1673007543456871,\n",
       " 'orders': 0.3166333854597084}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Metric: 0.2760\n"
     ]
    }
   ],
   "source": [
    "def get_score_metric(y_true, y_pred, type_target, k=20):\n",
    "    score = 0\n",
    "    if len(y_true)==0:\n",
    "        return None\n",
    "    if type_target=='clicks':\n",
    "        num_targets = 1\n",
    "        hits = len([x for x in y_pred if x==y_true[0]])\n",
    "    else:\n",
    "        num_targets = min(k, len(y_true))\n",
    "        hits = len([x for x in y_pred if x in y_true])\n",
    "    score = hits / num_targets\n",
    "    return score\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "# model = models.load_model('../2_Models/seq_len10_model_bert4rec_complete_v0.4_finetuned/', compile=False)\n",
    "list_paths_val = ['../tfrecords/tfrecords_v0.3/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.3/na_split=val')]\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=10, \n",
    "                                     seq_len_target=20, \n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0, \n",
    "                                     is_val=True,\n",
    "                                     get_session=True, \n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "list_sessions, list_predictions, list_trues, list_types = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(val_dataloader)):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time = features\n",
    "    target, type_target = targets\n",
    "    idxs = tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[x for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        labels = [list(set([_target for _type, _target in zip(type_target.numpy()[i], target.numpy()[i]) if dict_map_type[type_]==_type and _target!=0])) for i in range(target.shape[0])]\n",
    "        ###\n",
    "        list_sessions.append(session.numpy())\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_trues = list_trues + labels\n",
    "    if num_batch==1_000:\n",
    "        break\n",
    "\n",
    "df_val = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'trues' : list_trues,\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_val['score'] = df_val.progress_apply(lambda x: get_score_metric(x['trues'], x['predictions'], x['type']), axis=1)\n",
    "\n",
    "display(df_val.describe())\n",
    "dict_scores = df_val.groupby('type')['score'].mean().to_dict()\n",
    "display(dict_scores)\n",
    "kaggle_metric = 0.1*dict_scores['clicks'] + 0.3*dict_scores['carts'] + 0.6*dict_scores['orders']\n",
    "print(f'Kaggle Metric: {kaggle_metric:.4f}')\n",
    "\n",
    "# v0.4 seqlen=10\n",
    "# {'carts': 0.222143,\n",
    "#  'clicks': 0.163726,\n",
    "#  'orders': 0.301489}\n",
    "# Kaggle Metric: 0.2639089\n",
    "\n",
    "# v0.4_finetuned seqlen=10\n",
    "# {'carts': 0.23272587826464677,\n",
    "#  'clicks': 0.16818629058707774,\n",
    "#  'orders': 0.31957377011651095}\n",
    "# Kaggle Metric: 0.2783808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "tf.keras.backend.clear_session()\n",
    "model = models.load_model('../2_Models/seq_len10_model_bert4rec_complete_v0.4/', compile=False)\n",
    "\n",
    "\n",
    "list_paths_test = ['../tfrecords/tfrecords_v0.3/na_split=test/' + x for x in os.listdir('../tfrecords/tfrecords_v0.3/na_split=test')]\n",
    "test_dataloader = Bert4RecDataLoader(list_paths_test, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=10,  \n",
    "                                     batch_size=64, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     is_val=False,\n",
    "                                     is_test=True,\n",
    "                                     get_session=True,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "list_predictions, list_sessions, list_types, list_scores = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(test_dataloader)):\n",
    "    features, target, session = batch\n",
    "    seq_items, seq_type, seq_time = features\n",
    "    idxs = tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    ###\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[dict_map[x] for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        topk_idxs = topk_idxs - 1\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_sessions.append(session.numpy())\n",
    "    # if num_batch==100:\n",
    "    #     break\n",
    "    \n",
    "\n",
    "# 52244it [2:47:45,  5.19it/s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_submission = f\"submission_{datetime.now().__str__().split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_')}\"\n",
    "\n",
    "df_inference = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_inference['session_type'] = df_inference['session'].astype(str) + '_' + df_inference['type']\n",
    "df_inference['labels'] = df_inference['predictions'].apply(lambda x : ' '.join([str(y) for y in x]))\n",
    "df_inference[['session_type', 'labels']].to_csv(f'../3_Submissions/{name_submission}.csv', index=False)\n",
    "\n",
    "print(df_inference.shape)\n",
    "display(\n",
    "    df_inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c4b929e2472036a63dc2b4145b104daea13432f82a7dbc65e279332da4f8b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
