{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 19:45:19.404027: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 19:45:19.469938: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-24 19:45:19.485732: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-24 19:45:19.758203: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-11-24 19:45:19.758232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-11-24 19:45:19.758235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 19:45:20.058264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:45:20.071631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:45:20.071731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Libraries #\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, models, metrics, optimizers, constraints\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import Sequence\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# tfrecords for kaggle\n",
    "\n",
    "# name_dataset = 'tfrecords_v0.4_kaggle'\n",
    "# path_out = f'../tfrecords/{name_dataset}/'\n",
    "\n",
    "# if not os.path.exists(path_out):\n",
    "#     os.mkdir(path_out)\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_train'):\n",
    "#     os.rename(path_out + 'na_split_train/' + file, \n",
    "#               path_out + 'na_split_train/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_val'):\n",
    "#     os.rename(path_out + 'na_split_val/' + file, \n",
    "#               path_out + 'na_split_val/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_test'):\n",
    "#     os.rename(path_out + 'na_split_test/' + file, \n",
    "#               path_out + 'na_split_test/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_val_aug'):\n",
    "#     os.rename(path_out + 'na_split_val_aug/' + file, \n",
    "#               path_out + 'na_split_val_aug/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_test_aug'):\n",
    "#     os.rename(path_out + 'na_split_test_aug/' + file, \n",
    "#               path_out + 'na_split_test_aug/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1855603/1855603 [00:00<00:00, 8156985.33it/s]\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Paths & Global Variables\n",
    "\n",
    "# Train: (datetime.datetime(2022, 7, 31, 22, 0, 0, 25000), datetime.datetime(2022, 8, 28, 21, 59, 59, 984000))\n",
    "# Test: (datetime.datetime(2022, 8, 28, 22, 0, 0, 278000), datetime.datetime(2022, 9, 4, 21, 59, 51, 563000))\n",
    "\n",
    "path_data_raw = '../0_Data/'\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "df_mapping = pd.read_csv('../tfrecords/tfrecords_v0.4/df_mapping.csv')\n",
    "NUM_ITEMS = len(df_mapping['aid_map'].unique())\n",
    "print(NUM_ITEMS)\n",
    "\n",
    "dict_map = {}\n",
    "for x in tqdm(df_mapping.to_dict('records')):\n",
    "    dict_map[x['aid_map']] = x['aid']\n",
    "\n",
    "dict_map_type = {\n",
    "    'clicks' : 1,\n",
    "    'carts' : 2,\n",
    "    'orders' : 3\n",
    "  }\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert4RecDataLoader:\n",
    "    \"\"\"\n",
    "    Class that iterates over tfrecords in order to get the sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_paths, num_items, seq_len, batch_size, num_targets=-1, mask_prob=0.4, \n",
    "                 reverse_prob=0.2, get_session=False, get_only_first_on_val=False, seq_len_target=None,\n",
    "                 min_size_seq_to_mask=2, is_val=False, is_test=False, avoid_repeats=False, shuffle=False, drop_remainder=False):\n",
    "        self.list_paths = list_paths\n",
    "        self.num_items = num_items\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_targets = num_targets\n",
    "        self.mask_prob = mask_prob\n",
    "        self.reverse_prob = tf.constant(reverse_prob)\n",
    "        self.shuffle = shuffle\n",
    "        self.min_size_seq_to_mask = min_size_seq_to_mask\n",
    "        self.avoid_repeats = avoid_repeats\n",
    "        self.get_session = get_session\n",
    "        self.seq_len_target = seq_len if not seq_len_target else seq_len_target\n",
    "        self.get_only_first_on_val = get_only_first_on_val\n",
    "        self.is_val = is_val\n",
    "        self.is_test = is_test\n",
    "        self.drop_remainder = drop_remainder\n",
    "\n",
    "    def get_generator(self):\n",
    "        dataset = tf.data.TFRecordDataset(self.list_paths, num_parallel_reads=AUTO, compression_type='GZIP')\n",
    "        dataset = dataset.map(self.parse_tf_record, num_parallel_calls=AUTO)\n",
    "        if self.is_val:\n",
    "            dataset = dataset.map(self.make_transforms_val, num_parallel_calls=AUTO)\n",
    "        elif self.is_test:\n",
    "            dataset = dataset.map(self.make_transforms_test, num_parallel_calls=AUTO)\n",
    "        else:\n",
    "            dataset = dataset.map(self.make_transforms_train, num_parallel_calls=AUTO)\n",
    "        \n",
    "        dataset = dataset.map(self.set_shapes, num_parallel_calls=AUTO)\n",
    "        # dataset = dataset.map(self.normalize_features, num_parallel_calls=AUTO)\n",
    "        if self.shuffle:\n",
    "            dataset = dataset.shuffle(self.batch_size*50, reshuffle_each_iteration=True)\n",
    "\n",
    "        dataset = dataset.batch(self.batch_size, num_parallel_calls=AUTO, drop_remainder=self.drop_remainder).prefetch(AUTO)\n",
    "        return dataset\n",
    "\n",
    "    def parse_tf_record(self, data):\n",
    "        features_context = {\n",
    "             \"session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "             \"size_session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        if not self.is_val:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False),\n",
    "                \"seq_recency_aid\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        else:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_aid_target\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type_target\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False),\n",
    "                \"seq_recency_aid\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        data_context, data_sequence = tf.io.parse_single_sequence_example(data, context_features=features_context, sequence_features=features_seq)\n",
    "        return data_context, data_sequence\n",
    "\n",
    "    def pad_sequence(self, seq_to_pad, maxlen, return_pad_mask=False, dtype=tf.float32):\n",
    "        length, num_feats = tf.shape(seq_to_pad)[0], tf.shape(seq_to_pad)[-1]\n",
    "        ###\n",
    "        if length < maxlen:\n",
    "            pad = tf.zeros((maxlen - length, num_feats), dtype)\n",
    "            seq = tf.concat([seq_to_pad, pad], axis=0)\n",
    "            pad_mask = tf.concat([tf.ones(tf.shape(seq_to_pad), dtype=seq_to_pad.dtype), \n",
    "                                 pad], axis=0)\n",
    "        else:\n",
    "            seq = seq_to_pad[-maxlen:, :]\n",
    "            pad_mask = tf.ones((maxlen, tf.shape(seq_to_pad)[-1]), dtype=seq_to_pad.dtype)\n",
    "        if return_pad_mask:\n",
    "            return seq, pad_mask\n",
    "        return seq \n",
    "\n",
    "    def make_transforms_val(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        seq_items_target_raw, seq_type_target_raw =  dict_sequences['seq_aid_target'], dict_sequences['seq_type_target']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ###\n",
    "        # Build target\n",
    "        seq_items, seq_target = seq_items, seq_items_target_raw[:1] if not self.get_session else seq_items_target_raw[:self.seq_len_target]\n",
    "        seq_type, seq_type_target = seq_type, seq_type_target_raw[:1] if not self.get_session else seq_type_target_raw[:self.seq_len_target]\n",
    "        seq_items_target = tf.concat([seq_items, seq_target], axis=0)\n",
    "        seq_type_target = tf.concat([seq_type, seq_type_target], axis=0)\n",
    "        ###\n",
    "        #Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, seq_type_target[:1]], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        seq_recency = tf.concat([seq_recency, tf.zeros((1, tf.shape(seq_recency)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        idx_masked = tf.clip_by_value(tf.shape(seq_items)[0], 0, self.seq_len-1)\n",
    "        seq_items, _ = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_items_target = self.pad_sequence(seq_items_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "        seq_type_target = self.pad_sequence(seq_type_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)\n",
    "        \n",
    "        if self.get_session:\n",
    "            seq_items_target_all = self.pad_sequence(seq_items_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "            seq_type_target_all = self.pad_sequence(seq_type_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64) \n",
    "            return (seq_items, seq_type, seq_time_encoding, seq_recency), (seq_items_target_all[:, 0], seq_type_target_all[:, 0], idx_masked), session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), seq_items_target[:, 0]\n",
    "\n",
    "    def make_transforms_test(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ###\n",
    "        seq_items = seq_items[-self.seq_len:, :]\n",
    "        seq_type = seq_type[-self.seq_len:, :]\n",
    "        seq_time_encoding = seq_time_encoding[-self.seq_len:, :]\n",
    "        seq_recency = seq_recency[-self.seq_len:, :]\n",
    "        idx_masked = tf.clip_by_value(tf.shape(seq_items)[0], 0, self.seq_len-1)\n",
    "        # Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, tf.zeros((1, tf.shape(seq_type)[1]), tf.int64)], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        seq_recency = tf.concat([seq_recency, tf.zeros((1, tf.shape(seq_recency)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        seq_items, _ = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)   \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)   \n",
    "        if self.get_session:\n",
    "            return (seq_items, seq_type, seq_time_encoding, seq_recency), idx_masked, session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), idx_masked\n",
    "\n",
    "  \n",
    "    def make_transforms_train(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        qt_size_seq = dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ### \n",
    "        # With prob reverse\n",
    "        if tf.random.uniform(shape=(1,1)) <= self.reverse_prob:\n",
    "            seq_items = tf.reverse(seq_items, axis=[0])\n",
    "            seq_type = tf.reverse(seq_type, axis=[0])\n",
    "            seq_time_encoding = tf.reverse(seq_time_encoding, axis=[0])\n",
    "            seq_recency = tf.reverse(seq_recency, axis=[0])\n",
    "            \n",
    "        # If our seq is longer than seq_len we can use it for data augmentation purpose \n",
    "        # and select a random idx to begin with.\n",
    "        if tf.shape(seq_items)[0] > self.seq_len:\n",
    "            idx_list = tf.range(tf.shape(seq_items)[0]-self.seq_len) \n",
    "            rand_idx = tf.random.shuffle(idx_list)[0]\n",
    "            seq_items = seq_items[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_type = seq_type[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_time_encoding = seq_time_encoding[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_recency = seq_recency[rand_idx:(rand_idx+self.seq_len), :]\n",
    "        \n",
    "        qt_size_seq = tf.shape(seq_items)[0]\n",
    "\n",
    "        ## Get idxs to mask for inputs and targets\n",
    "        probs = tf.random.uniform(shape=(qt_size_seq,), minval=0, maxval=1)\n",
    "        idxs_inputs = tf.cast(tf.where(probs >= (1-self.mask_prob)), tf.int64) # -> we mask to zero the inputs as we dont want to leak \n",
    "        idxs_target = tf.cast(tf.where(probs < (1-self.mask_prob)), tf.int64) # -> we mask to zero the targets as the loss will only be applied on non zero\n",
    "\n",
    "        # If all items are masked we leave an item unmasked\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.cast(qt_size_seq, tf.int64):\n",
    "            idxs_target = idxs_inputs[-1:]\n",
    "            idxs_inputs = idxs_inputs[:-1]\n",
    "            \n",
    "        # If no item has been masked we leave at least one item masked(be careful of size=1 seqs)\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.constant(0, dtype=tf.int64):\n",
    "            all_idxs = tf.cast(tf.random.shuffle(tf.range(0, qt_size_seq)), dtype=tf.int64)\n",
    "            idxs_inputs = all_idxs[:1][:, tf.newaxis]\n",
    "            idxs_target = all_idxs[1:][:, tf.newaxis]\n",
    "\n",
    "        # Mask inputs and targets\n",
    "        seq_items_raw = seq_items\n",
    "        updates_items = tf.zeros((len(idxs_inputs), seq_items.shape[-1]), tf.int64)\n",
    "        # updates_type = tf.zeros((len(idxs_inputs), seq_type.shape[-1]), tf.int64)\n",
    "        updates_time_encoding = tf.zeros((len(idxs_inputs), seq_time_encoding.shape[-1]), tf.float32)\n",
    "        updates_recency = tf.zeros((len(idxs_inputs), seq_recency.shape[-1]), tf.float32)\n",
    "        updates_target = tf.zeros((len(idxs_target), seq_items_raw.shape[-1]), tf.int64)\n",
    "        \n",
    "        seq_items = tf.tensor_scatter_nd_update(seq_items, idxs_inputs, updates_items)\n",
    "        # seq_type = tf.tensor_scatter_nd_update(seq_type, idxs_inputs, updates_type)\n",
    "        seq_time_encoding = tf.tensor_scatter_nd_update(seq_time_encoding, idxs_inputs, updates_time_encoding)\n",
    "        seq_recency = tf.tensor_scatter_nd_update(seq_recency, idxs_inputs, updates_recency)\n",
    "        seq_target = tf.tensor_scatter_nd_update(seq_items_raw, idxs_target, updates_target)\n",
    "        \n",
    "        # Padding\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32) \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_target = self.pad_sequence(seq_target, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)  \n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), seq_target[:, 0]\n",
    "  \n",
    "    def normalize_features(self, features):\n",
    "        return (features - tf.constant(5.45)/tf.constant(1.09))\n",
    "\n",
    "    # def normalize_features(self, features, targets=None, session=None):\n",
    "    #     seq_items, seq_type, seq_time_encoding, seq_recency = features\n",
    "    #     seq_recency = (seq_recency - tf.constant(5.45)/tf.constant(1.09))\n",
    "    #     features = (seq_items, seq_type, seq_time_encoding, seq_recency)\n",
    "    #     return features, targets, session\n",
    "\n",
    "    def set_shapes(self, features, targets=None, session=None):\n",
    "        features[0].set_shape((self.seq_len, 1))\n",
    "        features[1].set_shape((self.seq_len, 1))\n",
    "        features[2].set_shape((self.seq_len, 8))\n",
    "        features[3].set_shape((self.seq_len, 1))\n",
    "        if self.get_session:\n",
    "            return features, targets, session\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 19:45:21.781148: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 19:45:21.782048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:45:21.782143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:45:21.782185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:45:22.031591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:45:22.031679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:45:22.031726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-24 19:45:22.031772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21927 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([32, 20, 1]), TensorShape([32, 20, 1]), TensorShape([32, 20, 8]), TensorShape([32, 20, 1])]\n",
      "[      0  505110  526989 1159498  400243       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0]\n",
      "[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 171550       0       0       0       0 1212417       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_paths = ['../tfrecords/tfrecords_v0.4/na_split=train/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=train')]\n",
    "# 5,45, 1,09\n",
    "dataloader = Bert4RecDataLoader(list_paths, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20, \n",
    "                                     seq_len_target=None,\n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.4, \n",
    "                                     reverse_prob=0.25, \n",
    "                                     get_session=False,\n",
    "                                     is_val=False,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "# Train\n",
    "for batch in tqdm(dataloader):\n",
    "    features, target = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    break\n",
    "\n",
    "# # Test\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, target, session = batch\n",
    "#     seq_items, seq_type, seq_time, seq_recency = features\n",
    "#     idx_mask = target\n",
    "#     break\n",
    "\n",
    "# Val\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, targets, session = batch\n",
    "#     seq_items, seq_type, seq_time, seq_recency = features\n",
    "#     target, type_target, idx_mask = targets\n",
    "#     break\n",
    "\n",
    "print([x.shape for x in features])\n",
    "\n",
    "idx = 2\n",
    "print(seq_items[idx].numpy().flatten())\n",
    "print(seq_type[idx].numpy().flatten())\n",
    "print(target[idx].numpy().flatten())\n",
    "# print(idx_mask[idx].numpy().flatten())\n",
    "# print(type_target[idx].numpy().flatten())\n",
    "\n",
    "del features, target, seq_items, seq_type, seq_time, seq_recency\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransposed(tf.keras.layers.Layer):\n",
    "    def __init__(self, tied_to=None, activation=None, **kwargs):\n",
    "        super(EmbeddingTransposed, self).__init__(**kwargs)\n",
    "        self.tied_to = tied_to\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.custom_weights = self.tied_to.weights[0]\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.tied_to.weights[0].shape[0]\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        output = tf.keras.backend.dot(inputs, tf.keras.backend.transpose(self.custom_weights))\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'activation': tf.keras.activations.serialize(self.activation)}\n",
    "        base_config = super(EmbeddingTransposed, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class EncoderTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, attention_axes=None, drop_rate=0.1, att_drop_rate=0.1):\n",
    "        super(EncoderTransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes, dropout=att_drop_rate)\n",
    "        self.ffn = tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation='gelu'), \n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, query, key, training, attention_mask=None):\n",
    "        attn_output = self.att(query, key, attention_mask=attention_mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "      \n",
    "                 \n",
    "class ModelBert4Rec(tf.keras.models.Model):\n",
    "    def __init__(self, num_items, model_cfg):\n",
    "        super(ModelBert4Rec, self).__init__()\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        self.num_items = num_items\n",
    "        self.model_cfg = model_cfg\n",
    "        self.std_init = np.sqrt(1/(model_cfg.emb_dim*3)).round(4) #0.02 if model_cfg.trf_dim < 1024 else \n",
    "        self.embed_items = tf.keras.layers.Embedding(\n",
    "            num_items, model_cfg.emb_dim, \n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=self.std_init)\n",
    "        )\n",
    "        self.embed_type = tf.keras.layers.Embedding(\n",
    "            3+1, \n",
    "            model_cfg.emb_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=self.std_init)\n",
    "        )\n",
    "        self.mlp_proj_time_encoding = tf.keras.models.Sequential([\n",
    "           tf.keras.layers.Dropout(model_cfg.drop_rate), \n",
    "           tf.keras.layers.Dense(model_cfg.trf_dim, kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=self.std_init)),\n",
    "           tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        ])\n",
    "        # self.mlp_proj_conts = tf.keras.models.Sequential([\n",
    "        #    tf.keras.layers.Dropout(model_cfg.drop_rate), \n",
    "        #    tf.keras.layers.Dense(model_cfg.trf_dim, kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=self.std_init)),\n",
    "        #    tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        # ])\n",
    "        self.list_transformer_block = [EncoderTransformerBlock(model_cfg.trf_dim, model_cfg.num_heads, \n",
    "                                                               model_cfg.ff_dim, attention_axes=None, \n",
    "                                                               drop_rate=model_cfg.drop_rate, \n",
    "                                                               att_drop_rate=model_cfg.att_drop_rate) \n",
    "                                       for _ in range(model_cfg.num_layers)]\n",
    "        # policy = mixed_precision.Policy('float32')\n",
    "        self.pred_layer = EmbeddingTransposed(tied_to=self.embed_items, activation='linear', dtype='float32')\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        x_seq_past, x_seq_type, x_seq_encoding, x_seq_recency = inputs\n",
    "        pad_mask = tf.cast(tf.where(tf.equal(x_seq_type, 0), 0, 1), tf.float32)\n",
    "        ###########\n",
    "        x_seq_past_items = self.embed_items(x_seq_past[:, :, 0])\n",
    "        x_seq_past_type = self.embed_type(x_seq_type[:, :, 0])\n",
    "        x_seq_time_encoding = self.mlp_proj_time_encoding(x_seq_encoding, training=training)\n",
    "        # x_seq_recency = self.mlp_proj_conts(x_seq_recency, training=training)\n",
    "        x_ones = tf.ones(tf.shape(x_seq_past_items))\n",
    "        ########### \n",
    "        x = x_seq_past_items * (x_ones + x_seq_past_type + x_seq_time_encoding)# + x_seq_recency)\n",
    "        for i in range(len(self.list_transformer_block)):\n",
    "            x = self.list_transformer_block[i](x, x, training=training, attention_mask=pad_mask)\n",
    "        probs = self.pred_layer(x)\n",
    "        return probs\n",
    "      \n",
    "\n",
    "def build_model_bert4Rec(num_items, model_cfg):\n",
    "    return ModelBert4Rec(num_items, model_cfg)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, weight_decay=None):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_decay_tensor = tf.cast(1. if not weight_decay else weight_decay, tf.float32)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          'd_model': self.d_model,\n",
    "          'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        if self.weight_decay:\n",
    "            return self.weight_decay_tensor * tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "        else:\n",
    "            return tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "    \n",
    "    \n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "def custom_loss_bert4rec(tensor_weights=None):\n",
    "    # @tf.function(jit_compile=True)\n",
    "    def loss(y_true, y_pred):\n",
    "        mask = tf.where(y_true >= 1, 1., 0.)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_true, y_pred)\n",
    "        if tensor_weights is not None:\n",
    "            weights = tf.gather(params=tensor_weights, indices=y_true)\n",
    "            return tf.reduce_sum(loss * weights * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "        else:\n",
    "            return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "    loss.__name__ = f'loss_bert4rec'\n",
    "    return loss\n",
    "\n",
    "def weighted_loss_bert4rec():\n",
    "    # @tf.function(jit_compile=True)\n",
    "    def loss(y_true, y_pred, y_type):\n",
    "        mask = tf.where(y_true >= 1, 1., 0.)\n",
    "        y_type = tf.squeeze(y_type, -1)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_true, y_pred)\n",
    "        w_clicks = tf.cast(y_type==1, tf.float32) * 0.1\n",
    "        w_cart = tf.cast(y_type==2, tf.float32) * 0.3\n",
    "        w_order = tf.cast(y_type==3, tf.float32) * 0.6\n",
    "        weights = tf.reduce_max(tf.stack([w_clicks, w_cart, w_order], axis=-1), -1)\n",
    "        return tf.reduce_sum(loss * mask * weights) / (tf.reduce_sum(mask * weights) + 1e-8)\n",
    "    loss.__name__ = f'weighted_loss_bert4rec'\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def custom_accuracy():\n",
    "    # @tf.function(jit_compile=True)\n",
    "    def masked_accuracy(y_true, y_pred):\n",
    "        y_pred = tf.argmax(y_pred, axis=2)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        match = y_true == y_pred\n",
    "        mask = y_true != 0\n",
    "        match = match & mask\n",
    "        match = tf.cast(match, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "    masked_accuracy.__name__ = f'seq_acc'\n",
    "    return masked_accuracy\n",
    "\n",
    "\n",
    "def mrr_topk_categorical(top_k):\n",
    "  \"\"\"\n",
    "  Mrr Topk Categorical metric\n",
    "  \"\"\"\n",
    "  def mrr(y_true, y_pred):                                      \n",
    "    n_samples = tf.shape(y_true)[0]\n",
    "    n_samples_mask = tf.where(tf.reduce_sum(y_true, -1) >= 1, 1., 0.)\n",
    "    _, top_index = tf.nn.top_k(y_pred, top_k)  \n",
    "    result = tf.constant(0.0)\n",
    "    top_index = tf.cast(top_index, tf.float32)\n",
    "    idxs_not_masked = tf.cast(tf.argmax(y_true, axis=-1), tf.int32)\n",
    "    for i in tf.range(n_samples):\n",
    "        ranked_indicies = tf.where(tf.equal(top_index[i, idxs_not_masked[i], :], y_true[i, :][:, tf.newaxis]))\n",
    "        if tf.shape(ranked_indicies)[0] > 0:\n",
    "            ranked_indicies = tf.cast(ranked_indicies[0], tf.int32)\n",
    "            #check that the prediction its not padding\n",
    "            if top_index[i, ranked_indicies[0], ranked_indicies[1]] != 0.0: \n",
    "                rr = tf.cast(1/(ranked_indicies[1]+1), tf.float32)\n",
    "            else:\n",
    "                rr = tf.constant(0.0)\n",
    "        else:\n",
    "            rr = tf.constant(0.0)\n",
    "        result+=rr\n",
    "    return result/(tf.reduce_sum(n_samples_mask) + 1e-8)\n",
    "  mrr.__name__ = f'mrr_{top_k}_categorical'\n",
    "  return mrr\n",
    "\n",
    "\n",
    "def recall_top_k(top_k=1, seq_len=10):\n",
    "    # @tf.function\n",
    "    def recall(y_true, y_pred):\n",
    "        n_samples = tf.shape(y_pred)[0]\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), tf.int32)\n",
    "        _, top_index = tf.nn.top_k(y_pred, top_k) \n",
    "        top_index = tf.cast(top_index, tf.int64)\n",
    "        # cum_sum = tf.zeros(n_samples, tf.int32)\n",
    "        result = tf.constant(0, tf.int32)\n",
    "        for i in tf.range(seq_len):\n",
    "            indexes_i = top_index[:, i, :]\n",
    "            is_true = tf.reduce_sum(tf.reduce_max(tf.where(y_true[:, i:i+1]==indexes_i, 1, 0), -1) * mask[:, i])\n",
    "            result += is_true\n",
    "        return tf.cast(result, tf.float32) / (tf.cast(tf.reduce_sum(mask), tf.float32) + 1e-8)\n",
    "    recall.__name__ = f'recall_{top_k}'\n",
    "    return recall\n",
    "\n",
    "\n",
    "def create_folder_with_version(base_name, checkpoint_path):\n",
    "    if os.path.exists(os.path.join(checkpoint_path, base_name)):\n",
    "        version_ = base_name.split('_v')\n",
    "        if not version_ or len(version_)==1:\n",
    "            base_name_no_version = base_name\n",
    "            version_ = '_v1'\n",
    "        else:\n",
    "            base_name_no_version = '_'.join(base_name.split('_v')[:-1])\n",
    "            version_ = f'_v{int(version_[-1])+1}'\n",
    "        base_name = base_name_no_version + version_\n",
    "        return create_folder_with_version(base_name, checkpoint_path)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_path, base_name)\n",
    "        os.mkdir(checkpoint_path)\n",
    "        return base_name\n",
    "\n",
    "def set_seed(seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_lr = CustomSchedule(128, 10_000, weight_decay=None)\n",
    "finetune_lr = CustomSchedule(512, 5_000, weight_decay=None)\n",
    "plt.plot(tmp_lr(tf.range(10_000_000 // (32* 5), dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show();\n",
    "\n",
    "plt.plot(finetune_lr(tf.range(2_300_000 // (32), dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "\n",
    "def flat_gradients(grads_or_idx_slices: tf.Tensor) -> tf.Tensor:\n",
    "    '''Convert gradients if it's tf.IndexedSlices.\n",
    "    When computing gradients for operation concerning `tf.gather`, the type of gradients \n",
    "    '''\n",
    "    if type(grads_or_idx_slices) == tf.IndexedSlices:\n",
    "        return tf.scatter_nd(\n",
    "            tf.expand_dims(grads_or_idx_slices.indices, 1),\n",
    "            grads_or_idx_slices.values,\n",
    "            tf.cast(grads_or_idx_slices.dense_shape, tf.int64)\n",
    "        )\n",
    "    return grads_or_idx_slices\n",
    "\n",
    "def backward_optimization(num_grad_steps, global_gradients, step_gradients, step, model, optimizer):\n",
    "    if not global_gradients:\n",
    "        global_gradients = step_gradients\n",
    "    else:\n",
    "        for i, g in enumerate(step_gradients):\n",
    "            global_gradients[i] += flat_gradients(g)\n",
    "    if (step + 1) % num_grad_steps == 0:\n",
    "        optimizer.apply_gradients(zip(global_gradients, model.trainable_variables))\n",
    "        global_gradients = []\n",
    "    return global_gradients\n",
    "\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def train_step(*inputs, target, model, optimizer, num_accum_steps, **kwargs):\n",
    "    l_loss, l_acc = kwargs['loss'], kwargs['acc']\n",
    "    seq_type = kwargs['seq_type']\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(*inputs, training=True)\n",
    "        loss = loss_function(target, predictions, seq_type)\n",
    "        acc = acc_function(target, predictions)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss) / num_accum_steps\n",
    "\n",
    "    gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(gradients)\n",
    "    # optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    l_loss(loss)\n",
    "    l_acc(acc)\n",
    "    return gradients\n",
    "  \n",
    "@tf.function\n",
    "def test_step(*inputs, target, **kwargs):\n",
    "    l_loss, l_acc = kwargs['loss'], kwargs['acc']\n",
    "    seq_type = kwargs['seq_type']\n",
    "    predictions = model(*inputs, training=False)\n",
    "    loss = loss_function(target, predictions, seq_type)\n",
    "    acc = acc_function(target, predictions)\n",
    "    l_loss(loss)\n",
    "    l_acc(acc)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def metrics_reset_states(*metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "\n",
    "def fancy_printer(loss_tracker, epoch, batch_num, start, step='train', dict_metrics={}, num_epochs=1, **kwargs):\n",
    "    num_step = kwargs['num_step']\n",
    "    dict_print_metrics = {' '.join(f\"{key}:{value:.6f}\" for key, value in dict_metrics.items())}\n",
    "    if step!='epoch':\n",
    "        printer = f'[{step} Epoch]{epoch + 1}/{num_epochs} [Time]{time.time() - start:.2f} [Step]{num_step} [Batch]{batch_num} [Speed]{((time.time() - start)/max(1, batch_num))*1000:.2f}ms/step '\n",
    "        printer += f'[Loss]{loss_tracker.result():.4f} ' + '[Metrics]' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "    else:\n",
    "        train_loss, val_loss = kwargs['train_loss'], kwargs['val_loss']\n",
    "        print(f'\\nTime taken for epoch {epoch+1}/{num_epochs}: {time.time() - start:.2f} secs')\n",
    "        printer = f'[Epoch]{epoch + 1}/{num_epochs} - [Train Loss]{train_loss.result():.4f} '\n",
    "        printer += f'- [Val Loss]{val_loss.result():.4f} ' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "\n",
    "\n",
    "def log_wandb_metrics(step='train', num_step=0, dict_metrics=None, gradients=None, plot_image=False, **kwargs):\n",
    "    # Scalar metrics\n",
    "    if step=='train' or step=='val':\n",
    "        wandb.log({name : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "    if step=='epoch':\n",
    "        wandb.log({f'epoch_{name}' : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "\n",
    "    # Gradients\n",
    "    if gradients:\n",
    "        wandb.log({\n",
    "            'mean_norm_gradients' : np.mean([tf.norm(x) for x in gradients]), \n",
    "            'max_norm_gradients': np.max([tf.norm(x) for x in gradients])\n",
    "        })\n",
    "\n",
    "def init_wandb(wandb_project='<your_project>', entity='', run_name='', dict_config=None):\n",
    "    wandb.init(project=wandb_project, entity=entity, name=run_name, settings=wandb.Settings(code_dir=\".\"),\n",
    "               config=dict_config)\n",
    "    wandb.run.log_code(\".\")\n",
    "\n",
    "\n",
    "def grad_accum_scheduler(num_samples, list_scheduler, max_grad_accum):\n",
    "    if num_samples >= len(list_scheduler):\n",
    "        return max_grad_accum\n",
    "    return list_scheduler[num_samples]\n",
    "\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menric1296\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/enric/SSD1TB/KAGGLE/025_Kaggle-OTTO Recsys-2022/1_Scripts/wandb/run-20221124_194523-31l3edi6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/enric1296/otto-recsys/runs/31l3edi6\" target=\"_blank\">model_bert4rec_complete_0.8.4_2022-11-24 19:45:23</a></strong> to <a href=\"https://wandb.ai/enric1296/otto-recsys\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n",
      "Latest checkpoint restored!!\n",
      "================================================================================\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 19:45:26.260257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/home/enric/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 335806208 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-11-24 19:45:27.433911: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x218d6c50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-24 19:45:27.433928: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2022-11-24 19:45:27.449136: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. model_bert4_rec/encoder_transformer_block/dropout_1/dropout/random_uniform/RandomUniform\n",
      "2022-11-24 19:45:27.451919: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-24 19:45:28.567149: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-11-24 19:45:29.055784: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch]1/2 [Time]2.85 [Step]0 [Batch]0 [Speed]2849.11ms/step [Loss]9.0450 [Metrics]{'train_loss:9.044979 train_acc:0.019231 lr:0.000000 grad_accum:5.000000 total_samples:0.000000'}\n",
      "[Train Epoch]1/2 [Time]46.47 [Step]100 [Batch]500 [Speed]92.94ms/step [Loss]7.5668 [Metrics]{'train_loss:7.566787 train_acc:0.087151 lr:0.000009 grad_accum:5.000000 total_samples:16000.000000'}\n",
      "[Train Epoch]1/2 [Time]88.87 [Step]200 [Batch]1000 [Speed]88.87ms/step [Loss]7.6054 [Metrics]{'train_loss:7.605384 train_acc:0.086069 lr:0.000018 grad_accum:5.000000 total_samples:32000.000000'}\n",
      "[Train Epoch]1/2 [Time]131.30 [Step]300 [Batch]1500 [Speed]87.53ms/step [Loss]7.6180 [Metrics]{'train_loss:7.618033 train_acc:0.084668 lr:0.000027 grad_accum:5.000000 total_samples:48000.000000'}\n",
      "[Train Epoch]1/2 [Time]173.56 [Step]400 [Batch]2000 [Speed]86.78ms/step [Loss]7.6107 [Metrics]{'train_loss:7.610662 train_acc:0.084879 lr:0.000035 grad_accum:5.000000 total_samples:64000.000000'}\n",
      "[Train Epoch]1/2 [Time]215.61 [Step]500 [Batch]2500 [Speed]86.25ms/step [Loss]7.6166 [Metrics]{'train_loss:7.616586 train_acc:0.085068 lr:0.000044 grad_accum:5.000000 total_samples:80000.000000'}\n",
      "[Train Epoch]1/2 [Time]257.26 [Step]600 [Batch]3000 [Speed]85.75ms/step [Loss]7.6212 [Metrics]{'train_loss:7.621200 train_acc:0.085111 lr:0.000053 grad_accum:5.000000 total_samples:96000.000000'}\n",
      "[Train Epoch]1/2 [Time]299.37 [Step]700 [Batch]3500 [Speed]85.53ms/step [Loss]7.6231 [Metrics]{'train_loss:7.623092 train_acc:0.085259 lr:0.000062 grad_accum:5.000000 total_samples:112000.000000'}\n",
      "[Train Epoch]1/2 [Time]341.39 [Step]800 [Batch]4000 [Speed]85.35ms/step [Loss]7.6239 [Metrics]{'train_loss:7.623918 train_acc:0.085146 lr:0.000071 grad_accum:5.000000 total_samples:128000.000000'}\n",
      "[Train Epoch]1/2 [Time]383.73 [Step]900 [Batch]4500 [Speed]85.27ms/step [Loss]7.6253 [Metrics]{'train_loss:7.625319 train_acc:0.085282 lr:0.000080 grad_accum:5.000000 total_samples:144000.000000'}\n",
      "[Train Epoch]1/2 [Time]425.76 [Step]1000 [Batch]5000 [Speed]85.15ms/step [Loss]7.6285 [Metrics]{'train_loss:7.628473 train_acc:0.085338 lr:0.000088 grad_accum:5.000000 total_samples:160000.000000'}\n",
      "[Train Epoch]1/2 [Time]467.66 [Step]1100 [Batch]5500 [Speed]85.03ms/step [Loss]7.6279 [Metrics]{'train_loss:7.627867 train_acc:0.085666 lr:0.000097 grad_accum:5.000000 total_samples:176000.000000'}\n",
      "[Train Epoch]1/2 [Time]509.82 [Step]1200 [Batch]6000 [Speed]84.97ms/step [Loss]7.6350 [Metrics]{'train_loss:7.635029 train_acc:0.085701 lr:0.000106 grad_accum:5.000000 total_samples:192000.000000'}\n",
      "[Train Epoch]1/2 [Time]551.21 [Step]1083 [Batch]6500 [Speed]84.80ms/step [Loss]7.6397 [Metrics]{'train_loss:7.639713 train_acc:0.085891 lr:0.000096 grad_accum:6.000000 total_samples:208064.000000'}\n",
      "[Train Epoch]1/2 [Time]592.04 [Step]1166 [Batch]7000 [Speed]84.58ms/step [Loss]7.6400 [Metrics]{'train_loss:7.640036 train_acc:0.085899 lr:0.000103 grad_accum:6.000000 total_samples:224000.000000'}\n",
      "[Train Epoch]1/2 [Time]632.69 [Step]1250 [Batch]7500 [Speed]84.36ms/step [Loss]7.6423 [Metrics]{'train_loss:7.642304 train_acc:0.085754 lr:0.000110 grad_accum:6.000000 total_samples:240128.000000'}\n",
      "[Train Epoch]1/2 [Time]673.70 [Step]1333 [Batch]8000 [Speed]84.21ms/step [Loss]7.6379 [Metrics]{'train_loss:7.637902 train_acc:0.085782 lr:0.000118 grad_accum:6.000000 total_samples:256064.000000'}\n",
      "[Train Epoch]1/2 [Time]714.96 [Step]1416 [Batch]8500 [Speed]84.11ms/step [Loss]7.6413 [Metrics]{'train_loss:7.641327 train_acc:0.085867 lr:0.000125 grad_accum:6.000000 total_samples:272000.000000'}\n",
      "[Train Epoch]1/2 [Time]756.43 [Step]1500 [Batch]9000 [Speed]84.05ms/step [Loss]7.6418 [Metrics]{'train_loss:7.641827 train_acc:0.085767 lr:0.000133 grad_accum:6.000000 total_samples:288128.000000'}\n",
      "[Train Epoch]1/2 [Time]797.94 [Step]1583 [Batch]9500 [Speed]83.99ms/step [Loss]7.6395 [Metrics]{'train_loss:7.639475 train_acc:0.085820 lr:0.000140 grad_accum:6.000000 total_samples:304064.000000'}\n",
      "[Train Epoch]1/2 [Time]839.28 [Step]1666 [Batch]10000 [Speed]83.93ms/step [Loss]7.6418 [Metrics]{'train_loss:7.641843 train_acc:0.085753 lr:0.000147 grad_accum:6.000000 total_samples:320000.000000'}\n",
      "[Train Epoch]1/2 [Time]880.84 [Step]1750 [Batch]10500 [Speed]83.89ms/step [Loss]7.6423 [Metrics]{'train_loss:7.642262 train_acc:0.085755 lr:0.000155 grad_accum:6.000000 total_samples:336128.000000'}\n",
      "[Train Epoch]1/2 [Time]922.44 [Step]1833 [Batch]11000 [Speed]83.86ms/step [Loss]7.6434 [Metrics]{'train_loss:7.643417 train_acc:0.085768 lr:0.000162 grad_accum:6.000000 total_samples:352064.000000'}\n",
      "[Train Epoch]1/2 [Time]964.86 [Step]1916 [Batch]11500 [Speed]83.90ms/step [Loss]7.6416 [Metrics]{'train_loss:7.641647 train_acc:0.085809 lr:0.000169 grad_accum:6.000000 total_samples:368000.000000'}\n",
      "[Train Epoch]1/2 [Time]1007.19 [Step]2000 [Batch]12000 [Speed]83.93ms/step [Loss]7.6408 [Metrics]{'train_loss:7.640801 train_acc:0.085824 lr:0.000177 grad_accum:6.000000 total_samples:384128.000000'}\n",
      "[Train Epoch]1/2 [Time]1049.51 [Step]1785 [Batch]12500 [Speed]83.96ms/step [Loss]7.6418 [Metrics]{'train_loss:7.641788 train_acc:0.085837 lr:0.000158 grad_accum:7.000000 total_samples:400064.000000'}\n",
      "[Train Epoch]1/2 [Time]1091.77 [Step]1857 [Batch]13000 [Speed]83.98ms/step [Loss]7.6407 [Metrics]{'train_loss:7.640681 train_acc:0.085915 lr:0.000164 grad_accum:7.000000 total_samples:416192.000000'}\n",
      "[Train Epoch]1/2 [Time]1134.18 [Step]1928 [Batch]13500 [Speed]84.01ms/step [Loss]7.6403 [Metrics]{'train_loss:7.640295 train_acc:0.085880 lr:0.000170 grad_accum:7.000000 total_samples:432096.000000'}\n",
      "[Train Epoch]1/2 [Time]1176.39 [Step]2000 [Batch]14000 [Speed]84.03ms/step [Loss]7.6386 [Metrics]{'train_loss:7.638596 train_acc:0.085829 lr:0.000177 grad_accum:7.000000 total_samples:448224.000000'}\n",
      "[Train Epoch]1/2 [Time]1218.44 [Step]2071 [Batch]14500 [Speed]84.03ms/step [Loss]7.6379 [Metrics]{'train_loss:7.637874 train_acc:0.085814 lr:0.000183 grad_accum:7.000000 total_samples:464128.000000'}\n",
      "[Train Epoch]1/2 [Time]1260.55 [Step]2142 [Batch]15000 [Speed]84.04ms/step [Loss]7.6364 [Metrics]{'train_loss:7.636444 train_acc:0.085733 lr:0.000189 grad_accum:7.000000 total_samples:480032.000000'}\n",
      "[Train Epoch]1/2 [Time]1302.91 [Step]2214 [Batch]15500 [Speed]84.06ms/step [Loss]7.6341 [Metrics]{'train_loss:7.634064 train_acc:0.085712 lr:0.000196 grad_accum:7.000000 total_samples:496160.000000'}\n",
      "[Train Epoch]1/2 [Time]1344.90 [Step]2285 [Batch]16000 [Speed]84.06ms/step [Loss]7.6335 [Metrics]{'train_loss:7.633484 train_acc:0.085735 lr:0.000202 grad_accum:7.000000 total_samples:512064.000000'}\n",
      "[Train Epoch]1/2 [Time]1387.07 [Step]2357 [Batch]16500 [Speed]84.07ms/step [Loss]7.6331 [Metrics]{'train_loss:7.633143 train_acc:0.085748 lr:0.000208 grad_accum:7.000000 total_samples:528192.000000'}\n",
      "[Train Epoch]1/2 [Time]1427.88 [Step]2428 [Batch]17000 [Speed]83.99ms/step [Loss]7.6308 [Metrics]{'train_loss:7.630779 train_acc:0.085811 lr:0.000215 grad_accum:7.000000 total_samples:544096.000000'}\n",
      "[Train Epoch]1/2 [Time]1469.60 [Step]2500 [Batch]17500 [Speed]83.98ms/step [Loss]7.6284 [Metrics]{'train_loss:7.628373 train_acc:0.085936 lr:0.000221 grad_accum:7.000000 total_samples:560224.000000'}\n",
      "[Train Epoch]1/2 [Time]1511.73 [Step]2571 [Batch]18000 [Speed]83.99ms/step [Loss]7.6277 [Metrics]{'train_loss:7.627719 train_acc:0.085976 lr:0.000227 grad_accum:7.000000 total_samples:576128.000000'}\n",
      "[Train Epoch]1/2 [Time]1553.47 [Step]2642 [Batch]18500 [Speed]83.97ms/step [Loss]7.6293 [Metrics]{'train_loss:7.629301 train_acc:0.085989 lr:0.000234 grad_accum:7.000000 total_samples:592032.000000'}\n",
      "[Train Epoch]1/2 [Time]1595.42 [Step]2375 [Batch]19000 [Speed]83.97ms/step [Loss]7.6293 [Metrics]{'train_loss:7.629326 train_acc:0.085953 lr:0.000210 grad_accum:8.000000 total_samples:608288.000000'}\n",
      "[Train Epoch]1/2 [Time]1637.43 [Step]2437 [Batch]19500 [Speed]83.97ms/step [Loss]7.6294 [Metrics]{'train_loss:7.629377 train_acc:0.085966 lr:0.000215 grad_accum:8.000000 total_samples:624160.000000'}\n",
      "[Train Epoch]1/2 [Time]1678.73 [Step]2500 [Batch]20000 [Speed]83.94ms/step [Loss]7.6297 [Metrics]{'train_loss:7.629736 train_acc:0.085969 lr:0.000221 grad_accum:8.000000 total_samples:640288.000000'}\n",
      "[Train Epoch]1/2 [Time]1719.63 [Step]2562 [Batch]20500 [Speed]83.88ms/step [Loss]7.6304 [Metrics]{'train_loss:7.630359 train_acc:0.085979 lr:0.000226 grad_accum:8.000000 total_samples:656160.000000'}\n",
      "[Train Epoch]1/2 [Time]1760.48 [Step]2625 [Batch]21000 [Speed]83.83ms/step [Loss]7.6295 [Metrics]{'train_loss:7.629488 train_acc:0.086040 lr:0.000232 grad_accum:8.000000 total_samples:672288.000000'}\n",
      "[Train Epoch]1/2 [Time]1801.93 [Step]2687 [Batch]21500 [Speed]83.81ms/step [Loss]7.6275 [Metrics]{'train_loss:7.627457 train_acc:0.086100 lr:0.000237 grad_accum:8.000000 total_samples:688160.000000'}\n",
      "[Train Epoch]1/2 [Time]1843.59 [Step]2750 [Batch]22000 [Speed]83.80ms/step [Loss]7.6262 [Metrics]{'train_loss:7.626227 train_acc:0.086110 lr:0.000243 grad_accum:8.000000 total_samples:704288.000000'}\n",
      "[Train Epoch]1/2 [Time]1885.15 [Step]2812 [Batch]22500 [Speed]83.78ms/step [Loss]7.6249 [Metrics]{'train_loss:7.624898 train_acc:0.086113 lr:0.000249 grad_accum:8.000000 total_samples:720160.000000'}\n",
      "[Train Epoch]1/2 [Time]1926.75 [Step]2875 [Batch]23000 [Speed]83.77ms/step [Loss]7.6234 [Metrics]{'train_loss:7.623378 train_acc:0.086133 lr:0.000254 grad_accum:8.000000 total_samples:736288.000000'}\n",
      "[Train Epoch]1/2 [Time]1968.22 [Step]2937 [Batch]23500 [Speed]83.75ms/step [Loss]7.6223 [Metrics]{'train_loss:7.622348 train_acc:0.086075 lr:0.000260 grad_accum:8.000000 total_samples:752160.000000'}\n",
      "[Train Epoch]1/2 [Time]2009.89 [Step]3000 [Batch]24000 [Speed]83.75ms/step [Loss]7.6208 [Metrics]{'train_loss:7.620833 train_acc:0.086065 lr:0.000265 grad_accum:8.000000 total_samples:768288.000000'}\n",
      "[Train Epoch]1/2 [Time]2051.59 [Step]3062 [Batch]24500 [Speed]83.74ms/step [Loss]7.6199 [Metrics]{'train_loss:7.619889 train_acc:0.086068 lr:0.000271 grad_accum:8.000000 total_samples:784160.000000'}\n",
      "Saving checkpoint for epoch 1 at step 25000 on path ../2_Models/model_bert4rec_complete_0.8.4/checkpoints/\n",
      "[Train Epoch]1/2 [Time]2102.06 [Step]2777 [Batch]25000 [Speed]84.08ms/step [Loss]7.6186 [Metrics]{'train_loss:7.618567 train_acc:0.086120 lr:0.000245 grad_accum:9.000000 total_samples:800320.000000'}\n",
      "[Train Epoch]1/2 [Time]2143.44 [Step]2833 [Batch]25500 [Speed]84.06ms/step [Loss]7.6181 [Metrics]{'train_loss:7.618068 train_acc:0.086111 lr:0.000250 grad_accum:9.000000 total_samples:816448.000000'}\n",
      "[Train Epoch]1/2 [Time]2184.46 [Step]2888 [Batch]26000 [Speed]84.02ms/step [Loss]7.6157 [Metrics]{'train_loss:7.615699 train_acc:0.086126 lr:0.000255 grad_accum:9.000000 total_samples:832288.000000'}\n",
      "[Train Epoch]1/2 [Time]2225.03 [Step]2944 [Batch]26500 [Speed]83.96ms/step [Loss]7.6149 [Metrics]{'train_loss:7.614893 train_acc:0.086228 lr:0.000260 grad_accum:9.000000 total_samples:848416.000000'}\n",
      "[Train Epoch]1/2 [Time]2265.52 [Step]3000 [Batch]27000 [Speed]83.91ms/step [Loss]7.6150 [Metrics]{'train_loss:7.614960 train_acc:0.086192 lr:0.000265 grad_accum:9.000000 total_samples:864544.000000'}\n",
      "[Train Epoch]1/2 [Time]2305.73 [Step]3055 [Batch]27500 [Speed]83.84ms/step [Loss]7.6140 [Metrics]{'train_loss:7.613976 train_acc:0.086204 lr:0.000270 grad_accum:9.000000 total_samples:880384.000000'}\n",
      "[Train Epoch]1/2 [Time]2346.23 [Step]3111 [Batch]28000 [Speed]83.79ms/step [Loss]7.6113 [Metrics]{'train_loss:7.611329 train_acc:0.086246 lr:0.000275 grad_accum:9.000000 total_samples:896512.000000'}\n",
      "[Train Epoch]1/2 [Time]2386.77 [Step]3166 [Batch]28500 [Speed]83.75ms/step [Loss]7.6098 [Metrics]{'train_loss:7.609801 train_acc:0.086229 lr:0.000280 grad_accum:9.000000 total_samples:912352.000000'}\n",
      "[Train Epoch]1/2 [Time]2427.49 [Step]3222 [Batch]29000 [Speed]83.71ms/step [Loss]7.6078 [Metrics]{'train_loss:7.607800 train_acc:0.086283 lr:0.000285 grad_accum:9.000000 total_samples:928480.000000'}\n",
      "[Train Epoch]1/2 [Time]2467.88 [Step]3277 [Batch]29500 [Speed]83.66ms/step [Loss]7.6062 [Metrics]{'train_loss:7.606248 train_acc:0.086275 lr:0.000290 grad_accum:9.000000 total_samples:944320.000000'}\n",
      "[Train Epoch]1/2 [Time]2508.57 [Step]3333 [Batch]30000 [Speed]83.62ms/step [Loss]7.6060 [Metrics]{'train_loss:7.605970 train_acc:0.086307 lr:0.000295 grad_accum:9.000000 total_samples:960448.000000'}\n",
      "[Train Epoch]1/2 [Time]2549.58 [Step]3388 [Batch]30500 [Speed]83.59ms/step [Loss]7.6055 [Metrics]{'train_loss:7.605454 train_acc:0.086369 lr:0.000299 grad_accum:9.000000 total_samples:976288.000000'}\n",
      "[Train Epoch]1/2 [Time]2590.31 [Step]3444 [Batch]31000 [Speed]83.56ms/step [Loss]7.6045 [Metrics]{'train_loss:7.604503 train_acc:0.086399 lr:0.000304 grad_accum:9.000000 total_samples:992416.000000'}\n",
      "[Train Epoch]1/2 [Time]2630.54 [Step]3150 [Batch]31500 [Speed]83.51ms/step [Loss]7.6043 [Metrics]{'train_loss:7.604342 train_acc:0.086418 lr:0.000278 grad_accum:10.000000 total_samples:1008832.000000'}\n",
      "[Train Epoch]1/2 [Time]2670.44 [Step]3200 [Batch]32000 [Speed]83.45ms/step [Loss]7.6040 [Metrics]{'train_loss:7.604015 train_acc:0.086392 lr:0.000283 grad_accum:10.000000 total_samples:1024832.000000'}\n",
      "[Train Epoch]1/2 [Time]2710.86 [Step]3250 [Batch]32500 [Speed]83.41ms/step [Loss]7.6032 [Metrics]{'train_loss:7.603170 train_acc:0.086431 lr:0.000287 grad_accum:10.000000 total_samples:1040832.000000'}\n",
      "[Train Epoch]1/2 [Time]2751.47 [Step]3300 [Batch]33000 [Speed]83.38ms/step [Loss]7.6031 [Metrics]{'train_loss:7.603127 train_acc:0.086446 lr:0.000292 grad_accum:10.000000 total_samples:1056832.000000'}\n",
      "[Train Epoch]1/2 [Time]2792.25 [Step]3350 [Batch]33500 [Speed]83.35ms/step [Loss]7.6023 [Metrics]{'train_loss:7.602262 train_acc:0.086441 lr:0.000296 grad_accum:10.000000 total_samples:1072832.000000'}\n",
      "[Train Epoch]1/2 [Time]2832.84 [Step]3400 [Batch]34000 [Speed]83.32ms/step [Loss]7.6023 [Metrics]{'train_loss:7.602324 train_acc:0.086447 lr:0.000301 grad_accum:10.000000 total_samples:1088832.000000'}\n",
      "[Train Epoch]1/2 [Time]2873.21 [Step]3450 [Batch]34500 [Speed]83.28ms/step [Loss]7.6027 [Metrics]{'train_loss:7.602734 train_acc:0.086473 lr:0.000305 grad_accum:10.000000 total_samples:1104832.000000'}\n",
      "[Train Epoch]1/2 [Time]2913.71 [Step]3500 [Batch]35000 [Speed]83.25ms/step [Loss]7.6034 [Metrics]{'train_loss:7.603439 train_acc:0.086466 lr:0.000309 grad_accum:10.000000 total_samples:1120832.000000'}\n",
      "[Train Epoch]1/2 [Time]2954.35 [Step]3550 [Batch]35500 [Speed]83.22ms/step [Loss]7.6044 [Metrics]{'train_loss:7.604427 train_acc:0.086451 lr:0.000314 grad_accum:10.000000 total_samples:1136832.000000'}\n",
      "[Train Epoch]1/2 [Time]2994.96 [Step]3600 [Batch]36000 [Speed]83.19ms/step [Loss]7.6043 [Metrics]{'train_loss:7.604256 train_acc:0.086458 lr:0.000318 grad_accum:10.000000 total_samples:1152832.000000'}\n",
      "[Train Epoch]1/2 [Time]3035.35 [Step]3650 [Batch]36500 [Speed]83.16ms/step [Loss]7.6039 [Metrics]{'train_loss:7.603906 train_acc:0.086434 lr:0.000323 grad_accum:10.000000 total_samples:1168832.000000'}\n",
      "[Train Epoch]1/2 [Time]3075.90 [Step]3700 [Batch]37000 [Speed]83.13ms/step [Loss]7.6045 [Metrics]{'train_loss:7.604520 train_acc:0.086435 lr:0.000327 grad_accum:10.000000 total_samples:1184832.000000'}\n",
      "[Train Epoch]1/2 [Time]3116.41 [Step]3750 [Batch]37500 [Speed]83.10ms/step [Loss]7.6047 [Metrics]{'train_loss:7.604674 train_acc:0.086419 lr:0.000331 grad_accum:10.000000 total_samples:1200832.000000'}\n",
      "[Train Epoch]1/2 [Time]3156.94 [Step]3800 [Batch]38000 [Speed]83.08ms/step [Loss]7.6047 [Metrics]{'train_loss:7.604682 train_acc:0.086398 lr:0.000336 grad_accum:10.000000 total_samples:1216832.000000'}\n",
      "[Train Epoch]1/2 [Time]3197.74 [Step]3850 [Batch]38500 [Speed]83.06ms/step [Loss]7.6043 [Metrics]{'train_loss:7.604321 train_acc:0.086383 lr:0.000340 grad_accum:10.000000 total_samples:1232832.000000'}\n",
      "[Train Epoch]1/2 [Time]3238.53 [Step]3900 [Batch]39000 [Speed]83.04ms/step [Loss]7.6053 [Metrics]{'train_loss:7.605335 train_acc:0.086377 lr:0.000345 grad_accum:10.000000 total_samples:1248832.000000'}\n",
      "[Train Epoch]1/2 [Time]3278.66 [Step]3950 [Batch]39500 [Speed]83.00ms/step [Loss]7.6058 [Metrics]{'train_loss:7.605843 train_acc:0.086392 lr:0.000349 grad_accum:10.000000 total_samples:1264832.000000'}\n",
      "[Train Epoch]1/2 [Time]3318.76 [Step]4000 [Batch]40000 [Speed]82.97ms/step [Loss]7.6069 [Metrics]{'train_loss:7.606879 train_acc:0.086414 lr:0.000354 grad_accum:10.000000 total_samples:1280832.000000'}\n",
      "[Train Epoch]1/2 [Time]3359.44 [Step]4050 [Batch]40500 [Speed]82.95ms/step [Loss]7.6080 [Metrics]{'train_loss:7.607962 train_acc:0.086403 lr:0.000358 grad_accum:10.000000 total_samples:1296832.000000'}\n",
      "[Train Epoch]1/2 [Time]3400.09 [Step]4100 [Batch]41000 [Speed]82.93ms/step [Loss]7.6081 [Metrics]{'train_loss:7.608139 train_acc:0.086392 lr:0.000362 grad_accum:10.000000 total_samples:1312832.000000'}\n",
      "[Train Epoch]1/2 [Time]3440.56 [Step]4150 [Batch]41500 [Speed]82.91ms/step [Loss]7.6079 [Metrics]{'train_loss:7.607883 train_acc:0.086413 lr:0.000367 grad_accum:10.000000 total_samples:1328832.000000'}\n",
      "[Train Epoch]1/2 [Time]3480.65 [Step]4200 [Batch]42000 [Speed]82.87ms/step [Loss]7.6080 [Metrics]{'train_loss:7.608000 train_acc:0.086421 lr:0.000371 grad_accum:10.000000 total_samples:1344832.000000'}\n",
      "[Train Epoch]1/2 [Time]3520.73 [Step]4250 [Batch]42500 [Speed]82.84ms/step [Loss]7.6074 [Metrics]{'train_loss:7.607438 train_acc:0.086414 lr:0.000376 grad_accum:10.000000 total_samples:1360832.000000'}\n",
      "[Train Epoch]1/2 [Time]3560.76 [Step]4300 [Batch]43000 [Speed]82.81ms/step [Loss]7.6062 [Metrics]{'train_loss:7.606239 train_acc:0.086416 lr:0.000380 grad_accum:10.000000 total_samples:1376832.000000'}\n",
      "[Train Epoch]1/2 [Time]3601.17 [Step]4350 [Batch]43500 [Speed]82.79ms/step [Loss]7.6067 [Metrics]{'train_loss:7.606699 train_acc:0.086384 lr:0.000384 grad_accum:10.000000 total_samples:1392832.000000'}\n",
      "[Train Epoch]1/2 [Time]3641.69 [Step]4400 [Batch]44000 [Speed]82.77ms/step [Loss]7.6068 [Metrics]{'train_loss:7.606805 train_acc:0.086429 lr:0.000389 grad_accum:10.000000 total_samples:1408832.000000'}\n",
      "[Train Epoch]1/2 [Time]3682.16 [Step]4450 [Batch]44500 [Speed]82.75ms/step [Loss]7.6070 [Metrics]{'train_loss:7.606964 train_acc:0.086420 lr:0.000393 grad_accum:10.000000 total_samples:1424832.000000'}\n",
      "[Train Epoch]1/2 [Time]3722.53 [Step]4500 [Batch]45000 [Speed]82.72ms/step [Loss]7.6066 [Metrics]{'train_loss:7.606553 train_acc:0.086401 lr:0.000398 grad_accum:10.000000 total_samples:1440832.000000'}\n",
      "[Train Epoch]1/2 [Time]3762.94 [Step]4550 [Batch]45500 [Speed]82.70ms/step [Loss]7.6071 [Metrics]{'train_loss:7.607062 train_acc:0.086375 lr:0.000402 grad_accum:10.000000 total_samples:1456832.000000'}\n",
      "[Train Epoch]1/2 [Time]3803.53 [Step]4600 [Batch]46000 [Speed]82.69ms/step [Loss]7.6079 [Metrics]{'train_loss:7.607940 train_acc:0.086356 lr:0.000407 grad_accum:10.000000 total_samples:1472832.000000'}\n",
      "[Train Epoch]1/2 [Time]3843.72 [Step]4650 [Batch]46500 [Speed]82.66ms/step [Loss]7.6083 [Metrics]{'train_loss:7.608256 train_acc:0.086373 lr:0.000411 grad_accum:10.000000 total_samples:1488832.000000'}\n",
      "[Train Epoch]1/2 [Time]3883.85 [Step]4700 [Batch]47000 [Speed]82.64ms/step [Loss]7.6076 [Metrics]{'train_loss:7.607562 train_acc:0.086387 lr:0.000415 grad_accum:10.000000 total_samples:1504832.000000'}\n",
      "[Train Epoch]1/2 [Time]3924.10 [Step]4750 [Batch]47500 [Speed]82.61ms/step [Loss]7.6070 [Metrics]{'train_loss:7.606966 train_acc:0.086415 lr:0.000420 grad_accum:10.000000 total_samples:1520832.000000'}\n",
      "[Train Epoch]1/2 [Time]3964.22 [Step]4800 [Batch]48000 [Speed]82.59ms/step [Loss]7.6068 [Metrics]{'train_loss:7.606777 train_acc:0.086393 lr:0.000424 grad_accum:10.000000 total_samples:1536832.000000'}\n",
      "[Train Epoch]1/2 [Time]4004.69 [Step]4850 [Batch]48500 [Speed]82.57ms/step [Loss]7.6059 [Metrics]{'train_loss:7.605874 train_acc:0.086402 lr:0.000429 grad_accum:10.000000 total_samples:1552832.000000'}\n",
      "[Train Epoch]1/2 [Time]4045.34 [Step]4900 [Batch]49000 [Speed]82.56ms/step [Loss]7.6061 [Metrics]{'train_loss:7.606058 train_acc:0.086380 lr:0.000433 grad_accum:10.000000 total_samples:1568832.000000'}\n",
      "[Train Epoch]1/2 [Time]4085.90 [Step]4950 [Batch]49500 [Speed]82.54ms/step [Loss]7.6063 [Metrics]{'train_loss:7.606332 train_acc:0.086382 lr:0.000438 grad_accum:10.000000 total_samples:1584832.000000'}\n",
      "Saving checkpoint for epoch 1 at step 50000 on path ../2_Models/model_bert4rec_complete_0.8.4/checkpoints/\n",
      "[Train Epoch]1/2 [Time]4134.09 [Step]5000 [Batch]50000 [Speed]82.68ms/step [Loss]7.6068 [Metrics]{'train_loss:7.606774 train_acc:0.086388 lr:0.000442 grad_accum:10.000000 total_samples:1600832.000000'}\n",
      "[Train Epoch]1/2 [Time]4174.34 [Step]5050 [Batch]50500 [Speed]82.66ms/step [Loss]7.6069 [Metrics]{'train_loss:7.606861 train_acc:0.086399 lr:0.000446 grad_accum:10.000000 total_samples:1616832.000000'}\n",
      "[Train Epoch]1/2 [Time]4214.80 [Step]5100 [Batch]51000 [Speed]82.64ms/step [Loss]7.6063 [Metrics]{'train_loss:7.606288 train_acc:0.086405 lr:0.000451 grad_accum:10.000000 total_samples:1632832.000000'}\n",
      "[Train Epoch]1/2 [Time]4255.42 [Step]5150 [Batch]51500 [Speed]82.63ms/step [Loss]7.6059 [Metrics]{'train_loss:7.605947 train_acc:0.086403 lr:0.000455 grad_accum:10.000000 total_samples:1648832.000000'}\n",
      "[Train Epoch]1/2 [Time]4295.54 [Step]5200 [Batch]52000 [Speed]82.61ms/step [Loss]7.6061 [Metrics]{'train_loss:7.606113 train_acc:0.086395 lr:0.000460 grad_accum:10.000000 total_samples:1664832.000000'}\n",
      "[Train Epoch]1/2 [Time]4335.77 [Step]5250 [Batch]52500 [Speed]82.59ms/step [Loss]7.6056 [Metrics]{'train_loss:7.605564 train_acc:0.086400 lr:0.000464 grad_accum:10.000000 total_samples:1680832.000000'}\n",
      "[Train Epoch]1/2 [Time]4376.17 [Step]5300 [Batch]53000 [Speed]82.57ms/step [Loss]7.6047 [Metrics]{'train_loss:7.604706 train_acc:0.086407 lr:0.000468 grad_accum:10.000000 total_samples:1696832.000000'}\n",
      "[Train Epoch]1/2 [Time]4416.45 [Step]5350 [Batch]53500 [Speed]82.55ms/step [Loss]7.6042 [Metrics]{'train_loss:7.604221 train_acc:0.086407 lr:0.000473 grad_accum:10.000000 total_samples:1712832.000000'}\n",
      "[Train Epoch]1/2 [Time]4457.25 [Step]5400 [Batch]54000 [Speed]82.54ms/step [Loss]7.6036 [Metrics]{'train_loss:7.603609 train_acc:0.086451 lr:0.000477 grad_accum:10.000000 total_samples:1728832.000000'}\n",
      "[Train Epoch]1/2 [Time]4497.57 [Step]5450 [Batch]54500 [Speed]82.52ms/step [Loss]7.6035 [Metrics]{'train_loss:7.603469 train_acc:0.086460 lr:0.000482 grad_accum:10.000000 total_samples:1744832.000000'}\n",
      "[Train Epoch]1/2 [Time]4537.53 [Step]5500 [Batch]55000 [Speed]82.50ms/step [Loss]7.6034 [Metrics]{'train_loss:7.603402 train_acc:0.086459 lr:0.000486 grad_accum:10.000000 total_samples:1760832.000000'}\n",
      "[Train Epoch]1/2 [Time]4577.74 [Step]5550 [Batch]55500 [Speed]82.48ms/step [Loss]7.6031 [Metrics]{'train_loss:7.603120 train_acc:0.086460 lr:0.000491 grad_accum:10.000000 total_samples:1776832.000000'}\n",
      "[Train Epoch]1/2 [Time]4618.37 [Step]5600 [Batch]56000 [Speed]82.47ms/step [Loss]7.6028 [Metrics]{'train_loss:7.602793 train_acc:0.086464 lr:0.000495 grad_accum:10.000000 total_samples:1792832.000000'}\n",
      "[Train Epoch]1/2 [Time]4658.99 [Step]5650 [Batch]56500 [Speed]82.46ms/step [Loss]7.6022 [Metrics]{'train_loss:7.602154 train_acc:0.086469 lr:0.000499 grad_accum:10.000000 total_samples:1808832.000000'}\n",
      "[Train Epoch]1/2 [Time]4699.54 [Step]5700 [Batch]57000 [Speed]82.45ms/step [Loss]7.6018 [Metrics]{'train_loss:7.601766 train_acc:0.086478 lr:0.000504 grad_accum:10.000000 total_samples:1824832.000000'}\n",
      "[Train Epoch]1/2 [Time]4740.74 [Step]5750 [Batch]57500 [Speed]82.45ms/step [Loss]7.6013 [Metrics]{'train_loss:7.601288 train_acc:0.086466 lr:0.000508 grad_accum:10.000000 total_samples:1840832.000000'}\n",
      "[Train Epoch]1/2 [Time]4781.34 [Step]5800 [Batch]58000 [Speed]82.44ms/step [Loss]7.6009 [Metrics]{'train_loss:7.600872 train_acc:0.086466 lr:0.000513 grad_accum:10.000000 total_samples:1856832.000000'}\n",
      "[Train Epoch]1/2 [Time]4820.82 [Step]5850 [Batch]58500 [Speed]82.41ms/step [Loss]7.6004 [Metrics]{'train_loss:7.600373 train_acc:0.086441 lr:0.000517 grad_accum:10.000000 total_samples:1872832.000000'}\n",
      "[Train Epoch]1/2 [Time]4857.96 [Step]5900 [Batch]59000 [Speed]82.34ms/step [Loss]7.5999 [Metrics]{'train_loss:7.599876 train_acc:0.086457 lr:0.000521 grad_accum:10.000000 total_samples:1888832.000000'}\n",
      "[Train Epoch]1/2 [Time]4895.14 [Step]5950 [Batch]59500 [Speed]82.27ms/step [Loss]7.6003 [Metrics]{'train_loss:7.600271 train_acc:0.086453 lr:0.000526 grad_accum:10.000000 total_samples:1904832.000000'}\n",
      "[Train Epoch]1/2 [Time]4932.25 [Step]6000 [Batch]60000 [Speed]82.20ms/step [Loss]7.6003 [Metrics]{'train_loss:7.600340 train_acc:0.086457 lr:0.000530 grad_accum:10.000000 total_samples:1920832.000000'}\n",
      "[Train Epoch]1/2 [Time]4969.37 [Step]6050 [Batch]60500 [Speed]82.14ms/step [Loss]7.6002 [Metrics]{'train_loss:7.600198 train_acc:0.086456 lr:0.000535 grad_accum:10.000000 total_samples:1936832.000000'}\n",
      "[Train Epoch]1/2 [Time]5006.49 [Step]6100 [Batch]61000 [Speed]82.07ms/step [Loss]7.5996 [Metrics]{'train_loss:7.599644 train_acc:0.086444 lr:0.000539 grad_accum:10.000000 total_samples:1952832.000000'}\n",
      "[Train Epoch]1/2 [Time]5043.66 [Step]6150 [Batch]61500 [Speed]82.01ms/step [Loss]7.5991 [Metrics]{'train_loss:7.599110 train_acc:0.086447 lr:0.000544 grad_accum:10.000000 total_samples:1968832.000000'}\n",
      "[Train Epoch]1/2 [Time]5080.82 [Step]6200 [Batch]62000 [Speed]81.95ms/step [Loss]7.5985 [Metrics]{'train_loss:7.598465 train_acc:0.086436 lr:0.000548 grad_accum:10.000000 total_samples:1984832.000000'}\n",
      "[Train Epoch]1/2 [Time]5117.99 [Step]6250 [Batch]62500 [Speed]81.89ms/step [Loss]7.5982 [Metrics]{'train_loss:7.598186 train_acc:0.086461 lr:0.000552 grad_accum:10.000000 total_samples:2000832.000000'}\n",
      "[Train Epoch]1/2 [Time]5155.14 [Step]6300 [Batch]63000 [Speed]81.83ms/step [Loss]7.5978 [Metrics]{'train_loss:7.597847 train_acc:0.086483 lr:0.000557 grad_accum:10.000000 total_samples:2016832.000000'}\n",
      "[Train Epoch]1/2 [Time]5192.23 [Step]6350 [Batch]63500 [Speed]81.77ms/step [Loss]7.5972 [Metrics]{'train_loss:7.597224 train_acc:0.086485 lr:0.000561 grad_accum:10.000000 total_samples:2032832.000000'}\n",
      "[Train Epoch]1/2 [Time]5229.38 [Step]6400 [Batch]64000 [Speed]81.71ms/step [Loss]7.5968 [Metrics]{'train_loss:7.596806 train_acc:0.086475 lr:0.000566 grad_accum:10.000000 total_samples:2048832.000000'}\n",
      "[Train Epoch]1/2 [Time]5266.54 [Step]6450 [Batch]64500 [Speed]81.65ms/step [Loss]7.5962 [Metrics]{'train_loss:7.596241 train_acc:0.086484 lr:0.000570 grad_accum:10.000000 total_samples:2064832.000000'}\n",
      "[Train Epoch]1/2 [Time]5303.71 [Step]6500 [Batch]65000 [Speed]81.60ms/step [Loss]7.5957 [Metrics]{'train_loss:7.595706 train_acc:0.086471 lr:0.000575 grad_accum:10.000000 total_samples:2080832.000000'}\n",
      "[Train Epoch]1/2 [Time]5340.82 [Step]6550 [Batch]65500 [Speed]81.54ms/step [Loss]7.5951 [Metrics]{'train_loss:7.595069 train_acc:0.086484 lr:0.000579 grad_accum:10.000000 total_samples:2096832.000000'}\n",
      "[Train Epoch]1/2 [Time]5377.93 [Step]6600 [Batch]66000 [Speed]81.48ms/step [Loss]7.5944 [Metrics]{'train_loss:7.594422 train_acc:0.086483 lr:0.000583 grad_accum:10.000000 total_samples:2112832.000000'}\n",
      "[Train Epoch]1/2 [Time]5415.04 [Step]6650 [Batch]66500 [Speed]81.43ms/step [Loss]7.5940 [Metrics]{'train_loss:7.594000 train_acc:0.086502 lr:0.000588 grad_accum:10.000000 total_samples:2128832.000000'}\n",
      "[Train Epoch]1/2 [Time]5452.23 [Step]6700 [Batch]67000 [Speed]81.38ms/step [Loss]7.5935 [Metrics]{'train_loss:7.593530 train_acc:0.086521 lr:0.000592 grad_accum:10.000000 total_samples:2144832.000000'}\n",
      "[Train Epoch]1/2 [Time]5489.39 [Step]6750 [Batch]67500 [Speed]81.32ms/step [Loss]7.5931 [Metrics]{'train_loss:7.593137 train_acc:0.086548 lr:0.000597 grad_accum:10.000000 total_samples:2160832.000000'}\n",
      "[Train Epoch]1/2 [Time]5526.55 [Step]6800 [Batch]68000 [Speed]81.27ms/step [Loss]7.5923 [Metrics]{'train_loss:7.592331 train_acc:0.086553 lr:0.000601 grad_accum:10.000000 total_samples:2176832.000000'}\n",
      "[Train Epoch]1/2 [Time]5563.71 [Step]6850 [Batch]68500 [Speed]81.22ms/step [Loss]7.5921 [Metrics]{'train_loss:7.592111 train_acc:0.086535 lr:0.000605 grad_accum:10.000000 total_samples:2192832.000000'}\n",
      "[Train Epoch]1/2 [Time]5600.84 [Step]6900 [Batch]69000 [Speed]81.17ms/step [Loss]7.5914 [Metrics]{'train_loss:7.591358 train_acc:0.086552 lr:0.000610 grad_accum:10.000000 total_samples:2208832.000000'}\n",
      "[Train Epoch]1/2 [Time]5638.00 [Step]6950 [Batch]69500 [Speed]81.12ms/step [Loss]7.5908 [Metrics]{'train_loss:7.590841 train_acc:0.086548 lr:0.000614 grad_accum:10.000000 total_samples:2224832.000000'}\n",
      "[Train Epoch]1/2 [Time]5675.14 [Step]7000 [Batch]70000 [Speed]81.07ms/step [Loss]7.5909 [Metrics]{'train_loss:7.590945 train_acc:0.086530 lr:0.000619 grad_accum:10.000000 total_samples:2240832.000000'}\n",
      "[Train Epoch]1/2 [Time]5712.29 [Step]7050 [Batch]70500 [Speed]81.03ms/step [Loss]7.5907 [Metrics]{'train_loss:7.590689 train_acc:0.086514 lr:0.000623 grad_accum:10.000000 total_samples:2256832.000000'}\n",
      "[Train Epoch]1/2 [Time]5749.49 [Step]7100 [Batch]71000 [Speed]80.98ms/step [Loss]7.5906 [Metrics]{'train_loss:7.590566 train_acc:0.086510 lr:0.000628 grad_accum:10.000000 total_samples:2272832.000000'}\n",
      "[Train Epoch]1/2 [Time]5786.62 [Step]7150 [Batch]71500 [Speed]80.93ms/step [Loss]7.5902 [Metrics]{'train_loss:7.590233 train_acc:0.086506 lr:0.000632 grad_accum:10.000000 total_samples:2288832.000000'}\n",
      "[Train Epoch]1/2 [Time]5823.72 [Step]7200 [Batch]72000 [Speed]80.89ms/step [Loss]7.5902 [Metrics]{'train_loss:7.590171 train_acc:0.086500 lr:0.000636 grad_accum:10.000000 total_samples:2304832.000000'}\n",
      "[Train Epoch]1/2 [Time]5860.95 [Step]7250 [Batch]72500 [Speed]80.84ms/step [Loss]7.5899 [Metrics]{'train_loss:7.589874 train_acc:0.086486 lr:0.000641 grad_accum:10.000000 total_samples:2320832.000000'}\n",
      "[Train Epoch]1/2 [Time]5898.17 [Step]7300 [Batch]73000 [Speed]80.80ms/step [Loss]7.5896 [Metrics]{'train_loss:7.589624 train_acc:0.086488 lr:0.000645 grad_accum:10.000000 total_samples:2336832.000000'}\n",
      "[Train Epoch]1/2 [Time]5935.31 [Step]7350 [Batch]73500 [Speed]80.75ms/step [Loss]7.5893 [Metrics]{'train_loss:7.589285 train_acc:0.086484 lr:0.000650 grad_accum:10.000000 total_samples:2352832.000000'}\n",
      "[Train Epoch]1/2 [Time]5972.41 [Step]7400 [Batch]74000 [Speed]80.71ms/step [Loss]7.5888 [Metrics]{'train_loss:7.588817 train_acc:0.086472 lr:0.000654 grad_accum:10.000000 total_samples:2368832.000000'}\n",
      "[Train Epoch]1/2 [Time]6010.24 [Step]7450 [Batch]74500 [Speed]80.67ms/step [Loss]7.5883 [Metrics]{'train_loss:7.588304 train_acc:0.086478 lr:0.000658 grad_accum:10.000000 total_samples:2384832.000000'}\n",
      "Saving checkpoint for epoch 1 at step 75000 on path ../2_Models/model_bert4rec_complete_0.8.4/checkpoints/\n",
      "[Train Epoch]1/2 [Time]6059.38 [Step]7500 [Batch]75000 [Speed]80.79ms/step [Loss]7.5877 [Metrics]{'train_loss:7.587660 train_acc:0.086485 lr:0.000663 grad_accum:10.000000 total_samples:2400832.000000'}\n",
      "[Train Epoch]1/2 [Time]6099.97 [Step]7550 [Batch]75500 [Speed]80.79ms/step [Loss]7.5874 [Metrics]{'train_loss:7.587381 train_acc:0.086496 lr:0.000667 grad_accum:10.000000 total_samples:2416832.000000'}\n",
      "[Train Epoch]1/2 [Time]6140.90 [Step]7600 [Batch]76000 [Speed]80.80ms/step [Loss]7.5868 [Metrics]{'train_loss:7.586817 train_acc:0.086508 lr:0.000672 grad_accum:10.000000 total_samples:2432832.000000'}\n",
      "[Train Epoch]1/2 [Time]6181.45 [Step]7650 [Batch]76500 [Speed]80.80ms/step [Loss]7.5864 [Metrics]{'train_loss:7.586447 train_acc:0.086517 lr:0.000676 grad_accum:10.000000 total_samples:2448832.000000'}\n",
      "[Train Epoch]1/2 [Time]6222.00 [Step]7700 [Batch]77000 [Speed]80.81ms/step [Loss]7.5862 [Metrics]{'train_loss:7.586233 train_acc:0.086536 lr:0.000681 grad_accum:10.000000 total_samples:2464832.000000'}\n",
      "[Train Epoch]1/2 [Time]6262.46 [Step]7750 [Batch]77500 [Speed]80.81ms/step [Loss]7.5866 [Metrics]{'train_loss:7.586637 train_acc:0.086532 lr:0.000685 grad_accum:10.000000 total_samples:2480832.000000'}\n",
      "[Train Epoch]1/2 [Time]6302.85 [Step]7800 [Batch]78000 [Speed]80.81ms/step [Loss]7.5877 [Metrics]{'train_loss:7.587712 train_acc:0.086522 lr:0.000689 grad_accum:10.000000 total_samples:2496832.000000'}\n",
      "[Train Epoch]1/2 [Time]6343.20 [Step]7850 [Batch]78500 [Speed]80.81ms/step [Loss]7.5882 [Metrics]{'train_loss:7.588248 train_acc:0.086523 lr:0.000694 grad_accum:10.000000 total_samples:2512832.000000'}\n",
      "[Train Epoch]1/2 [Time]6383.51 [Step]7900 [Batch]79000 [Speed]80.80ms/step [Loss]7.5884 [Metrics]{'train_loss:7.588418 train_acc:0.086528 lr:0.000698 grad_accum:10.000000 total_samples:2528832.000000'}\n",
      "[Train Epoch]1/2 [Time]6423.92 [Step]7950 [Batch]79500 [Speed]80.80ms/step [Loss]7.5885 [Metrics]{'train_loss:7.588468 train_acc:0.086537 lr:0.000703 grad_accum:10.000000 total_samples:2544832.000000'}\n",
      "[Train Epoch]1/2 [Time]6464.12 [Step]8000 [Batch]80000 [Speed]80.80ms/step [Loss]7.5889 [Metrics]{'train_loss:7.588907 train_acc:0.086575 lr:0.000707 grad_accum:10.000000 total_samples:2560832.000000'}\n",
      "[Train Epoch]1/2 [Time]6504.68 [Step]8050 [Batch]80500 [Speed]80.80ms/step [Loss]7.5890 [Metrics]{'train_loss:7.589046 train_acc:0.086589 lr:0.000712 grad_accum:10.000000 total_samples:2576832.000000'}\n",
      "[Train Epoch]1/2 [Time]6545.38 [Step]8100 [Batch]81000 [Speed]80.81ms/step [Loss]7.5888 [Metrics]{'train_loss:7.588830 train_acc:0.086584 lr:0.000716 grad_accum:10.000000 total_samples:2592832.000000'}\n",
      "[Train Epoch]1/2 [Time]6586.07 [Step]8150 [Batch]81500 [Speed]80.81ms/step [Loss]7.5882 [Metrics]{'train_loss:7.588185 train_acc:0.086602 lr:0.000720 grad_accum:10.000000 total_samples:2608832.000000'}\n",
      "[Train Epoch]1/2 [Time]6626.21 [Step]8200 [Batch]82000 [Speed]80.81ms/step [Loss]7.5878 [Metrics]{'train_loss:7.587758 train_acc:0.086584 lr:0.000725 grad_accum:10.000000 total_samples:2624832.000000'}\n",
      "[Train Epoch]1/2 [Time]6666.41 [Step]8250 [Batch]82500 [Speed]80.80ms/step [Loss]7.5877 [Metrics]{'train_loss:7.587726 train_acc:0.086575 lr:0.000729 grad_accum:10.000000 total_samples:2640832.000000'}\n",
      "[Train Epoch]1/2 [Time]6706.65 [Step]8300 [Batch]83000 [Speed]80.80ms/step [Loss]7.5874 [Metrics]{'train_loss:7.587354 train_acc:0.086567 lr:0.000734 grad_accum:10.000000 total_samples:2656832.000000'}\n",
      "[Train Epoch]1/2 [Time]6747.25 [Step]8350 [Batch]83500 [Speed]80.81ms/step [Loss]7.5871 [Metrics]{'train_loss:7.587149 train_acc:0.086572 lr:0.000738 grad_accum:10.000000 total_samples:2672832.000000'}\n",
      "[Train Epoch]1/2 [Time]6787.49 [Step]8400 [Batch]84000 [Speed]80.80ms/step [Loss]7.5865 [Metrics]{'train_loss:7.586537 train_acc:0.086580 lr:0.000742 grad_accum:10.000000 total_samples:2688832.000000'}\n",
      "[Train Epoch]1/2 [Time]6827.97 [Step]8450 [Batch]84500 [Speed]80.80ms/step [Loss]7.5862 [Metrics]{'train_loss:7.586239 train_acc:0.086585 lr:0.000747 grad_accum:10.000000 total_samples:2704832.000000'}\n",
      "[Train Epoch]1/2 [Time]6865.20 [Step]8500 [Batch]85000 [Speed]80.77ms/step [Loss]7.5860 [Metrics]{'train_loss:7.585959 train_acc:0.086575 lr:0.000751 grad_accum:10.000000 total_samples:2720832.000000'}\n",
      "[Train Epoch]1/2 [Time]6902.29 [Step]8550 [Batch]85500 [Speed]80.73ms/step [Loss]7.5856 [Metrics]{'train_loss:7.585596 train_acc:0.086570 lr:0.000756 grad_accum:10.000000 total_samples:2736832.000000'}\n",
      "[Train Epoch]1/2 [Time]6939.43 [Step]8600 [Batch]86000 [Speed]80.69ms/step [Loss]7.5849 [Metrics]{'train_loss:7.584872 train_acc:0.086582 lr:0.000760 grad_accum:10.000000 total_samples:2752832.000000'}\n",
      "[Train Epoch]1/2 [Time]6976.51 [Step]8650 [Batch]86500 [Speed]80.65ms/step [Loss]7.5847 [Metrics]{'train_loss:7.584695 train_acc:0.086584 lr:0.000765 grad_accum:10.000000 total_samples:2768832.000000'}\n",
      "[Train Epoch]1/2 [Time]7013.59 [Step]8700 [Batch]87000 [Speed]80.62ms/step [Loss]7.5844 [Metrics]{'train_loss:7.584406 train_acc:0.086581 lr:0.000769 grad_accum:10.000000 total_samples:2784832.000000'}\n",
      "[Train Epoch]1/2 [Time]7050.74 [Step]8750 [Batch]87500 [Speed]80.58ms/step [Loss]7.5841 [Metrics]{'train_loss:7.584123 train_acc:0.086568 lr:0.000773 grad_accum:10.000000 total_samples:2800832.000000'}\n",
      "[Train Epoch]1/2 [Time]7087.82 [Step]8800 [Batch]88000 [Speed]80.54ms/step [Loss]7.5834 [Metrics]{'train_loss:7.583405 train_acc:0.086577 lr:0.000778 grad_accum:10.000000 total_samples:2816832.000000'}\n",
      "[Train Epoch]1/2 [Time]7124.95 [Step]8850 [Batch]88500 [Speed]80.51ms/step [Loss]7.5831 [Metrics]{'train_loss:7.583120 train_acc:0.086577 lr:0.000782 grad_accum:10.000000 total_samples:2832832.000000'}\n",
      "[Train Epoch]1/2 [Time]7162.09 [Step]8900 [Batch]89000 [Speed]80.47ms/step [Loss]7.5828 [Metrics]{'train_loss:7.582759 train_acc:0.086580 lr:0.000787 grad_accum:10.000000 total_samples:2848832.000000'}\n",
      "[Train Epoch]1/2 [Time]7199.26 [Step]8950 [Batch]89500 [Speed]80.44ms/step [Loss]7.5823 [Metrics]{'train_loss:7.582275 train_acc:0.086592 lr:0.000791 grad_accum:10.000000 total_samples:2864832.000000'}\n",
      "[Train Epoch]1/2 [Time]7236.34 [Step]9000 [Batch]90000 [Speed]80.40ms/step [Loss]7.5824 [Metrics]{'train_loss:7.582370 train_acc:0.086589 lr:0.000795 grad_accum:10.000000 total_samples:2880832.000000'}\n",
      "[Train Epoch]1/2 [Time]7273.46 [Step]9050 [Batch]90500 [Speed]80.37ms/step [Loss]7.5821 [Metrics]{'train_loss:7.582097 train_acc:0.086616 lr:0.000800 grad_accum:10.000000 total_samples:2896832.000000'}\n",
      "[Train Epoch]1/2 [Time]7310.53 [Step]9100 [Batch]91000 [Speed]80.34ms/step [Loss]7.5821 [Metrics]{'train_loss:7.582091 train_acc:0.086634 lr:0.000804 grad_accum:10.000000 total_samples:2912832.000000'}\n",
      "[Train Epoch]1/2 [Time]7347.60 [Step]9150 [Batch]91500 [Speed]80.30ms/step [Loss]7.5818 [Metrics]{'train_loss:7.581772 train_acc:0.086634 lr:0.000809 grad_accum:10.000000 total_samples:2928832.000000'}\n",
      "[Train Epoch]1/2 [Time]7385.14 [Step]9200 [Batch]92000 [Speed]80.27ms/step [Loss]7.5813 [Metrics]{'train_loss:7.581338 train_acc:0.086631 lr:0.000813 grad_accum:10.000000 total_samples:2944832.000000'}\n",
      "[Train Epoch]1/2 [Time]7425.86 [Step]9250 [Batch]92500 [Speed]80.28ms/step [Loss]7.5808 [Metrics]{'train_loss:7.580779 train_acc:0.086629 lr:0.000818 grad_accum:10.000000 total_samples:2960832.000000'}\n",
      "[Train Epoch]1/2 [Time]7466.10 [Step]9300 [Batch]93000 [Speed]80.28ms/step [Loss]7.5803 [Metrics]{'train_loss:7.580330 train_acc:0.086626 lr:0.000822 grad_accum:10.000000 total_samples:2976832.000000'}\n",
      "[Train Epoch]1/2 [Time]7506.42 [Step]9350 [Batch]93500 [Speed]80.28ms/step [Loss]7.5798 [Metrics]{'train_loss:7.579805 train_acc:0.086633 lr:0.000826 grad_accum:10.000000 total_samples:2992832.000000'}\n",
      "[Train Epoch]1/2 [Time]7547.74 [Step]9400 [Batch]94000 [Speed]80.30ms/step [Loss]7.5791 [Metrics]{'train_loss:7.579066 train_acc:0.086638 lr:0.000831 grad_accum:10.000000 total_samples:3008832.000000'}\n",
      "[Train Epoch]1/2 [Time]7588.42 [Step]9450 [Batch]94500 [Speed]80.30ms/step [Loss]7.5787 [Metrics]{'train_loss:7.578679 train_acc:0.086628 lr:0.000835 grad_accum:10.000000 total_samples:3024832.000000'}\n",
      "[Train Epoch]1/2 [Time]7628.92 [Step]9500 [Batch]95000 [Speed]80.30ms/step [Loss]7.5782 [Metrics]{'train_loss:7.578157 train_acc:0.086627 lr:0.000840 grad_accum:10.000000 total_samples:3040832.000000'}\n",
      "[Train Epoch]1/2 [Time]7669.63 [Step]9550 [Batch]95500 [Speed]80.31ms/step [Loss]7.5781 [Metrics]{'train_loss:7.578097 train_acc:0.086610 lr:0.000844 grad_accum:10.000000 total_samples:3056832.000000'}\n",
      "[Train Epoch]1/2 [Time]7710.42 [Step]9600 [Batch]96000 [Speed]80.32ms/step [Loss]7.5776 [Metrics]{'train_loss:7.577643 train_acc:0.086611 lr:0.000849 grad_accum:10.000000 total_samples:3072832.000000'}\n",
      "[Train Epoch]1/2 [Time]7748.40 [Step]9650 [Batch]96500 [Speed]80.29ms/step [Loss]7.5772 [Metrics]{'train_loss:7.577197 train_acc:0.086620 lr:0.000853 grad_accum:10.000000 total_samples:3088832.000000'}\n",
      "[Train Epoch]1/2 [Time]7789.03 [Step]9700 [Batch]97000 [Speed]80.30ms/step [Loss]7.5767 [Metrics]{'train_loss:7.576709 train_acc:0.086623 lr:0.000857 grad_accum:10.000000 total_samples:3104832.000000'}\n",
      "[Train Epoch]1/2 [Time]7829.31 [Step]9750 [Batch]97500 [Speed]80.30ms/step [Loss]7.5762 [Metrics]{'train_loss:7.576226 train_acc:0.086606 lr:0.000862 grad_accum:10.000000 total_samples:3120832.000000'}\n",
      "[Train Epoch]1/2 [Time]7869.76 [Step]9800 [Batch]98000 [Speed]80.30ms/step [Loss]7.5761 [Metrics]{'train_loss:7.576113 train_acc:0.086606 lr:0.000866 grad_accum:10.000000 total_samples:3136832.000000'}\n",
      "[Train Epoch]1/2 [Time]7910.08 [Step]9850 [Batch]98500 [Speed]80.31ms/step [Loss]7.5759 [Metrics]{'train_loss:7.575865 train_acc:0.086600 lr:0.000871 grad_accum:10.000000 total_samples:3152832.000000'}\n",
      "[Train Epoch]1/2 [Time]7950.43 [Step]9900 [Batch]99000 [Speed]80.31ms/step [Loss]7.5754 [Metrics]{'train_loss:7.575379 train_acc:0.086602 lr:0.000875 grad_accum:10.000000 total_samples:3168832.000000'}\n",
      "[Train Epoch]1/2 [Time]7990.94 [Step]9950 [Batch]99500 [Speed]80.31ms/step [Loss]7.5751 [Metrics]{'train_loss:7.575089 train_acc:0.086592 lr:0.000879 grad_accum:10.000000 total_samples:3184832.000000'}\n",
      "Saving checkpoint for epoch 1 at step 100000 on path ../2_Models/model_bert4rec_complete_0.8.4/checkpoints/\n",
      "[Train Epoch]1/2 [Time]8039.49 [Step]10000 [Batch]100000 [Speed]80.39ms/step [Loss]7.5747 [Metrics]{'train_loss:7.574718 train_acc:0.086590 lr:0.000884 grad_accum:10.000000 total_samples:3200832.000000'}\n",
      "[Train Epoch]1/2 [Time]8079.57 [Step]10050 [Batch]100500 [Speed]80.39ms/step [Loss]7.5740 [Metrics]{'train_loss:7.573972 train_acc:0.086602 lr:0.000882 grad_accum:10.000000 total_samples:3216832.000000'}\n",
      "[Train Epoch]1/2 [Time]8119.81 [Step]10100 [Batch]101000 [Speed]80.39ms/step [Loss]7.5734 [Metrics]{'train_loss:7.573359 train_acc:0.086599 lr:0.000879 grad_accum:10.000000 total_samples:3232832.000000'}\n",
      "[Train Epoch]1/2 [Time]8159.79 [Step]10150 [Batch]101500 [Speed]80.39ms/step [Loss]7.5729 [Metrics]{'train_loss:7.572865 train_acc:0.086600 lr:0.000877 grad_accum:10.000000 total_samples:3248832.000000'}\n",
      "[Train Epoch]1/2 [Time]8199.96 [Step]10200 [Batch]102000 [Speed]80.39ms/step [Loss]7.5727 [Metrics]{'train_loss:7.572693 train_acc:0.086594 lr:0.000875 grad_accum:10.000000 total_samples:3264832.000000'}\n",
      "[Train Epoch]1/2 [Time]8240.40 [Step]10250 [Batch]102500 [Speed]80.39ms/step [Loss]7.5719 [Metrics]{'train_loss:7.571900 train_acc:0.086606 lr:0.000873 grad_accum:10.000000 total_samples:3280832.000000'}\n",
      "[Train Epoch]1/2 [Time]8280.92 [Step]10300 [Batch]103000 [Speed]80.40ms/step [Loss]7.5718 [Metrics]{'train_loss:7.571833 train_acc:0.086594 lr:0.000871 grad_accum:10.000000 total_samples:3296832.000000'}\n",
      "[Train Epoch]1/2 [Time]8321.79 [Step]10350 [Batch]103500 [Speed]80.40ms/step [Loss]7.5712 [Metrics]{'train_loss:7.571195 train_acc:0.086594 lr:0.000869 grad_accum:10.000000 total_samples:3312832.000000'}\n",
      "[Train Epoch]1/2 [Time]8362.40 [Step]10400 [Batch]104000 [Speed]80.41ms/step [Loss]7.5710 [Metrics]{'train_loss:7.571044 train_acc:0.086581 lr:0.000867 grad_accum:10.000000 total_samples:3328832.000000'}\n",
      "[Train Epoch]1/2 [Time]8403.07 [Step]10450 [Batch]104500 [Speed]80.41ms/step [Loss]7.5705 [Metrics]{'train_loss:7.570537 train_acc:0.086584 lr:0.000865 grad_accum:10.000000 total_samples:3344832.000000'}\n",
      "[Train Epoch]1/2 [Time]8443.85 [Step]10500 [Batch]105000 [Speed]80.42ms/step [Loss]7.5700 [Metrics]{'train_loss:7.570037 train_acc:0.086583 lr:0.000863 grad_accum:10.000000 total_samples:3360832.000000'}\n",
      "[Train Epoch]1/2 [Time]8484.29 [Step]10550 [Batch]105500 [Speed]80.42ms/step [Loss]7.5698 [Metrics]{'train_loss:7.569808 train_acc:0.086576 lr:0.000861 grad_accum:10.000000 total_samples:3376832.000000'}\n",
      "[Train Epoch]1/2 [Time]8524.32 [Step]10600 [Batch]106000 [Speed]80.42ms/step [Loss]7.5693 [Metrics]{'train_loss:7.569340 train_acc:0.086569 lr:0.000859 grad_accum:10.000000 total_samples:3392832.000000'}\n",
      "[Train Epoch]1/2 [Time]8564.20 [Step]10650 [Batch]106500 [Speed]80.41ms/step [Loss]7.5689 [Metrics]{'train_loss:7.568894 train_acc:0.086566 lr:0.000856 grad_accum:10.000000 total_samples:3408832.000000'}\n",
      "[Train Epoch]1/2 [Time]8604.77 [Step]10700 [Batch]107000 [Speed]80.42ms/step [Loss]7.5684 [Metrics]{'train_loss:7.568446 train_acc:0.086572 lr:0.000854 grad_accum:10.000000 total_samples:3424832.000000'}\n",
      "[Train Epoch]1/2 [Time]8645.61 [Step]10750 [Batch]107500 [Speed]80.42ms/step [Loss]7.5678 [Metrics]{'train_loss:7.567815 train_acc:0.086576 lr:0.000852 grad_accum:10.000000 total_samples:3440832.000000'}\n",
      "[Train Epoch]1/2 [Time]8686.15 [Step]10800 [Batch]108000 [Speed]80.43ms/step [Loss]7.5673 [Metrics]{'train_loss:7.567347 train_acc:0.086574 lr:0.000851 grad_accum:10.000000 total_samples:3456832.000000'}\n",
      "[Train Epoch]1/2 [Time]8726.48 [Step]10850 [Batch]108500 [Speed]80.43ms/step [Loss]7.5672 [Metrics]{'train_loss:7.567155 train_acc:0.086569 lr:0.000849 grad_accum:10.000000 total_samples:3472832.000000'}\n",
      "[Train Epoch]1/2 [Time]8767.17 [Step]10900 [Batch]109000 [Speed]80.43ms/step [Loss]7.5666 [Metrics]{'train_loss:7.566586 train_acc:0.086566 lr:0.000847 grad_accum:10.000000 total_samples:3488832.000000'}\n",
      "[Train Epoch]1/2 [Time]8807.74 [Step]10950 [Batch]109500 [Speed]80.44ms/step [Loss]7.5660 [Metrics]{'train_loss:7.566031 train_acc:0.086571 lr:0.000845 grad_accum:10.000000 total_samples:3504832.000000'}\n",
      "[Train Epoch]1/2 [Time]8848.17 [Step]11000 [Batch]110000 [Speed]80.44ms/step [Loss]7.5658 [Metrics]{'train_loss:7.565756 train_acc:0.086566 lr:0.000843 grad_accum:10.000000 total_samples:3520832.000000'}\n",
      "[Train Epoch]1/2 [Time]8888.29 [Step]11050 [Batch]110500 [Speed]80.44ms/step [Loss]7.5655 [Metrics]{'train_loss:7.565477 train_acc:0.086562 lr:0.000841 grad_accum:10.000000 total_samples:3536832.000000'}\n",
      "[Train Epoch]1/2 [Time]8928.42 [Step]11100 [Batch]111000 [Speed]80.44ms/step [Loss]7.5654 [Metrics]{'train_loss:7.565445 train_acc:0.086562 lr:0.000839 grad_accum:10.000000 total_samples:3552832.000000'}\n",
      "[Train Epoch]1/2 [Time]8968.88 [Step]11150 [Batch]111500 [Speed]80.44ms/step [Loss]7.5651 [Metrics]{'train_loss:7.565112 train_acc:0.086568 lr:0.000837 grad_accum:10.000000 total_samples:3568832.000000'}\n",
      "[Train Epoch]1/2 [Time]9009.46 [Step]11200 [Batch]112000 [Speed]80.44ms/step [Loss]7.5645 [Metrics]{'train_loss:7.564523 train_acc:0.086582 lr:0.000835 grad_accum:10.000000 total_samples:3584832.000000'}\n",
      "[Train Epoch]1/2 [Time]9050.14 [Step]11250 [Batch]112500 [Speed]80.45ms/step [Loss]7.5638 [Metrics]{'train_loss:7.563829 train_acc:0.086579 lr:0.000833 grad_accum:10.000000 total_samples:3600832.000000'}\n",
      "[Train Epoch]1/2 [Time]9090.60 [Step]11300 [Batch]113000 [Speed]80.45ms/step [Loss]7.5633 [Metrics]{'train_loss:7.563294 train_acc:0.086589 lr:0.000831 grad_accum:10.000000 total_samples:3616832.000000'}\n",
      "[Train Epoch]1/2 [Time]9130.95 [Step]11350 [Batch]113500 [Speed]80.45ms/step [Loss]7.5630 [Metrics]{'train_loss:7.562962 train_acc:0.086582 lr:0.000830 grad_accum:10.000000 total_samples:3632832.000000'}\n",
      "[Train Epoch]1/2 [Time]9171.12 [Step]11400 [Batch]114000 [Speed]80.45ms/step [Loss]7.5624 [Metrics]{'train_loss:7.562355 train_acc:0.086585 lr:0.000828 grad_accum:10.000000 total_samples:3648832.000000'}\n",
      "[Train Epoch]1/2 [Time]9211.21 [Step]11450 [Batch]114500 [Speed]80.45ms/step [Loss]7.5620 [Metrics]{'train_loss:7.562014 train_acc:0.086577 lr:0.000826 grad_accum:10.000000 total_samples:3664832.000000'}\n",
      "[Train Epoch]1/2 [Time]9251.37 [Step]11500 [Batch]115000 [Speed]80.45ms/step [Loss]7.5615 [Metrics]{'train_loss:7.561531 train_acc:0.086585 lr:0.000824 grad_accum:10.000000 total_samples:3680832.000000'}\n",
      "[Train Epoch]1/2 [Time]9292.06 [Step]11550 [Batch]115500 [Speed]80.45ms/step [Loss]7.5609 [Metrics]{'train_loss:7.560855 train_acc:0.086589 lr:0.000822 grad_accum:10.000000 total_samples:3696832.000000'}\n",
      "[Train Epoch]1/2 [Time]9332.27 [Step]11600 [Batch]116000 [Speed]80.45ms/step [Loss]7.5600 [Metrics]{'train_loss:7.560029 train_acc:0.086596 lr:0.000821 grad_accum:10.000000 total_samples:3712832.000000'}\n",
      "[Train Epoch]1/2 [Time]9372.71 [Step]11650 [Batch]116500 [Speed]80.45ms/step [Loss]7.5597 [Metrics]{'train_loss:7.559698 train_acc:0.086596 lr:0.000819 grad_accum:10.000000 total_samples:3728832.000000'}\n",
      "[Train Epoch]1/2 [Time]9412.73 [Step]11700 [Batch]117000 [Speed]80.45ms/step [Loss]7.5593 [Metrics]{'train_loss:7.559280 train_acc:0.086597 lr:0.000817 grad_accum:10.000000 total_samples:3744832.000000'}\n",
      "[Train Epoch]1/2 [Time]9449.88 [Step]11750 [Batch]117500 [Speed]80.42ms/step [Loss]7.5588 [Metrics]{'train_loss:7.558816 train_acc:0.086603 lr:0.000815 grad_accum:10.000000 total_samples:3760832.000000'}\n",
      "[Train Epoch]1/2 [Time]9487.01 [Step]11800 [Batch]118000 [Speed]80.40ms/step [Loss]7.5588 [Metrics]{'train_loss:7.558808 train_acc:0.086606 lr:0.000814 grad_accum:10.000000 total_samples:3776832.000000'}\n",
      "[Train Epoch]1/2 [Time]9524.17 [Step]11850 [Batch]118500 [Speed]80.37ms/step [Loss]7.5586 [Metrics]{'train_loss:7.558589 train_acc:0.086607 lr:0.000812 grad_accum:10.000000 total_samples:3792832.000000'}\n",
      "[Train Epoch]1/2 [Time]9561.24 [Step]11900 [Batch]119000 [Speed]80.35ms/step [Loss]7.5581 [Metrics]{'train_loss:7.558064 train_acc:0.086600 lr:0.000810 grad_accum:10.000000 total_samples:3808832.000000'}\n",
      "[Train Epoch]1/2 [Time]9598.38 [Step]11950 [Batch]119500 [Speed]80.32ms/step [Loss]7.5575 [Metrics]{'train_loss:7.557519 train_acc:0.086603 lr:0.000809 grad_accum:10.000000 total_samples:3824832.000000'}\n",
      "[Train Epoch]1/2 [Time]9635.51 [Step]12000 [Batch]120000 [Speed]80.30ms/step [Loss]7.5572 [Metrics]{'train_loss:7.557238 train_acc:0.086603 lr:0.000807 grad_accum:10.000000 total_samples:3840832.000000'}\n",
      "[Train Epoch]1/2 [Time]9672.63 [Step]12050 [Batch]120500 [Speed]80.27ms/step [Loss]7.5564 [Metrics]{'train_loss:7.556400 train_acc:0.086622 lr:0.000805 grad_accum:10.000000 total_samples:3856832.000000'}\n",
      "[Train Epoch]1/2 [Time]9709.84 [Step]12100 [Batch]121000 [Speed]80.25ms/step [Loss]7.5559 [Metrics]{'train_loss:7.555886 train_acc:0.086630 lr:0.000804 grad_accum:10.000000 total_samples:3872832.000000'}\n",
      "[Train Epoch]1/2 [Time]9746.97 [Step]12150 [Batch]121500 [Speed]80.22ms/step [Loss]7.5556 [Metrics]{'train_loss:7.555573 train_acc:0.086625 lr:0.000802 grad_accum:10.000000 total_samples:3888832.000000'}\n",
      "[Train Epoch]1/2 [Time]9784.07 [Step]12200 [Batch]122000 [Speed]80.20ms/step [Loss]7.5550 [Metrics]{'train_loss:7.555032 train_acc:0.086627 lr:0.000800 grad_accum:10.000000 total_samples:3904832.000000'}\n",
      "[Train Epoch]1/2 [Time]9821.24 [Step]12250 [Batch]122500 [Speed]80.17ms/step [Loss]7.5546 [Metrics]{'train_loss:7.554588 train_acc:0.086622 lr:0.000799 grad_accum:10.000000 total_samples:3920832.000000'}\n",
      "[Train Epoch]1/2 [Time]9858.38 [Step]12300 [Batch]123000 [Speed]80.15ms/step [Loss]7.5539 [Metrics]{'train_loss:7.553869 train_acc:0.086626 lr:0.000797 grad_accum:10.000000 total_samples:3936832.000000'}\n",
      "[Train Epoch]1/2 [Time]9895.54 [Step]12350 [Batch]123500 [Speed]80.13ms/step [Loss]7.5536 [Metrics]{'train_loss:7.553567 train_acc:0.086628 lr:0.000795 grad_accum:10.000000 total_samples:3952832.000000'}\n",
      "[Train Epoch]1/2 [Time]9932.60 [Step]12400 [Batch]124000 [Speed]80.10ms/step [Loss]7.5531 [Metrics]{'train_loss:7.553139 train_acc:0.086641 lr:0.000794 grad_accum:10.000000 total_samples:3968832.000000'}\n",
      "[Train Epoch]1/2 [Time]9969.73 [Step]12450 [Batch]124500 [Speed]80.08ms/step [Loss]7.5525 [Metrics]{'train_loss:7.552481 train_acc:0.086650 lr:0.000792 grad_accum:10.000000 total_samples:3984832.000000'}\n",
      "Saving checkpoint for epoch 1 at step 125000 on path ../2_Models/model_bert4rec_complete_0.8.4/checkpoints/\n",
      "[Train Epoch]1/2 [Time]10015.23 [Step]12500 [Batch]125000 [Speed]80.12ms/step [Loss]7.5521 [Metrics]{'train_loss:7.552140 train_acc:0.086640 lr:0.000791 grad_accum:10.000000 total_samples:4000832.000000'}\n",
      "[Train Epoch]1/2 [Time]10052.20 [Step]12550 [Batch]125500 [Speed]80.10ms/step [Loss]7.5518 [Metrics]{'train_loss:7.551819 train_acc:0.086642 lr:0.000789 grad_accum:10.000000 total_samples:4016832.000000'}\n",
      "[Train Epoch]1/2 [Time]10089.41 [Step]12600 [Batch]126000 [Speed]80.07ms/step [Loss]7.5512 [Metrics]{'train_loss:7.551206 train_acc:0.086654 lr:0.000787 grad_accum:10.000000 total_samples:4032832.000000'}\n",
      "[Train Epoch]1/2 [Time]10126.57 [Step]12650 [Batch]126500 [Speed]80.05ms/step [Loss]7.5507 [Metrics]{'train_loss:7.550713 train_acc:0.086660 lr:0.000786 grad_accum:10.000000 total_samples:4048832.000000'}\n",
      "[Train Epoch]1/2 [Time]10163.68 [Step]12700 [Batch]127000 [Speed]80.03ms/step [Loss]7.5502 [Metrics]{'train_loss:7.550206 train_acc:0.086665 lr:0.000784 grad_accum:10.000000 total_samples:4064832.000000'}\n",
      "[Train Epoch]1/2 [Time]10200.82 [Step]12750 [Batch]127500 [Speed]80.01ms/step [Loss]7.5498 [Metrics]{'train_loss:7.549808 train_acc:0.086662 lr:0.000783 grad_accum:10.000000 total_samples:4080832.000000'}\n",
      "[Train Epoch]1/2 [Time]10238.00 [Step]12800 [Batch]128000 [Speed]79.98ms/step [Loss]7.5493 [Metrics]{'train_loss:7.549334 train_acc:0.086662 lr:0.000781 grad_accum:10.000000 total_samples:4096832.000000'}\n",
      "[Train Epoch]1/2 [Time]10275.17 [Step]12850 [Batch]128500 [Speed]79.96ms/step [Loss]7.5490 [Metrics]{'train_loss:7.549013 train_acc:0.086669 lr:0.000780 grad_accum:10.000000 total_samples:4112832.000000'}\n",
      "[Train Epoch]1/2 [Time]10312.29 [Step]12900 [Batch]129000 [Speed]79.94ms/step [Loss]7.5486 [Metrics]{'train_loss:7.548640 train_acc:0.086676 lr:0.000778 grad_accum:10.000000 total_samples:4128832.000000'}\n",
      "[Train Epoch]1/2 [Time]10349.42 [Step]12950 [Batch]129500 [Speed]79.92ms/step [Loss]7.5480 [Metrics]{'train_loss:7.548026 train_acc:0.086678 lr:0.000777 grad_accum:10.000000 total_samples:4144832.000000'}\n",
      "[Train Epoch]1/2 [Time]10386.55 [Step]13000 [Batch]130000 [Speed]79.90ms/step [Loss]7.5476 [Metrics]{'train_loss:7.547555 train_acc:0.086677 lr:0.000775 grad_accum:10.000000 total_samples:4160832.000000'}\n",
      "[Train Epoch]1/2 [Time]10423.62 [Step]13050 [Batch]130500 [Speed]79.87ms/step [Loss]7.5472 [Metrics]{'train_loss:7.547194 train_acc:0.086675 lr:0.000774 grad_accum:10.000000 total_samples:4176832.000000'}\n",
      "[Train Epoch]1/2 [Time]10460.82 [Step]13100 [Batch]131000 [Speed]79.85ms/step [Loss]7.5470 [Metrics]{'train_loss:7.546962 train_acc:0.086681 lr:0.000772 grad_accum:10.000000 total_samples:4192832.000000'}\n",
      "[Train Epoch]1/2 [Time]10497.94 [Step]13150 [Batch]131500 [Speed]79.83ms/step [Loss]7.5467 [Metrics]{'train_loss:7.546693 train_acc:0.086689 lr:0.000771 grad_accum:10.000000 total_samples:4208832.000000'}\n",
      "[Train Epoch]1/2 [Time]10535.07 [Step]13200 [Batch]132000 [Speed]79.81ms/step [Loss]7.5463 [Metrics]{'train_loss:7.546326 train_acc:0.086706 lr:0.000769 grad_accum:10.000000 total_samples:4224832.000000'}\n",
      "[Train Epoch]1/2 [Time]10572.22 [Step]13250 [Batch]132500 [Speed]79.79ms/step [Loss]7.5462 [Metrics]{'train_loss:7.546226 train_acc:0.086703 lr:0.000768 grad_accum:10.000000 total_samples:4240832.000000'}\n",
      "[Train Epoch]1/2 [Time]10609.42 [Step]13300 [Batch]133000 [Speed]79.77ms/step [Loss]7.5459 [Metrics]{'train_loss:7.545885 train_acc:0.086717 lr:0.000766 grad_accum:10.000000 total_samples:4256832.000000'}\n",
      "[Train Epoch]1/2 [Time]10646.55 [Step]13350 [Batch]133500 [Speed]79.75ms/step [Loss]7.5453 [Metrics]{'train_loss:7.545274 train_acc:0.086720 lr:0.000765 grad_accum:10.000000 total_samples:4272832.000000'}\n",
      "[Train Epoch]1/2 [Time]10683.67 [Step]13400 [Batch]134000 [Speed]79.73ms/step [Loss]7.5450 [Metrics]{'train_loss:7.544991 train_acc:0.086712 lr:0.000764 grad_accum:10.000000 total_samples:4288832.000000'}\n",
      "[Train Epoch]1/2 [Time]10720.80 [Step]13450 [Batch]134500 [Speed]79.71ms/step [Loss]7.5448 [Metrics]{'train_loss:7.544765 train_acc:0.086706 lr:0.000762 grad_accum:10.000000 total_samples:4304832.000000'}\n",
      "[Train Epoch]1/2 [Time]10757.92 [Step]13500 [Batch]135000 [Speed]79.69ms/step [Loss]7.5443 [Metrics]{'train_loss:7.544323 train_acc:0.086700 lr:0.000761 grad_accum:10.000000 total_samples:4320832.000000'}\n",
      "[Train Epoch]1/2 [Time]10795.04 [Step]13550 [Batch]135500 [Speed]79.67ms/step [Loss]7.5438 [Metrics]{'train_loss:7.543755 train_acc:0.086700 lr:0.000759 grad_accum:10.000000 total_samples:4336832.000000'}\n",
      "[Train Epoch]1/2 [Time]10832.14 [Step]13600 [Batch]136000 [Speed]79.65ms/step [Loss]7.5436 [Metrics]{'train_loss:7.543564 train_acc:0.086694 lr:0.000758 grad_accum:10.000000 total_samples:4352832.000000'}\n",
      "[Train Epoch]1/2 [Time]10869.25 [Step]13650 [Batch]136500 [Speed]79.63ms/step [Loss]7.5431 [Metrics]{'train_loss:7.543090 train_acc:0.086689 lr:0.000757 grad_accum:10.000000 total_samples:4368832.000000'}\n",
      "[Train Epoch]1/2 [Time]10906.37 [Step]13700 [Batch]137000 [Speed]79.61ms/step [Loss]7.5426 [Metrics]{'train_loss:7.542570 train_acc:0.086688 lr:0.000755 grad_accum:10.000000 total_samples:4384832.000000'}\n",
      "[Train Epoch]1/2 [Time]10943.52 [Step]13750 [Batch]137500 [Speed]79.59ms/step [Loss]7.5421 [Metrics]{'train_loss:7.542054 train_acc:0.086691 lr:0.000754 grad_accum:10.000000 total_samples:4400832.000000'}\n",
      "[Train Epoch]1/2 [Time]10980.65 [Step]13800 [Batch]138000 [Speed]79.57ms/step [Loss]7.5418 [Metrics]{'train_loss:7.541759 train_acc:0.086697 lr:0.000752 grad_accum:10.000000 total_samples:4416832.000000'}\n",
      "[Train Epoch]1/2 [Time]11017.79 [Step]13850 [Batch]138500 [Speed]79.55ms/step [Loss]7.5415 [Metrics]{'train_loss:7.541522 train_acc:0.086703 lr:0.000751 grad_accum:10.000000 total_samples:4432832.000000'}\n",
      "[Train Epoch]1/2 [Time]11054.97 [Step]13900 [Batch]139000 [Speed]79.53ms/step [Loss]7.5413 [Metrics]{'train_loss:7.541262 train_acc:0.086704 lr:0.000750 grad_accum:10.000000 total_samples:4448832.000000'}\n",
      "[Train Epoch]1/2 [Time]11092.16 [Step]13950 [Batch]139500 [Speed]79.51ms/step [Loss]7.5407 [Metrics]{'train_loss:7.540744 train_acc:0.086710 lr:0.000748 grad_accum:10.000000 total_samples:4464832.000000'}\n",
      "[Train Epoch]1/2 [Time]11129.34 [Step]14000 [Batch]140000 [Speed]79.50ms/step [Loss]7.5402 [Metrics]{'train_loss:7.540245 train_acc:0.086708 lr:0.000747 grad_accum:10.000000 total_samples:4480832.000000'}\n",
      "[Train Epoch]1/2 [Time]11166.52 [Step]14050 [Batch]140500 [Speed]79.48ms/step [Loss]7.5398 [Metrics]{'train_loss:7.539841 train_acc:0.086709 lr:0.000746 grad_accum:10.000000 total_samples:4496832.000000'}\n",
      "[Train Epoch]1/2 [Time]11203.60 [Step]14100 [Batch]141000 [Speed]79.46ms/step [Loss]7.5393 [Metrics]{'train_loss:7.539264 train_acc:0.086703 lr:0.000744 grad_accum:10.000000 total_samples:4512832.000000'}\n",
      "[Train Epoch]1/2 [Time]11240.71 [Step]14150 [Batch]141500 [Speed]79.44ms/step [Loss]7.5388 [Metrics]{'train_loss:7.538764 train_acc:0.086701 lr:0.000743 grad_accum:10.000000 total_samples:4528832.000000'}\n",
      "[Train Epoch]1/2 [Time]11277.87 [Step]14200 [Batch]142000 [Speed]79.42ms/step [Loss]7.5383 [Metrics]{'train_loss:7.538253 train_acc:0.086704 lr:0.000742 grad_accum:10.000000 total_samples:4544832.000000'}\n",
      "[Train Epoch]1/2 [Time]11314.95 [Step]14250 [Batch]142500 [Speed]79.40ms/step [Loss]7.5378 [Metrics]{'train_loss:7.537801 train_acc:0.086709 lr:0.000740 grad_accum:10.000000 total_samples:4560832.000000'}\n",
      "[Train Epoch]1/2 [Time]11352.08 [Step]14300 [Batch]143000 [Speed]79.39ms/step [Loss]7.5374 [Metrics]{'train_loss:7.537398 train_acc:0.086715 lr:0.000739 grad_accum:10.000000 total_samples:4576832.000000'}\n",
      "[Train Epoch]1/2 [Time]11389.21 [Step]14350 [Batch]143500 [Speed]79.37ms/step [Loss]7.5370 [Metrics]{'train_loss:7.537029 train_acc:0.086730 lr:0.000738 grad_accum:10.000000 total_samples:4592832.000000'}\n",
      "[Train Epoch]1/2 [Time]11426.30 [Step]14400 [Batch]144000 [Speed]79.35ms/step [Loss]7.5369 [Metrics]{'train_loss:7.536891 train_acc:0.086737 lr:0.000737 grad_accum:10.000000 total_samples:4608832.000000'}\n",
      "[Train Epoch]1/2 [Time]11463.40 [Step]14450 [Batch]144500 [Speed]79.33ms/step [Loss]7.5363 [Metrics]{'train_loss:7.536302 train_acc:0.086747 lr:0.000735 grad_accum:10.000000 total_samples:4624832.000000'}\n",
      "[Train Epoch]1/2 [Time]11500.51 [Step]14500 [Batch]145000 [Speed]79.31ms/step [Loss]7.5357 [Metrics]{'train_loss:7.535691 train_acc:0.086761 lr:0.000734 grad_accum:10.000000 total_samples:4640832.000000'}\n",
      "[Train Epoch]1/2 [Time]11537.68 [Step]14550 [Batch]145500 [Speed]79.30ms/step [Loss]7.5353 [Metrics]{'train_loss:7.535295 train_acc:0.086770 lr:0.000733 grad_accum:10.000000 total_samples:4656832.000000'}\n",
      "[Train Epoch]1/2 [Time]11574.84 [Step]14600 [Batch]146000 [Speed]79.28ms/step [Loss]7.5347 [Metrics]{'train_loss:7.534744 train_acc:0.086786 lr:0.000732 grad_accum:10.000000 total_samples:4672832.000000'}\n",
      "[Train Epoch]1/2 [Time]11611.92 [Step]14650 [Batch]146500 [Speed]79.26ms/step [Loss]7.5343 [Metrics]{'train_loss:7.534331 train_acc:0.086803 lr:0.000730 grad_accum:10.000000 total_samples:4688832.000000'}\n",
      "[Train Epoch]1/2 [Time]11649.01 [Step]14700 [Batch]147000 [Speed]79.24ms/step [Loss]7.5340 [Metrics]{'train_loss:7.534024 train_acc:0.086799 lr:0.000729 grad_accum:10.000000 total_samples:4704832.000000'}\n",
      "[Train Epoch]1/2 [Time]11686.09 [Step]14750 [Batch]147500 [Speed]79.23ms/step [Loss]7.5335 [Metrics]{'train_loss:7.533512 train_acc:0.086807 lr:0.000728 grad_accum:10.000000 total_samples:4720832.000000'}\n",
      "[Train Epoch]1/2 [Time]11723.23 [Step]14800 [Batch]148000 [Speed]79.21ms/step [Loss]7.5332 [Metrics]{'train_loss:7.533183 train_acc:0.086805 lr:0.000727 grad_accum:10.000000 total_samples:4736832.000000'}\n",
      "[Train Epoch]1/2 [Time]11760.37 [Step]14850 [Batch]148500 [Speed]79.19ms/step [Loss]7.5328 [Metrics]{'train_loss:7.532759 train_acc:0.086813 lr:0.000725 grad_accum:10.000000 total_samples:4752832.000000'}\n",
      "[Train Epoch]1/2 [Time]11797.52 [Step]14900 [Batch]149000 [Speed]79.18ms/step [Loss]7.5323 [Metrics]{'train_loss:7.532309 train_acc:0.086814 lr:0.000724 grad_accum:10.000000 total_samples:4768832.000000'}\n",
      "[Train Epoch]1/2 [Time]11834.63 [Step]14950 [Batch]149500 [Speed]79.16ms/step [Loss]7.5318 [Metrics]{'train_loss:7.531779 train_acc:0.086818 lr:0.000723 grad_accum:10.000000 total_samples:4784832.000000'}\n",
      "Saving checkpoint for epoch 1 at step 150000 on path ../2_Models/model_bert4rec_complete_0.8.4/checkpoints/\n",
      "[Train Epoch]1/2 [Time]11880.10 [Step]15000 [Batch]150000 [Speed]79.20ms/step [Loss]7.5316 [Metrics]{'train_loss:7.531594 train_acc:0.086822 lr:0.000722 grad_accum:10.000000 total_samples:4800832.000000'}\n",
      "[Train Epoch]1/2 [Time]11917.10 [Step]15050 [Batch]150500 [Speed]79.18ms/step [Loss]7.5314 [Metrics]{'train_loss:7.531389 train_acc:0.086823 lr:0.000720 grad_accum:10.000000 total_samples:4816832.000000'}\n",
      "[Train Epoch]1/2 [Time]11954.23 [Step]15100 [Batch]151000 [Speed]79.17ms/step [Loss]7.5309 [Metrics]{'train_loss:7.530903 train_acc:0.086833 lr:0.000719 grad_accum:10.000000 total_samples:4832832.000000'}\n",
      "[Train Epoch]1/2 [Time]11991.44 [Step]15150 [Batch]151500 [Speed]79.15ms/step [Loss]7.5305 [Metrics]{'train_loss:7.530536 train_acc:0.086840 lr:0.000718 grad_accum:10.000000 total_samples:4848832.000000'}\n",
      "[Train Epoch]1/2 [Time]12028.59 [Step]15200 [Batch]152000 [Speed]79.14ms/step [Loss]7.5302 [Metrics]{'train_loss:7.530222 train_acc:0.086843 lr:0.000717 grad_accum:10.000000 total_samples:4864832.000000'}\n",
      "[Train Epoch]1/2 [Time]12065.75 [Step]15250 [Batch]152500 [Speed]79.12ms/step [Loss]7.5299 [Metrics]{'train_loss:7.529949 train_acc:0.086842 lr:0.000716 grad_accum:10.000000 total_samples:4880832.000000'}\n",
      "[Train Epoch]1/2 [Time]12102.90 [Step]15300 [Batch]153000 [Speed]79.10ms/step [Loss]7.5296 [Metrics]{'train_loss:7.529644 train_acc:0.086841 lr:0.000715 grad_accum:10.000000 total_samples:4896832.000000'}\n",
      "[Train Epoch]1/2 [Time]12140.02 [Step]15350 [Batch]153500 [Speed]79.09ms/step [Loss]7.5293 [Metrics]{'train_loss:7.529257 train_acc:0.086846 lr:0.000713 grad_accum:10.000000 total_samples:4912832.000000'}\n",
      "[Train Epoch]1/2 [Time]12177.22 [Step]15400 [Batch]154000 [Speed]79.07ms/step [Loss]7.5289 [Metrics]{'train_loss:7.528900 train_acc:0.086853 lr:0.000712 grad_accum:10.000000 total_samples:4928832.000000'}\n",
      "[Train Epoch]1/2 [Time]12214.42 [Step]15450 [Batch]154500 [Speed]79.06ms/step [Loss]7.5287 [Metrics]{'train_loss:7.528663 train_acc:0.086862 lr:0.000711 grad_accum:10.000000 total_samples:4944832.000000'}\n",
      "[Train Epoch]1/2 [Time]12251.61 [Step]15500 [Batch]155000 [Speed]79.04ms/step [Loss]7.5282 [Metrics]{'train_loss:7.528244 train_acc:0.086872 lr:0.000710 grad_accum:10.000000 total_samples:4960832.000000'}\n",
      "[Train Epoch]1/2 [Time]12288.77 [Step]15550 [Batch]155500 [Speed]79.03ms/step [Loss]7.5278 [Metrics]{'train_loss:7.527850 train_acc:0.086872 lr:0.000709 grad_accum:10.000000 total_samples:4976832.000000'}\n",
      "[Train Epoch]1/2 [Time]12325.95 [Step]15600 [Batch]156000 [Speed]79.01ms/step [Loss]7.5275 [Metrics]{'train_loss:7.527477 train_acc:0.086864 lr:0.000708 grad_accum:10.000000 total_samples:4992832.000000'}\n",
      "[Train Epoch]1/2 [Time]12363.14 [Step]15650 [Batch]156500 [Speed]79.00ms/step [Loss]7.5271 [Metrics]{'train_loss:7.527110 train_acc:0.086854 lr:0.000707 grad_accum:10.000000 total_samples:5008832.000000'}\n",
      "[Train Epoch]1/2 [Time]12400.34 [Step]15700 [Batch]157000 [Speed]78.98ms/step [Loss]7.5267 [Metrics]{'train_loss:7.526669 train_acc:0.086855 lr:0.000705 grad_accum:10.000000 total_samples:5024832.000000'}\n",
      "[Train Epoch]1/2 [Time]12437.55 [Step]15750 [Batch]157500 [Speed]78.97ms/step [Loss]7.5262 [Metrics]{'train_loss:7.526201 train_acc:0.086852 lr:0.000704 grad_accum:10.000000 total_samples:5040832.000000'}\n",
      "[Train Epoch]1/2 [Time]12474.70 [Step]15800 [Batch]158000 [Speed]78.95ms/step [Loss]7.5256 [Metrics]{'train_loss:7.525645 train_acc:0.086851 lr:0.000703 grad_accum:10.000000 total_samples:5056832.000000'}\n",
      "[Train Epoch]1/2 [Time]12511.82 [Step]15850 [Batch]158500 [Speed]78.94ms/step [Loss]7.5252 [Metrics]{'train_loss:7.525207 train_acc:0.086851 lr:0.000702 grad_accum:10.000000 total_samples:5072832.000000'}\n",
      "[Train Epoch]1/2 [Time]12548.99 [Step]15900 [Batch]159000 [Speed]78.92ms/step [Loss]7.5247 [Metrics]{'train_loss:7.524667 train_acc:0.086848 lr:0.000701 grad_accum:10.000000 total_samples:5088832.000000'}\n",
      "[Train Epoch]1/2 [Time]12586.22 [Step]15950 [Batch]159500 [Speed]78.91ms/step [Loss]7.5240 [Metrics]{'train_loss:7.524041 train_acc:0.086855 lr:0.000700 grad_accum:10.000000 total_samples:5104832.000000'}\n",
      "[Train Epoch]1/2 [Time]12623.43 [Step]16000 [Batch]160000 [Speed]78.90ms/step [Loss]7.5235 [Metrics]{'train_loss:7.523469 train_acc:0.086861 lr:0.000699 grad_accum:10.000000 total_samples:5120832.000000'}\n",
      "[Train Epoch]1/2 [Time]12660.58 [Step]16050 [Batch]160500 [Speed]78.88ms/step [Loss]7.5229 [Metrics]{'train_loss:7.522878 train_acc:0.086861 lr:0.000698 grad_accum:10.000000 total_samples:5136832.000000'}\n",
      "[Train Epoch]1/2 [Time]12697.74 [Step]16100 [Batch]161000 [Speed]78.87ms/step [Loss]7.5224 [Metrics]{'train_loss:7.522368 train_acc:0.086860 lr:0.000697 grad_accum:10.000000 total_samples:5152832.000000'}\n",
      "[Train Epoch]1/2 [Time]12734.86 [Step]16150 [Batch]161500 [Speed]78.85ms/step [Loss]7.5218 [Metrics]{'train_loss:7.521848 train_acc:0.086865 lr:0.000696 grad_accum:10.000000 total_samples:5168832.000000'}\n",
      "[Train Epoch]1/2 [Time]12772.05 [Step]16200 [Batch]162000 [Speed]78.84ms/step [Loss]7.5215 [Metrics]{'train_loss:7.521472 train_acc:0.086862 lr:0.000694 grad_accum:10.000000 total_samples:5184832.000000'}\n",
      "[Train Epoch]1/2 [Time]12809.25 [Step]16250 [Batch]162500 [Speed]78.83ms/step [Loss]7.5211 [Metrics]{'train_loss:7.521065 train_acc:0.086860 lr:0.000693 grad_accum:10.000000 total_samples:5200832.000000'}\n",
      "[Train Epoch]1/2 [Time]12846.39 [Step]16300 [Batch]163000 [Speed]78.81ms/step [Loss]7.5206 [Metrics]{'train_loss:7.520627 train_acc:0.086864 lr:0.000692 grad_accum:10.000000 total_samples:5216832.000000'}\n",
      "[Train Epoch]1/2 [Time]12883.50 [Step]16350 [Batch]163500 [Speed]78.80ms/step [Loss]7.5206 [Metrics]{'train_loss:7.520551 train_acc:0.086865 lr:0.000691 grad_accum:10.000000 total_samples:5232832.000000'}\n",
      "[Train Epoch]1/2 [Time]12920.61 [Step]16400 [Batch]164000 [Speed]78.78ms/step [Loss]7.5204 [Metrics]{'train_loss:7.520373 train_acc:0.086867 lr:0.000690 grad_accum:10.000000 total_samples:5248832.000000'}\n",
      "[Train Epoch]1/2 [Time]12957.81 [Step]16450 [Batch]164500 [Speed]78.77ms/step [Loss]7.5199 [Metrics]{'train_loss:7.519943 train_acc:0.086872 lr:0.000689 grad_accum:10.000000 total_samples:5264832.000000'}\n",
      "[Train Epoch]1/2 [Time]12995.02 [Step]16500 [Batch]165000 [Speed]78.76ms/step [Loss]7.5197 [Metrics]{'train_loss:7.519667 train_acc:0.086874 lr:0.000688 grad_accum:10.000000 total_samples:5280832.000000'}\n",
      "[Train Epoch]1/2 [Time]13032.23 [Step]16550 [Batch]165500 [Speed]78.74ms/step [Loss]7.5193 [Metrics]{'train_loss:7.519340 train_acc:0.086878 lr:0.000687 grad_accum:10.000000 total_samples:5296832.000000'}\n",
      "[Train Epoch]1/2 [Time]13069.43 [Step]16600 [Batch]166000 [Speed]78.73ms/step [Loss]7.5190 [Metrics]{'train_loss:7.518965 train_acc:0.086888 lr:0.000686 grad_accum:10.000000 total_samples:5312832.000000'}\n",
      "[Train Epoch]1/2 [Time]13106.55 [Step]16650 [Batch]166500 [Speed]78.72ms/step [Loss]7.5187 [Metrics]{'train_loss:7.518712 train_acc:0.086894 lr:0.000685 grad_accum:10.000000 total_samples:5328832.000000'}\n",
      "[Train Epoch]1/2 [Time]13143.68 [Step]16700 [Batch]167000 [Speed]78.70ms/step [Loss]7.5184 [Metrics]{'train_loss:7.518439 train_acc:0.086899 lr:0.000684 grad_accum:10.000000 total_samples:5344832.000000'}\n",
      "[Train Epoch]1/2 [Time]13180.85 [Step]16750 [Batch]167500 [Speed]78.69ms/step [Loss]7.5184 [Metrics]{'train_loss:7.518381 train_acc:0.086896 lr:0.000683 grad_accum:10.000000 total_samples:5360832.000000'}\n",
      "[Train Epoch]1/2 [Time]13218.04 [Step]16800 [Batch]168000 [Speed]78.68ms/step [Loss]7.5179 [Metrics]{'train_loss:7.517936 train_acc:0.086902 lr:0.000682 grad_accum:10.000000 total_samples:5376832.000000'}\n",
      "[Train Epoch]1/2 [Time]13255.20 [Step]16850 [Batch]168500 [Speed]78.67ms/step [Loss]7.5175 [Metrics]{'train_loss:7.517489 train_acc:0.086902 lr:0.000681 grad_accum:10.000000 total_samples:5392832.000000'}\n",
      "[Train Epoch]1/2 [Time]13292.39 [Step]16900 [Batch]169000 [Speed]78.65ms/step [Loss]7.5172 [Metrics]{'train_loss:7.517179 train_acc:0.086901 lr:0.000680 grad_accum:10.000000 total_samples:5408832.000000'}\n",
      "[Train Epoch]1/2 [Time]13329.56 [Step]16950 [Batch]169500 [Speed]78.64ms/step [Loss]7.5169 [Metrics]{'train_loss:7.516859 train_acc:0.086905 lr:0.000679 grad_accum:10.000000 total_samples:5424832.000000'}\n",
      "[Train Epoch]1/2 [Time]13366.76 [Step]17000 [Batch]170000 [Speed]78.63ms/step [Loss]7.5164 [Metrics]{'train_loss:7.516387 train_acc:0.086912 lr:0.000678 grad_accum:10.000000 total_samples:5440832.000000'}\n",
      "[Train Epoch]1/2 [Time]13403.96 [Step]17050 [Batch]170500 [Speed]78.62ms/step [Loss]7.5159 [Metrics]{'train_loss:7.515941 train_acc:0.086908 lr:0.000677 grad_accum:10.000000 total_samples:5456832.000000'}\n",
      "[Train Epoch]1/2 [Time]13441.20 [Step]17100 [Batch]171000 [Speed]78.60ms/step [Loss]7.5154 [Metrics]{'train_loss:7.515443 train_acc:0.086909 lr:0.000676 grad_accum:10.000000 total_samples:5472832.000000'}\n",
      "[Train Epoch]1/2 [Time]13478.39 [Step]17150 [Batch]171500 [Speed]78.59ms/step [Loss]7.5148 [Metrics]{'train_loss:7.514849 train_acc:0.086909 lr:0.000675 grad_accum:10.000000 total_samples:5488832.000000'}\n",
      "[Train Epoch]1/2 [Time]13515.57 [Step]17200 [Batch]172000 [Speed]78.58ms/step [Loss]7.5143 [Metrics]{'train_loss:7.514283 train_acc:0.086907 lr:0.000674 grad_accum:10.000000 total_samples:5504832.000000'}\n",
      "[Train Epoch]1/2 [Time]13552.71 [Step]17250 [Batch]172500 [Speed]78.57ms/step [Loss]7.5138 [Metrics]{'train_loss:7.513789 train_acc:0.086906 lr:0.000673 grad_accum:10.000000 total_samples:5520832.000000'}\n",
      "[Train Epoch]1/2 [Time]13589.88 [Step]17300 [Batch]173000 [Speed]78.55ms/step [Loss]7.5133 [Metrics]{'train_loss:7.513308 train_acc:0.086903 lr:0.000672 grad_accum:10.000000 total_samples:5536832.000000'}\n",
      "[Train Epoch]1/2 [Time]13627.07 [Step]17350 [Batch]173500 [Speed]78.54ms/step [Loss]7.5130 [Metrics]{'train_loss:7.512950 train_acc:0.086902 lr:0.000671 grad_accum:10.000000 total_samples:5552832.000000'}\n",
      "[Train Epoch]1/2 [Time]13664.23 [Step]17400 [Batch]174000 [Speed]78.53ms/step [Loss]7.5127 [Metrics]{'train_loss:7.512740 train_acc:0.086903 lr:0.000670 grad_accum:10.000000 total_samples:5568832.000000'}\n",
      "[Train Epoch]1/2 [Time]13701.46 [Step]17450 [Batch]174500 [Speed]78.52ms/step [Loss]7.5126 [Metrics]{'train_loss:7.512640 train_acc:0.086906 lr:0.000669 grad_accum:10.000000 total_samples:5584832.000000'}\n",
      "Saving checkpoint for epoch 1 at step 175000 on path ../2_Models/model_bert4rec_complete_0.8.4/checkpoints/\n",
      "[Train Epoch]1/2 [Time]13746.84 [Step]17500 [Batch]175000 [Speed]78.55ms/step [Loss]7.5126 [Metrics]{'train_loss:7.512564 train_acc:0.086897 lr:0.000668 grad_accum:10.000000 total_samples:5600832.000000'}\n",
      "[Train Epoch]1/2 [Time]13783.87 [Step]17550 [Batch]175500 [Speed]78.54ms/step [Loss]7.5124 [Metrics]{'train_loss:7.512359 train_acc:0.086900 lr:0.000667 grad_accum:10.000000 total_samples:5616832.000000'}\n",
      "[Train Epoch]1/2 [Time]13820.98 [Step]17600 [Batch]176000 [Speed]78.53ms/step [Loss]7.5124 [Metrics]{'train_loss:7.512430 train_acc:0.086895 lr:0.000666 grad_accum:10.000000 total_samples:5632832.000000'}\n",
      "[Train Epoch]1/2 [Time]13858.18 [Step]17650 [Batch]176500 [Speed]78.52ms/step [Loss]7.5127 [Metrics]{'train_loss:7.512720 train_acc:0.086881 lr:0.000665 grad_accum:10.000000 total_samples:5648832.000000'}\n",
      "[Train Epoch]1/2 [Time]13895.35 [Step]17700 [Batch]177000 [Speed]78.50ms/step [Loss]7.5127 [Metrics]{'train_loss:7.512671 train_acc:0.086879 lr:0.000664 grad_accum:10.000000 total_samples:5664832.000000'}\n",
      "[Train Epoch]1/2 [Time]13932.50 [Step]17750 [Batch]177500 [Speed]78.49ms/step [Loss]7.5128 [Metrics]{'train_loss:7.512813 train_acc:0.086868 lr:0.000663 grad_accum:10.000000 total_samples:5680832.000000'}\n",
      "[Train Epoch]1/2 [Time]13969.67 [Step]17800 [Batch]178000 [Speed]78.48ms/step [Loss]7.5128 [Metrics]{'train_loss:7.512818 train_acc:0.086866 lr:0.000662 grad_accum:10.000000 total_samples:5696832.000000'}\n",
      "[Train Epoch]1/2 [Time]14006.87 [Step]17850 [Batch]178500 [Speed]78.47ms/step [Loss]7.5129 [Metrics]{'train_loss:7.512898 train_acc:0.086867 lr:0.000662 grad_accum:10.000000 total_samples:5712832.000000'}\n",
      "[Train Epoch]1/2 [Time]14044.09 [Step]17900 [Batch]179000 [Speed]78.46ms/step [Loss]7.5129 [Metrics]{'train_loss:7.512886 train_acc:0.086863 lr:0.000661 grad_accum:10.000000 total_samples:5728832.000000'}\n",
      "[Train Epoch]1/2 [Time]14081.28 [Step]17950 [Batch]179500 [Speed]78.45ms/step [Loss]7.5128 [Metrics]{'train_loss:7.512826 train_acc:0.086864 lr:0.000660 grad_accum:10.000000 total_samples:5744832.000000'}\n",
      "[Train Epoch]1/2 [Time]14118.45 [Step]18000 [Batch]180000 [Speed]78.44ms/step [Loss]7.5129 [Metrics]{'train_loss:7.512888 train_acc:0.086861 lr:0.000659 grad_accum:10.000000 total_samples:5760832.000000'}\n",
      "[Train Epoch]1/2 [Time]14155.58 [Step]18050 [Batch]180500 [Speed]78.42ms/step [Loss]7.5128 [Metrics]{'train_loss:7.512827 train_acc:0.086855 lr:0.000658 grad_accum:10.000000 total_samples:5776832.000000'}\n",
      "[Train Epoch]1/2 [Time]14192.71 [Step]18100 [Batch]181000 [Speed]78.41ms/step [Loss]7.5127 [Metrics]{'train_loss:7.512697 train_acc:0.086856 lr:0.000657 grad_accum:10.000000 total_samples:5792832.000000'}\n",
      "[Train Epoch]1/2 [Time]14229.87 [Step]18150 [Batch]181500 [Speed]78.40ms/step [Loss]7.5125 [Metrics]{'train_loss:7.512527 train_acc:0.086860 lr:0.000656 grad_accum:10.000000 total_samples:5808832.000000'}\n",
      "[Train Epoch]1/2 [Time]14267.04 [Step]18200 [Batch]182000 [Speed]78.39ms/step [Loss]7.5125 [Metrics]{'train_loss:7.512520 train_acc:0.086857 lr:0.000655 grad_accum:10.000000 total_samples:5824832.000000'}\n",
      "[Train Epoch]1/2 [Time]14304.26 [Step]18250 [Batch]182500 [Speed]78.38ms/step [Loss]7.5124 [Metrics]{'train_loss:7.512440 train_acc:0.086850 lr:0.000654 grad_accum:10.000000 total_samples:5840832.000000'}\n",
      "[Train Epoch]1/2 [Time]14341.44 [Step]18300 [Batch]183000 [Speed]78.37ms/step [Loss]7.5124 [Metrics]{'train_loss:7.512434 train_acc:0.086846 lr:0.000653 grad_accum:10.000000 total_samples:5856832.000000'}\n",
      "[Train Epoch]1/2 [Time]14378.62 [Step]18350 [Batch]183500 [Speed]78.36ms/step [Loss]7.5124 [Metrics]{'train_loss:7.512394 train_acc:0.086842 lr:0.000652 grad_accum:10.000000 total_samples:5872832.000000'}\n",
      "[Train Epoch]1/2 [Time]14415.79 [Step]18400 [Batch]184000 [Speed]78.35ms/step [Loss]7.5122 [Metrics]{'train_loss:7.512223 train_acc:0.086846 lr:0.000652 grad_accum:10.000000 total_samples:5888832.000000'}\n",
      "[Train Epoch]1/2 [Time]14452.96 [Step]18450 [Batch]184500 [Speed]78.34ms/step [Loss]7.5123 [Metrics]{'train_loss:7.512288 train_acc:0.086846 lr:0.000651 grad_accum:10.000000 total_samples:5904832.000000'}\n",
      "[Train Epoch]1/2 [Time]14490.13 [Step]18500 [Batch]185000 [Speed]78.33ms/step [Loss]7.5121 [Metrics]{'train_loss:7.512146 train_acc:0.086846 lr:0.000650 grad_accum:10.000000 total_samples:5920832.000000'}\n",
      "[Train Epoch]1/2 [Time]14527.29 [Step]18550 [Batch]185500 [Speed]78.31ms/step [Loss]7.5120 [Metrics]{'train_loss:7.512031 train_acc:0.086850 lr:0.000649 grad_accum:10.000000 total_samples:5936832.000000'}\n",
      "[Train Epoch]1/2 [Time]14564.40 [Step]18600 [Batch]186000 [Speed]78.30ms/step [Loss]7.5121 [Metrics]{'train_loss:7.512071 train_acc:0.086852 lr:0.000648 grad_accum:10.000000 total_samples:5952832.000000'}\n",
      "[Train Epoch]1/2 [Time]14601.61 [Step]18650 [Batch]186500 [Speed]78.29ms/step [Loss]7.5121 [Metrics]{'train_loss:7.512058 train_acc:0.086845 lr:0.000647 grad_accum:10.000000 total_samples:5968832.000000'}\n",
      "[Train Epoch]1/2 [Time]14638.78 [Step]18700 [Batch]187000 [Speed]78.28ms/step [Loss]7.5119 [Metrics]{'train_loss:7.511884 train_acc:0.086842 lr:0.000646 grad_accum:10.000000 total_samples:5984832.000000'}\n",
      "[Train Epoch]1/2 [Time]14675.93 [Step]18750 [Batch]187500 [Speed]78.27ms/step [Loss]7.5118 [Metrics]{'train_loss:7.511808 train_acc:0.086840 lr:0.000645 grad_accum:10.000000 total_samples:6000832.000000'}\n",
      "[Train Epoch]1/2 [Time]14713.11 [Step]18800 [Batch]188000 [Speed]78.26ms/step [Loss]7.5115 [Metrics]{'train_loss:7.511542 train_acc:0.086837 lr:0.000645 grad_accum:10.000000 total_samples:6016832.000000'}\n",
      "[Train Epoch]1/2 [Time]14750.32 [Step]18850 [Batch]188500 [Speed]78.25ms/step [Loss]7.5114 [Metrics]{'train_loss:7.511371 train_acc:0.086826 lr:0.000644 grad_accum:10.000000 total_samples:6032832.000000'}\n",
      "[Train Epoch]1/2 [Time]14787.49 [Step]18900 [Batch]189000 [Speed]78.24ms/step [Loss]7.5112 [Metrics]{'train_loss:7.511206 train_acc:0.086825 lr:0.000643 grad_accum:10.000000 total_samples:6048832.000000'}\n",
      "[Train Epoch]1/2 [Time]14824.66 [Step]18950 [Batch]189500 [Speed]78.23ms/step [Loss]7.5111 [Metrics]{'train_loss:7.511106 train_acc:0.086825 lr:0.000642 grad_accum:10.000000 total_samples:6064832.000000'}\n",
      "[Train Epoch]1/2 [Time]14861.84 [Step]19000 [Batch]190000 [Speed]78.22ms/step [Loss]7.5111 [Metrics]{'train_loss:7.511099 train_acc:0.086826 lr:0.000641 grad_accum:10.000000 total_samples:6080832.000000'}\n",
      "[Train Epoch]1/2 [Time]14899.06 [Step]19050 [Batch]190500 [Speed]78.21ms/step [Loss]7.5111 [Metrics]{'train_loss:7.511053 train_acc:0.086820 lr:0.000640 grad_accum:10.000000 total_samples:6096832.000000'}\n",
      "[Train Epoch]1/2 [Time]14936.19 [Step]19100 [Batch]191000 [Speed]78.20ms/step [Loss]7.5110 [Metrics]{'train_loss:7.510972 train_acc:0.086820 lr:0.000640 grad_accum:10.000000 total_samples:6112832.000000'}\n",
      "[Train Epoch]1/2 [Time]14973.33 [Step]19150 [Batch]191500 [Speed]78.19ms/step [Loss]7.5112 [Metrics]{'train_loss:7.511181 train_acc:0.086814 lr:0.000639 grad_accum:10.000000 total_samples:6128832.000000'}\n",
      "[Train Epoch]1/2 [Time]15010.48 [Step]19200 [Batch]192000 [Speed]78.18ms/step [Loss]7.5111 [Metrics]{'train_loss:7.511078 train_acc:0.086814 lr:0.000638 grad_accum:10.000000 total_samples:6144832.000000'}\n",
      "[Train Epoch]1/2 [Time]15047.61 [Step]19250 [Batch]192500 [Speed]78.17ms/step [Loss]7.5108 [Metrics]{'train_loss:7.510839 train_acc:0.086812 lr:0.000637 grad_accum:10.000000 total_samples:6160832.000000'}\n",
      "[Train Epoch]1/2 [Time]15084.75 [Step]19300 [Batch]193000 [Speed]78.16ms/step [Loss]7.5106 [Metrics]{'train_loss:7.510602 train_acc:0.086817 lr:0.000636 grad_accum:10.000000 total_samples:6176832.000000'}\n",
      "[Train Epoch]1/2 [Time]15121.99 [Step]19350 [Batch]193500 [Speed]78.15ms/step [Loss]7.5107 [Metrics]{'train_loss:7.510735 train_acc:0.086814 lr:0.000635 grad_accum:10.000000 total_samples:6192832.000000'}\n",
      "[Train Epoch]1/2 [Time]15159.13 [Step]19400 [Batch]194000 [Speed]78.14ms/step [Loss]7.5110 [Metrics]{'train_loss:7.510982 train_acc:0.086808 lr:0.000635 grad_accum:10.000000 total_samples:6208832.000000'}\n",
      "[Train Epoch]1/2 [Time]15196.34 [Step]19450 [Batch]194500 [Speed]78.13ms/step [Loss]7.5109 [Metrics]{'train_loss:7.510873 train_acc:0.086805 lr:0.000634 grad_accum:10.000000 total_samples:6224832.000000'}\n",
      "[Train Epoch]1/2 [Time]15233.51 [Step]19500 [Batch]195000 [Speed]78.12ms/step [Loss]7.5108 [Metrics]{'train_loss:7.510783 train_acc:0.086801 lr:0.000633 grad_accum:10.000000 total_samples:6240832.000000'}\n",
      "[Train Epoch]1/2 [Time]15270.72 [Step]19550 [Batch]195500 [Speed]78.11ms/step [Loss]7.5107 [Metrics]{'train_loss:7.510658 train_acc:0.086801 lr:0.000632 grad_accum:10.000000 total_samples:6256832.000000'}\n",
      "[Train Epoch]1/2 [Time]15307.89 [Step]19600 [Batch]196000 [Speed]78.10ms/step [Loss]7.5107 [Metrics]{'train_loss:7.510668 train_acc:0.086789 lr:0.000631 grad_accum:10.000000 total_samples:6272832.000000'}\n",
      "[Train Epoch]1/2 [Time]15345.02 [Step]19650 [Batch]196500 [Speed]78.09ms/step [Loss]7.5106 [Metrics]{'train_loss:7.510630 train_acc:0.086785 lr:0.000631 grad_accum:10.000000 total_samples:6288832.000000'}\n",
      "[Train Epoch]1/2 [Time]15382.14 [Step]19700 [Batch]197000 [Speed]78.08ms/step [Loss]7.5105 [Metrics]{'train_loss:7.510481 train_acc:0.086782 lr:0.000630 grad_accum:10.000000 total_samples:6304832.000000'}\n",
      "[Train Epoch]1/2 [Time]15419.78 [Step]19750 [Batch]197500 [Speed]78.07ms/step [Loss]7.5104 [Metrics]{'train_loss:7.510355 train_acc:0.086783 lr:0.000629 grad_accum:10.000000 total_samples:6320832.000000'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 119\u001b[0m\n\u001b[1;32m    115\u001b[0m inputs, target \u001b[39m=\u001b[39m batch_data\n\u001b[1;32m    116\u001b[0m grad_accum \u001b[39m=\u001b[39m grad_accum_scheduler(total_samples,\n\u001b[1;32m    117\u001b[0m                                   list_scheduler\u001b[39m=\u001b[39mlist_scheduler, \n\u001b[1;32m    118\u001b[0m                                   max_grad_accum\u001b[39m=\u001b[39mBERT4REC_CONFIG\u001b[39m.\u001b[39mtup_scheduler_grad_accum[\u001b[39m1\u001b[39m])                                                             \n\u001b[0;32m--> 119\u001b[0m step_gradients \u001b[39m=\u001b[39m train_step(inputs, target\u001b[39m=\u001b[39;49mtarget, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, num_accum_steps\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mconstant(grad_accum, tf\u001b[39m.\u001b[39;49mfloat32), loss\u001b[39m=\u001b[39;49mtrain_loss, acc\u001b[39m=\u001b[39;49mtrain_acc, seq_type\u001b[39m=\u001b[39;49minputs[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    120\u001b[0m global_gradients \u001b[39m=\u001b[39m backward_optimization(grad_accum, global_gradients, step_gradients, total_step, model, optimizer)\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m batch_num \u001b[39m%\u001b[39m BERT4REC_CONFIG\u001b[39m.\u001b[39mbatch_num_printer_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '1_Model_v0.4.ipynb'\n",
    "\n",
    "class BERT4REC_CONFIG:\n",
    "    seed = 42 #12\n",
    "    num_items = NUM_ITEMS\n",
    "    path_tfrecords = '../tfrecords/tfrecords_v0.4/'\n",
    "    restore_last_chekpoint = (True, 'model_bert4rec_complete_0.8.4/checkpoints/', 'ckpt-10')\n",
    "    model_name = 'model_bert4rec_complete_0.8.4'\n",
    "    checkpoint_filepath = f'../2_Models/'\n",
    "    num_records_dataset = 10_000_000\n",
    "    batch_size = 32\n",
    "    tup_scheduler_grad_accum = (5, 10, 1_000_000) #(start_grad_accum, max_grad_accum, ramp_up_samples)\n",
    "    seq_len = 20\n",
    "    mask_prob = 0.3\n",
    "    reverse_prob = 0.5\n",
    "    emb_dim = 256\n",
    "    trf_dim = 256\n",
    "    num_heads = 4\n",
    "    num_layers = 1\n",
    "    ff_dim = trf_dim*4\n",
    "    drop_rate = 0.1\n",
    "    att_drop_rate = 0.1\n",
    "    epochs = 2\n",
    "    early_stopping = 5\n",
    "    batch_num_printer_train = 500\n",
    "    batch_num_printer_val = 250\n",
    "    clipnorm = 1.0\n",
    "    num_iters_save_checkpoint = 25_000\n",
    "    scheduler_scaler = 128 \n",
    "    warmup_steps = 10_000\n",
    "    weight_decay = 1e-1\n",
    "    log_wandb = True\n",
    "\n",
    "set_seed(BERT4REC_CONFIG.seed)\n",
    "\n",
    "list_scheduler = np.linspace(BERT4REC_CONFIG.tup_scheduler_grad_accum[0], \n",
    "                             BERT4REC_CONFIG.tup_scheduler_grad_accum[1], \n",
    "                             BERT4REC_CONFIG.tup_scheduler_grad_accum[2]).astype(np.uint8).tolist()\n",
    "\n",
    "if BERT4REC_CONFIG.log_wandb:\n",
    "    time_suffix = datetime.now().__str__().split('.')[0]\n",
    "    dict_config = {k : v for k, v in zip(BERT4REC_CONFIG.__dict__.keys(), BERT4REC_CONFIG.__dict__.values()) if not k.startswith('__')}\n",
    "    init_wandb(wandb_project='otto-recsys', entity='enric1296', run_name=f'{BERT4REC_CONFIG.model_name}_{time_suffix}', dict_config=dict_config)\n",
    "    \n",
    "\n",
    "list_paths_train = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=train/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=train')]\n",
    "np.random.shuffle(list_paths_train)\n",
    "list_paths_val = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=val/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=val')]\n",
    "\n",
    "train_dataloader = Bert4RecDataLoader(list_paths_train, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len, \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=BERT4REC_CONFIG.mask_prob, \n",
    "                                     reverse_prob=BERT4REC_CONFIG.reverse_prob, \n",
    "                                     is_test=False,\n",
    "                                     is_val=False,\n",
    "                                     shuffle=True,\n",
    "                                     drop_remainder=True).get_generator()\n",
    "\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len,  \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     get_session=False,\n",
    "                                     is_val=True,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "optimizer = optimizers.Adam(learning_rate=CustomSchedule(BERT4REC_CONFIG.scheduler_scaler, warmup_steps=BERT4REC_CONFIG.warmup_steps),\n",
    "                            clipnorm=BERT4REC_CONFIG.clipnorm)\n",
    "                            # weight_decay=BERT4REC_CONFIG.weight_decay)                  \n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)                           \n",
    "                            \n",
    "# Build utils\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "if BERT4REC_CONFIG.restore_last_chekpoint[0]:\n",
    "    checkpoint_path = os.path.join(BERT4REC_CONFIG.checkpoint_filepath, BERT4REC_CONFIG.restore_last_chekpoint[1])\n",
    "    ckpt.restore(os.path.join(checkpoint_path, BERT4REC_CONFIG.restore_last_chekpoint[2]))\n",
    "    print('Latest checkpoint restored!!')\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n",
    "else:\n",
    "    checkpoint_path = create_folder_with_version(BERT4REC_CONFIG.model_name, BERT4REC_CONFIG.checkpoint_filepath)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, os.path.join(BERT4REC_CONFIG.checkpoint_filepath, checkpoint_path, 'checkpoints'), \n",
    "                                            max_to_keep=10)\n",
    "\n",
    "# Loss function\n",
    "loss_function = weighted_loss_bert4rec()#custom_loss_bert4rec()\n",
    "acc_function = custom_accuracy()\n",
    "\n",
    "# Trackers\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "train_acc = tf.keras.metrics.Mean(name='train_acc')\n",
    "val_acc = tf.keras.metrics.Mean(name='val_acc')\n",
    "\n",
    "##############################################\n",
    "\n",
    "global_gradients = []\n",
    "total_step, val_step, total_samples = 0, 0, 0\n",
    "for epoch in range(BERT4REC_CONFIG.epochs):\n",
    "    start = time.time()\n",
    "    print('===='*20)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    metrics_reset_states(train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    for batch_num, batch_data in enumerate(train_dataloader):\n",
    "        inputs, target = batch_data\n",
    "        grad_accum = grad_accum_scheduler(total_samples,\n",
    "                                          list_scheduler=list_scheduler, \n",
    "                                          max_grad_accum=BERT4REC_CONFIG.tup_scheduler_grad_accum[1])                                                             \n",
    "        step_gradients = train_step(inputs, target=target, model=model, optimizer=optimizer, num_accum_steps=tf.constant(grad_accum, tf.float32), loss=train_loss, acc=train_acc, seq_type=inputs[1])\n",
    "        global_gradients = backward_optimization(grad_accum, global_gradients, step_gradients, total_step, model, optimizer)\n",
    "        if batch_num % BERT4REC_CONFIG.batch_num_printer_train == 0:\n",
    "            train_dict_metrics = {x.name : x.result() for x in [train_loss, train_acc]}\n",
    "            train_dict_metrics.update({'lr' : optimizer.lr(total_step//grad_accum).numpy().astype(np.float32), 'grad_accum' : grad_accum, 'total_samples' : total_samples})\n",
    "            fancy_printer(train_loss, epoch, batch_num, start, step='Train', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=train_dict_metrics, num_step=total_step // grad_accum)\n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                train_dict_metrics.update({'step_grad' : total_step//grad_accum, 'step' : total_step})\n",
    "                log_wandb_metrics(step='train', num_step=total_step, gradients=global_gradients, dict_metrics=train_dict_metrics)     \n",
    "        total_step += 1  \n",
    "        total_samples += BERT4REC_CONFIG.batch_size * grad_accum if total_step % grad_accum==0 else 0\n",
    "        if total_step % BERT4REC_CONFIG.num_iters_save_checkpoint==0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print(f'Saving checkpoint for epoch {epoch+1} at step {total_step} on path {checkpoint_path}') \n",
    "     \n",
    "    for val_batch_num, val_batch_data in enumerate(val_dataloader):\n",
    "        inputs, target = val_batch_data\n",
    "        predictions = test_step(inputs, target=target, loss=val_loss, acc=val_acc, seq_type=inputs[1])\n",
    "        val_step += 1\n",
    "        if val_batch_num % BERT4REC_CONFIG.batch_num_printer_val == 0:\n",
    "            val_dict_metrics = {x.name : x.result() for x in [val_loss, val_acc]}\n",
    "            fancy_printer(val_loss, epoch, val_batch_num, start, step='Val', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=val_dict_metrics, num_step=val_step)    \n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                log_wandb_metrics(step='val', num_step=val_step, dict_metrics=val_dict_metrics) \n",
    "                # if val_batch_num==0:\n",
    "                #     log_wandb_metrics(step=None, plot_image=True, \n",
    "                #                       model=model, inputs=inputs, epoch=epoch, target=target, stats=stats)\n",
    "    \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {checkpoint_path}')        \n",
    "    \n",
    "    epoch_dict_metrics = {x.name : x.result() for x in [train_loss, val_loss, train_recall_k]}\n",
    "    printer = fancy_printer(None, epoch, epoch, start, step='epoch', num_step=epoch, dict_metrics=epoch_dict_metrics, \n",
    "                            train_loss=train_loss, val_loss=val_loss)\n",
    "    if BERT4REC_CONFIG.log_wandb:\n",
    "        log_wandb_metrics(step='epoch', num_step=total_step, dict_metrics=epoch_dict_metrics)\n",
    "\n",
    "if BERT4REC_CONFIG.log_wandb:\n",
    "    # wandb.save(checkpoint_path)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:58,  8.59it/s]\n",
      "100%|██████████| 48096/48096 [00:00<00:00, 221481.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.809600e+04</td>\n",
       "      <td>20382.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.386382e+06</td>\n",
       "      <td>0.282225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.733164e+06</td>\n",
       "      <td>0.438716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.110570e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.359522e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.590063e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.289973e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            session         score\n",
       "count  4.809600e+04  20382.000000\n",
       "mean   6.386382e+06      0.282225\n",
       "std    3.733164e+06      0.438716\n",
       "min    7.040000e+02      0.000000\n",
       "25%    3.110570e+06      0.000000\n",
       "50%    6.359522e+06      0.000000\n",
       "75%    9.590063e+06      1.000000\n",
       "max    1.289973e+07      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'carts': 0.3685273404345569,\n",
       " 'clicks': 0.23584843923826976,\n",
       " 'orders': 0.5460814923612543}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Metric: 0.4618\n"
     ]
    }
   ],
   "source": [
    "def get_score_metric(y_true, y_pred, type_target, k=20):\n",
    "    score = 0 \n",
    "    if len(y_true)==0:\n",
    "        return None\n",
    "    if type_target=='clicks':\n",
    "        num_targets = 1\n",
    "        hits = len([x for x in y_pred if x==y_true[0]])\n",
    "    else:\n",
    "        num_targets = min(k, len(y_true))\n",
    "        hits = len([x for x in y_pred if x in y_true])\n",
    "    score = hits / num_targets\n",
    "    return score\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "# model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "# ckpt = tf.train.Checkpoint(model=model)\n",
    "# ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.8.4/checkpoints'))\n",
    "list_paths_val = ['../tfrecords/tfrecords_v0.4/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=val')]\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20, \n",
    "                                     seq_len_target=20, \n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0, \n",
    "                                     is_val=True,\n",
    "                                     get_session=True, \n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "list_sessions, list_past_items, list_predictions, list_trues, list_types = [], [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(val_dataloader)):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    target, type_target, idx_mask = targets\n",
    "    idxs = idx_mask.numpy() #tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[x for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        labels = [list(set([_target for _type, _target in zip(type_target.numpy()[i], target.numpy()[i]) if dict_map_type[type_]==_type and _target!=0])) for i in range(target.shape[0])]\n",
    "        ###\n",
    "        list_sessions.append(session.numpy())\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_trues = list_trues + labels\n",
    "        list_past_items.append(seq_items.numpy()[:, :, 0])\n",
    "    if num_batch==500:\n",
    "        break\n",
    "\n",
    "df_val = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'past_items' : np.concatenate(list_past_items),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'trues' : list_trues,\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_val['score'] = df_val.progress_apply(lambda x: get_score_metric(x['trues'], x['predictions'], x['type']), axis=1)\n",
    "\n",
    "display(df_val.describe())\n",
    "dict_scores = df_val.groupby('type')['score'].mean().to_dict()\n",
    "display(dict_scores)\n",
    "kaggle_metric = 0.1*dict_scores['clicks'] + 0.3*dict_scores['carts'] + 0.6*dict_scores['orders']\n",
    "print(f'Kaggle Metric: {kaggle_metric:.4f}')\n",
    "\n",
    "\n",
    "# (seq_len=20)model_bert4rec_complete_0.8.4 - ckpt10\n",
    "# {'carts': 0.3504520904714635,\n",
    "#  'clicks': 0.22126267582597317,\n",
    "#  'orders': 0.5380787421674682}\n",
    "# Kaggle Metric: 0.4501\n",
    "\n",
    "# (seq_len=20)model_bert4rec_complete_0.8.4 - ckpt18\n",
    "# 'carts': 0.3685273404345569,\n",
    "#  'clicks': 0.23584843923826976,\n",
    "#  'orders': 0.5460814923612543}\n",
    "# Kaggle Metric: 0.4618\n",
    "\n",
    "# import wandb\n",
    "# api = wandb.Api()\n",
    "\n",
    "# run = api.run(\"<path to run>\")\n",
    "# run.summary[\"kaggle_metric\"] = kaggle_metric\n",
    "# run.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96640/96640 [00:00<00:00, 218537.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.664000e+04</td>\n",
       "      <td>40969.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.382368e+06</td>\n",
       "      <td>0.059107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.729289e+06</td>\n",
       "      <td>0.228825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.040000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.129969e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.363249e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.595140e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.289973e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            session         score\n",
       "count  9.664000e+04  40969.000000\n",
       "mean   6.382368e+06      0.059107\n",
       "std    3.729289e+06      0.228825\n",
       "min    7.040000e+02      0.000000\n",
       "25%    3.129969e+06      0.000000\n",
       "50%    6.363249e+06      0.000000\n",
       "75%    9.595140e+06      0.000000\n",
       "max    1.289973e+07      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'carts': 0.10665166808103793,\n",
       " 'clicks': 0.03558545383640441,\n",
       " 'orders': 0.18522078966011882}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Metric: 0.1467\n"
     ]
    }
   ],
   "source": [
    "## 500 iters bs=64 | seq_len=20\n",
    "# {'carts': 0.10665166808103793,\n",
    "#  'clicks': 0.03558545383640441,\n",
    "#  'orders': 0.18522078966011882}\n",
    "# Kaggle Metric: 0.1467\n",
    "\n",
    "## 500 iters bs=64 | seq_len=30 | mask_prob=0.3\n",
    "# {'carts': 0.08837243991883166,\n",
    "#  'clicks': 0.026765264053399646,\n",
    "#  'orders': 0.15859163250036265}\n",
    "# Kaggle Metric: 0.1243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2azkkync) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aff4a8bf8940a29c3471a931b49af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.240576…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model_bert4rec_complete_0.8_finetune_fold_0</strong>: <a href=\"https://wandb.ai/enric1296/otto-recsys/runs/2azkkync\" target=\"_blank\">https://wandb.ai/enric1296/otto-recsys/runs/2azkkync</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221122_114558-2azkkync/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2azkkync). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e74e0d3acf4cf0b53a2d666f6c51de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668100016666663, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/enric/SSD1TB/KAGGLE/025_Kaggle-OTTO Recsys-2022/1_Scripts/wandb/run-20221122_114635-32p0b058</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/enric1296/otto-recsys/runs/32p0b058\" target=\"_blank\">model_bert4rec_complete_0.8_finetune_fold_0</a></strong> to <a href=\"https://wandb.ai/enric1296/otto-recsys\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Fold: 0\n",
      "========================================================================================================================\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 11:46:42.819523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n",
      "/home/enric/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 167903104 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-11-22 11:46:46.800646: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14149/Unknown - 4639s 328ms/step - loss: 8.2624 - seq_acc: 0.0945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m ckpt\u001b[39m.\u001b[39mrestore(tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mlatest_checkpoint(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../2_Models/model_bert4rec_complete_0.8/checkpoints\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     46\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m4e-5\u001b[39m, clipnorm\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m),\n\u001b[1;32m     47\u001b[0m               loss\u001b[39m=\u001b[39mloss_function,\n\u001b[1;32m     48\u001b[0m               metrics\u001b[39m=\u001b[39m[acc_function])\n\u001b[0;32m---> 50\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataloader,\n\u001b[1;32m     51\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m     52\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     53\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[WandbCallback()],\n\u001b[1;32m     54\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     55\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     57\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../2_Models/model_bert4rec_complete_0.7_finetuned_fold_\u001b[39m\u001b[39m{\u001b[39;00mnum_fold\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)                   \n\u001b[1;32m     58\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/engine/training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \n\u001b[1;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/utils/tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/utils/tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "list_paths = ['../tfrecords/tfrecords_v0.4/na_split=test_aug/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=test_aug')]# + \\\n",
    "            #  ['../tfrecords/tfrecords_v0.4/na_split=val_aug/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=val_aug')] \n",
    "np.random.shuffle(list_paths)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for num_fold, (train_idxs, val_idxs) in enumerate(kfold.split(list_paths)):\n",
    "    train_paths = np.asarray(list_paths)[train_idxs]\n",
    "    val_paths = np.asarray(list_paths)[val_idxs]\n",
    "    if BERT4REC_CONFIG.log_wandb:\n",
    "        time_suffix = datetime.now().__str__().split('.')[0]\n",
    "        dict_config = {k : v for k, v in zip(BERT4REC_CONFIG.__dict__.keys(), BERT4REC_CONFIG.__dict__.values()) if not k.startswith('__')}\n",
    "        init_wandb(wandb_project='otto-recsys', entity='enric1296', run_name=f'{BERT4REC_CONFIG.model_name}_finetune_fold_{num_fold}', dict_config=dict_config)\n",
    "    print('===='*30)\n",
    "    print(f'Fold: {num_fold}')\n",
    "    print('===='*30)\n",
    "\n",
    "    train_dataloader = Bert4RecDataLoader(train_paths, \n",
    "                                         num_items=NUM_ITEMS, \n",
    "                                        seq_len=20,  \n",
    "                                        batch_size=32, \n",
    "                                        mask_prob=0.4, \n",
    "                                        reverse_prob=0.25,  \n",
    "                                        is_val=False,\n",
    "                                        is_test=False,\n",
    "                                        get_session=False,\n",
    "                                        shuffle=True).get_generator()\n",
    "\n",
    "    val_dataloader = Bert4RecDataLoader(val_paths, \n",
    "                                        num_items=NUM_ITEMS, \n",
    "                                        seq_len=20,  \n",
    "                                        batch_size=32, \n",
    "                                        mask_prob=0.4, \n",
    "                                        reverse_prob=0.25,  \n",
    "                                        is_val=True,\n",
    "                                        is_test=False,\n",
    "                                        get_session=False,\n",
    "                                        shuffle=False).get_generator()\n",
    "\n",
    "    loss_function = custom_loss_bert4rec()\n",
    "    acc_function = custom_accuracy()\n",
    "    model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.8/checkpoints'))\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=4e-5, clipnorm=1.0),\n",
    "                  loss=loss_function,\n",
    "                  metrics=[acc_function])\n",
    "\n",
    "    history = model.fit(train_dataloader,\n",
    "                        validation_data=val_dataloader,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[WandbCallback()],\n",
    "                        epochs=1,\n",
    "                        verbose=1)\n",
    "\n",
    "    model.save(f'../2_Models/model_bert4rec_complete_0.7_finetuned_fold_{num_fold}/')                   \n",
    "    wandb.finish()\n",
    "\n",
    "# 173/Unknown - 22s 113ms/step - loss: 7.9368 - recall_20: 0.3452\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 19:15:23.433225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "0it [00:00, ?it/s]2022-11-20 19:15:24.337587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "26122it [55:08,  7.90it/s]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.7/checkpoints'))\n",
    "\n",
    "\n",
    "list_paths_test = ['../tfrecords/tfrecords_v0.4/na_split=test/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=test')]\n",
    "test_dataloader = Bert4RecDataLoader(list_paths_test, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20,  \n",
    "                                     batch_size=64, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     is_val=False,\n",
    "                                     is_test=True,\n",
    "                                     get_session=True,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "list_predictions, list_sessions, list_types, list_scores = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(test_dataloader)):\n",
    "    features, idxs, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    idxs = idxs.numpy()\n",
    "    # idxs = tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    ###\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[dict_map[x] for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        topk_idxs = topk_idxs - 1\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_sessions.append(session.numpy())\n",
    "    # if num_batch==100:\n",
    "    #     break\n",
    "    \n",
    "\n",
    "# 26122it [54:28,  7.99it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_submission = f\"submission_{datetime.now().__str__().split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_')}\"\n",
    "\n",
    "df_inference = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_inference['session_type'] = df_inference['session'].astype(str) + '_' + df_inference['type']\n",
    "df_inference['labels'] = df_inference['predictions'].apply(lambda x : ' '.join([str(y) for y in x]))\n",
    "df_inference[['session_type', 'labels']].to_csv(f'../3_Submissions/{name_submission}.csv', index=False)\n",
    "\n",
    "print(df_inference.shape)\n",
    "display(\n",
    "    df_inference\n",
    ")\n",
    "\n",
    "import gzip\n",
    "with open(f'../3_Submissions/{name_submission}.csv', 'rb') as f_in, gzip.open(f'../3_Submissions/{name_submission}.csv.gz', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0432fa0070c5c9f7d9e158f590013ccc765eb84f02e6f69521746370c3bf6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
