{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 19:14:53.954956: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-20 19:14:54.029907: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-20 19:14:54.325716: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-11-20 19:14:54.325743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-11-20 19:14:54.325745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 19:14:54.605565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-20 19:14:54.619540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-20 19:14:54.619625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Libraries #\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, models, metrics, optimizers, constraints\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import Sequence\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# tfrecords for kaggle\n",
    "\n",
    "# name_dataset = 'tfrecords_v0.4_kaggle'\n",
    "# path_out = f'../tfrecords/{name_dataset}/'\n",
    "\n",
    "# if not os.path.exists(path_out):\n",
    "#     os.mkdir(path_out)\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_train'):\n",
    "#     os.rename(path_out + 'na_split_train/' + file, \n",
    "#               path_out + 'na_split_train/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_val'):\n",
    "#     os.rename(path_out + 'na_split_val/' + file, \n",
    "#               path_out + 'na_split_val/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_test'):\n",
    "#     os.rename(path_out + 'na_split_test/' + file, \n",
    "#               path_out + 'na_split_test/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_val_aug'):\n",
    "#     os.rename(path_out + 'na_split_val_aug/' + file, \n",
    "#               path_out + 'na_split_val_aug/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_test_aug'):\n",
    "#     os.rename(path_out + 'na_split_test_aug/' + file, \n",
    "#               path_out + 'na_split_test_aug/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1855603/1855603 [00:00<00:00, 7956616.23it/s]\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Paths & Global Variables\n",
    "\n",
    "# Train: (datetime.datetime(2022, 7, 31, 22, 0, 0, 25000), datetime.datetime(2022, 8, 28, 21, 59, 59, 984000))\n",
    "# Test: (datetime.datetime(2022, 8, 28, 22, 0, 0, 278000), datetime.datetime(2022, 9, 4, 21, 59, 51, 563000))\n",
    "\n",
    "path_data_raw = '../0_Data/'\n",
    "\n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "df_mapping = pd.read_csv('../tfrecords/tfrecords_v0.4/df_mapping.csv')\n",
    "NUM_ITEMS = len(df_mapping['aid_map'].unique())\n",
    "print(NUM_ITEMS)\n",
    "\n",
    "dict_map = {}\n",
    "for x in tqdm(df_mapping.to_dict('records')):\n",
    "    dict_map[x['aid_map']] = x['aid']\n",
    "\n",
    "dict_map_type = {\n",
    "    'clicks' : 1,\n",
    "    'carts' : 2,\n",
    "    'orders' : 3\n",
    "  }\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert4RecDataLoader:\n",
    "    \"\"\"\n",
    "    Class that iterates over tfrecords in order to get the sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_paths, num_items, seq_len, batch_size, num_targets=-1, mask_prob=0.4, \n",
    "                 reverse_prob=0.2, get_session=False, get_only_first_on_val=False, seq_len_target=None,\n",
    "                 min_size_seq_to_mask=2, is_val=False, is_test=False, avoid_repeats=False, shuffle=False, drop_remainder=False):\n",
    "        self.list_paths = list_paths\n",
    "        self.num_items = num_items\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_targets = num_targets\n",
    "        self.mask_prob = mask_prob\n",
    "        self.reverse_prob = tf.constant(reverse_prob)\n",
    "        self.shuffle = shuffle\n",
    "        self.min_size_seq_to_mask = min_size_seq_to_mask\n",
    "        self.avoid_repeats = avoid_repeats\n",
    "        self.get_session = get_session\n",
    "        self.seq_len_target = seq_len if not seq_len_target else seq_len_target\n",
    "        self.get_only_first_on_val = get_only_first_on_val\n",
    "        self.is_val = is_val\n",
    "        self.is_test = is_test\n",
    "        self.drop_remainder = drop_remainder\n",
    "\n",
    "    def get_generator(self):\n",
    "        dataset = tf.data.TFRecordDataset(self.list_paths, num_parallel_reads=AUTO, compression_type='GZIP')\n",
    "        dataset = dataset.map(self.parse_tf_record, num_parallel_calls=AUTO)\n",
    "        if self.is_val:\n",
    "            dataset = dataset.map(self.make_transforms_val, num_parallel_calls=AUTO)\n",
    "        elif self.is_test:\n",
    "            dataset = dataset.map(self.make_transforms_test, num_parallel_calls=AUTO)\n",
    "        else:\n",
    "            dataset = dataset.map(self.make_transforms_train, num_parallel_calls=AUTO)\n",
    "        \n",
    "        dataset = dataset.map(self.set_shapes, num_parallel_calls=AUTO)\n",
    "        # dataset = dataset.map(self.normalize_features, num_parallel_calls=AUTO)\n",
    "        if self.shuffle:\n",
    "            dataset = dataset.shuffle(self.batch_size*50, reshuffle_each_iteration=True)\n",
    "\n",
    "        dataset = dataset.batch(self.batch_size, num_parallel_calls=AUTO, drop_remainder=self.drop_remainder).prefetch(AUTO)\n",
    "        return dataset\n",
    "\n",
    "    def parse_tf_record(self, data):\n",
    "        features_context = {\n",
    "             \"session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "             \"size_session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        if not self.is_val:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False),\n",
    "                \"seq_recency_aid\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        else:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_aid_target\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type_target\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False),\n",
    "                \"seq_recency_aid\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        data_context, data_sequence = tf.io.parse_single_sequence_example(data, context_features=features_context, sequence_features=features_seq)\n",
    "        return data_context, data_sequence\n",
    "\n",
    "    def pad_sequence(self, seq_to_pad, maxlen, return_pad_mask=False, dtype=tf.float32):\n",
    "        length, num_feats = tf.shape(seq_to_pad)[0], tf.shape(seq_to_pad)[-1]\n",
    "        ###\n",
    "        if length < maxlen:\n",
    "            pad = tf.zeros((maxlen - length, num_feats), dtype)\n",
    "            seq = tf.concat([seq_to_pad, pad], axis=0)\n",
    "            pad_mask = tf.concat([tf.ones(tf.shape(seq_to_pad), dtype=seq_to_pad.dtype), \n",
    "                                 pad], axis=0)\n",
    "        else:\n",
    "            seq = seq_to_pad[-maxlen:, :]\n",
    "            pad_mask = tf.ones((maxlen, tf.shape(seq_to_pad)[-1]), dtype=seq_to_pad.dtype)\n",
    "        if return_pad_mask:\n",
    "            return seq, pad_mask\n",
    "        return seq \n",
    "\n",
    "    def make_transforms_val(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        seq_items_target_raw, seq_type_target_raw =  dict_sequences['seq_aid_target'], dict_sequences['seq_type_target']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ###\n",
    "        # Build target\n",
    "        seq_items, seq_target = seq_items, seq_items_target_raw[:1] if not self.get_session else seq_items_target_raw[:self.seq_len_target]\n",
    "        seq_type, seq_type_target = seq_type, seq_type_target_raw[:1] if not self.get_session else seq_type_target_raw[:self.seq_len_target]\n",
    "        seq_items_target = tf.concat([seq_items, seq_target], axis=0)\n",
    "        seq_type_target = tf.concat([seq_type, seq_type_target], axis=0)\n",
    "        ###\n",
    "        #Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, seq_type_target[:1]], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        seq_recency = tf.concat([seq_recency, tf.zeros((1, tf.shape(seq_recency)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        idx_masked = tf.clip_by_value(tf.shape(seq_items)[0], 0, self.seq_len-1)\n",
    "        seq_items, _ = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_items_target = self.pad_sequence(seq_items_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "        seq_type_target = self.pad_sequence(seq_type_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)\n",
    "        \n",
    "        if self.get_session:\n",
    "            seq_items_target_all = self.pad_sequence(seq_items_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "            seq_type_target_all = self.pad_sequence(seq_type_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64) \n",
    "            return (seq_items, seq_type, seq_time_encoding, seq_recency), (seq_items_target_all[:, 0], seq_type_target_all[:, 0], idx_masked), session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), seq_items_target[:, 0]\n",
    "\n",
    "    def make_transforms_test(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ###\n",
    "        seq_items = seq_items[-self.seq_len:, :]\n",
    "        seq_type = seq_type[-self.seq_len:, :]\n",
    "        seq_time_encoding = seq_time_encoding[-self.seq_len:, :]\n",
    "        seq_recency = seq_recency[-self.seq_len:, :]\n",
    "        idx_masked = tf.clip_by_value(tf.shape(seq_items)[0], 0, self.seq_len-1)\n",
    "        # Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, tf.zeros((1, tf.shape(seq_type)[1]), tf.int64)], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        seq_recency = tf.concat([seq_recency, tf.zeros((1, tf.shape(seq_recency)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        seq_items, _ = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)   \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)   \n",
    "        if self.get_session:\n",
    "            return (seq_items, seq_type, seq_time_encoding, seq_recency), idx_masked, session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), idx_masked\n",
    "\n",
    "  \n",
    "    def make_transforms_train(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        qt_size_seq = dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ### \n",
    "        # With prob reverse\n",
    "        if tf.random.uniform(shape=(1,1)) <= self.reverse_prob:\n",
    "            seq_items = tf.reverse(seq_items, axis=[0])\n",
    "            seq_type = tf.reverse(seq_type, axis=[0])\n",
    "            seq_time_encoding = tf.reverse(seq_time_encoding, axis=[0])\n",
    "            seq_recency = tf.reverse(seq_recency, axis=[0])\n",
    "            \n",
    "        # If our seq is longer than seq_len we can use it for data augmentation purpose \n",
    "        # and select a random idx to begin with.\n",
    "        if tf.shape(seq_items)[0] > self.seq_len:\n",
    "            idx_list = tf.range(tf.shape(seq_items)[0]-self.seq_len) \n",
    "            rand_idx = tf.random.shuffle(idx_list)[0]\n",
    "            seq_items = seq_items[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_type = seq_type[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_time_encoding = seq_time_encoding[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_recency = seq_recency[rand_idx:(rand_idx+self.seq_len), :]\n",
    "        \n",
    "        qt_size_seq = tf.shape(seq_items)[0]\n",
    "\n",
    "        ## Get idxs to mask for inputs and targets\n",
    "        probs = tf.random.uniform(shape=(qt_size_seq,), minval=0, maxval=1)\n",
    "        idxs_inputs = tf.cast(tf.where(probs >= (1-self.mask_prob)), tf.int64) # -> we mask to zero the inputs as we dont want to leak \n",
    "        idxs_target = tf.cast(tf.where(probs < (1-self.mask_prob)), tf.int64) # -> we mask to zero the targets as the loss will only be applied on non zero\n",
    "\n",
    "        # If all items are masked we leave an item unmasked\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.cast(qt_size_seq, tf.int64):\n",
    "            idxs_target = idxs_inputs[-1:]\n",
    "            idxs_inputs = idxs_inputs[:-1]\n",
    "            \n",
    "        # If no item has been masked we leave at least one item masked(be careful of size=1 seqs)\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.constant(0, dtype=tf.int64):\n",
    "            all_idxs = tf.cast(tf.random.shuffle(tf.range(0, qt_size_seq)), dtype=tf.int64)\n",
    "            idxs_inputs = all_idxs[:1][:, tf.newaxis]\n",
    "            idxs_target = all_idxs[1:][:, tf.newaxis]\n",
    "\n",
    "        # Mask inputs and targets\n",
    "        seq_items_raw = seq_items\n",
    "        updates_items = tf.zeros((len(idxs_inputs), seq_items.shape[-1]), tf.int64)\n",
    "        # updates_type = tf.zeros((len(idxs_inputs), seq_type.shape[-1]), tf.int64)\n",
    "        updates_time_encoding = tf.zeros((len(idxs_inputs), seq_time_encoding.shape[-1]), tf.float32)\n",
    "        updates_recency = tf.zeros((len(idxs_inputs), seq_recency.shape[-1]), tf.float32)\n",
    "        updates_target = tf.zeros((len(idxs_target), seq_items_raw.shape[-1]), tf.int64)\n",
    "        \n",
    "        seq_items = tf.tensor_scatter_nd_update(seq_items, idxs_inputs, updates_items)\n",
    "        # seq_type = tf.tensor_scatter_nd_update(seq_type, idxs_inputs, updates_type)\n",
    "        seq_time_encoding = tf.tensor_scatter_nd_update(seq_time_encoding, idxs_inputs, updates_time_encoding)\n",
    "        seq_recency = tf.tensor_scatter_nd_update(seq_recency, idxs_inputs, updates_recency)\n",
    "        seq_target = tf.tensor_scatter_nd_update(seq_items_raw, idxs_target, updates_target)\n",
    "        \n",
    "        # Padding\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32) \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_target = self.pad_sequence(seq_target, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)  \n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), seq_target[:, 0]\n",
    "  \n",
    "    def normalize_features(self, features):\n",
    "        return (features - tf.constant(5.45)/tf.constant(1.09))\n",
    "\n",
    "    # def normalize_features(self, features, targets=None, session=None):\n",
    "    #     seq_items, seq_type, seq_time_encoding, seq_recency = features\n",
    "    #     seq_recency = (seq_recency - tf.constant(5.45)/tf.constant(1.09))\n",
    "    #     features = (seq_items, seq_type, seq_time_encoding, seq_recency)\n",
    "    #     return features, targets, session\n",
    "\n",
    "    def set_shapes(self, features, targets=None, session=None):\n",
    "        features[0].set_shape((self.seq_len, 1))\n",
    "        features[1].set_shape((self.seq_len, 1))\n",
    "        features[2].set_shape((self.seq_len, 8))\n",
    "        features[3].set_shape((self.seq_len, 1))\n",
    "        if self.get_session:\n",
    "            return features, targets, session\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 19:14:56.394281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-20 19:14:56.394871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-20 19:14:56.394955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-20 19:14:56.394994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-20 19:14:56.662851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-20 19:14:56.662932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-20 19:14:56.662977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-20 19:14:56.663020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21471 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([32, 20, 1]), TensorShape([32, 20, 1]), TensorShape([32, 20, 8]), TensorShape([32, 20, 1])]\n",
      "[878879      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1]\n",
      "[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_paths = ['../tfrecords/tfrecords_v0.4/na_split=test/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=test')]\n",
    "# 5,45, 1,09\n",
    "dataloader = Bert4RecDataLoader(list_paths, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20, \n",
    "                                     seq_len_target=None,\n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.4, \n",
    "                                     reverse_prob=0.25, \n",
    "                                     get_session=True,\n",
    "                                     is_val=False,\n",
    "                                     is_test=True,\n",
    "                                     shuffle=False).get_generator()\n",
    "# # Train\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, target = batch\n",
    "#     seq_items, seq_type, seq_time, seq_recency = features\n",
    "#     break\n",
    "\n",
    "# # Test\n",
    "for batch in tqdm(dataloader):\n",
    "    features, target, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    idx_mask = target\n",
    "    break\n",
    "\n",
    "# Val\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, targets, session = batch\n",
    "#     seq_items, seq_type, seq_time, seq_recency = features\n",
    "#     target, type_target, idx_mask = targets\n",
    "#     break\n",
    "\n",
    "print([x.shape for x in features])\n",
    "\n",
    "idx = 2\n",
    "print(seq_items[idx].numpy().flatten())\n",
    "print(seq_type[idx].numpy().flatten())\n",
    "print(target[idx].numpy().flatten())\n",
    "print(idx_mask[idx].numpy().flatten())\n",
    "# print(type_target[idx].numpy().flatten())\n",
    "\n",
    "del features, target, seq_items, seq_type, seq_time, seq_recency\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransposed(tf.keras.layers.Layer):\n",
    "    def __init__(self, tied_to=None, activation=None, **kwargs):\n",
    "        super(EmbeddingTransposed, self).__init__(**kwargs)\n",
    "        self.tied_to = tied_to\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.custom_weights = self.tied_to.weights[0]\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.tied_to.weights[0].shape[0]\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        output = tf.keras.backend.dot(inputs, tf.keras.backend.transpose(self.custom_weights))\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'activation': tf.keras.activations.serialize(self.activation)}\n",
    "        base_config = super(EmbeddingTransposed, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class EncoderTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, attention_axes=None, drop_rate=0.1, att_drop_rate=0.1):\n",
    "        super(EncoderTransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes, dropout=att_drop_rate)\n",
    "        self.ffn = tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation='gelu'), \n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, query, key, training, attention_mask=None):\n",
    "        attn_output = self.att(query, key, attention_mask=attention_mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "      \n",
    "                 \n",
    "class ModelBert4Rec(tf.keras.models.Model):\n",
    "    def __init__(self, num_items, model_cfg):\n",
    "        super(ModelBert4Rec, self).__init__()\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        self.num_items = num_items\n",
    "        self.model_cfg = model_cfg\n",
    "        self.embed_items = tf.keras.layers.Embedding(\n",
    "            num_items, model_cfg.emb_dim, \n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.02)\n",
    "        )\n",
    "        self.embed_type = tf.keras.layers.Embedding(\n",
    "            3+1, \n",
    "            model_cfg.emb_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.02)\n",
    "        )\n",
    "        self.mlp_proj_time_encoding = tf.keras.models.Sequential([\n",
    "           tf.keras.layers.Dropout(model_cfg.drop_rate), \n",
    "           tf.keras.layers.Dense(model_cfg.trf_dim, kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.02)),\n",
    "           tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        ])\n",
    "        self.mlp_proj_conts = tf.keras.models.Sequential([\n",
    "           tf.keras.layers.Dropout(model_cfg.drop_rate), \n",
    "           tf.keras.layers.Dense(model_cfg.trf_dim, kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=0.02)),\n",
    "           tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        ])\n",
    "        self.list_transformer_block = [EncoderTransformerBlock(model_cfg.trf_dim, model_cfg.num_heads, \n",
    "                                                               model_cfg.ff_dim, attention_axes=None, \n",
    "                                                               drop_rate=model_cfg.drop_rate, \n",
    "                                                               att_drop_rate=model_cfg.att_drop_rate) \n",
    "                                       for _ in range(model_cfg.num_layers)]\n",
    "        # policy = mixed_precision.Policy('float32')\n",
    "        self.pred_layer = EmbeddingTransposed(tied_to=self.embed_items, activation='linear', dtype='float32')\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        x_seq_past, x_seq_type, x_seq_encoding, x_seq_recency = inputs\n",
    "        pad_mask = tf.cast(tf.where(tf.equal(x_seq_type, 0), 0, 1), tf.float32)\n",
    "        ###########\n",
    "        x_seq_past_items = self.embed_items(x_seq_past[:, :, 0])\n",
    "        x_seq_past_type = self.embed_type(x_seq_type[:, :, 0])\n",
    "        x_seq_time_encoding = self.mlp_proj_time_encoding(x_seq_encoding, training=training)\n",
    "        x_seq_recency = self.mlp_proj_conts(x_seq_recency, training=training)\n",
    "        x_ones = tf.ones(tf.shape(x_seq_past_items))\n",
    "        ########### \n",
    "        x = x_seq_past_items * (x_ones + x_seq_past_type + x_seq_time_encoding + x_seq_recency)\n",
    "        for i in range(len(self.list_transformer_block)):\n",
    "            x = self.list_transformer_block[i](x, x, training=training, attention_mask=pad_mask)\n",
    "        probs = self.pred_layer(x)\n",
    "        return probs\n",
    "      \n",
    "\n",
    "def build_model_bert4Rec(num_items, model_cfg):\n",
    "    return ModelBert4Rec(num_items, model_cfg)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, weight_decay=None):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_decay_tensor = tf.cast(1. if not weight_decay else weight_decay, tf.float32)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          'd_model': self.d_model,\n",
    "          'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        if self.weight_decay:\n",
    "            return self.weight_decay_tensor * tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "        else:\n",
    "            return tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "    \n",
    "    \n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "def custom_loss_bert4rec(tensor_weights=None):\n",
    "    def loss(y_true, y_pred):\n",
    "        mask = tf.where(y_true >= 1, 1., 0.)\n",
    "        ones = tf.ones(tf.shape(y_true))\n",
    "        y_pred = y_pred\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_true, y_pred)\n",
    "        if tensor_weights is not None:\n",
    "            weights = tf.gather(params=tensor_weights, indices=y_true)\n",
    "            return tf.reduce_sum(loss * weights * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "        else:\n",
    "            return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "    loss.__name__ = f'loss_bert4rec'\n",
    "    return loss\n",
    "\n",
    "\n",
    "def mrr_topk_categorical(top_k):\n",
    "  \"\"\"\n",
    "  Mrr Topk Categorical metric\n",
    "  \"\"\"\n",
    "  def mrr(y_true, y_pred):                                      \n",
    "    n_samples = tf.shape(y_true)[0]\n",
    "    n_samples_mask = tf.where(tf.reduce_sum(y_true, -1) >= 1, 1., 0.)\n",
    "    _, top_index = tf.nn.top_k(y_pred, top_k)  \n",
    "    result = tf.constant(0.0)\n",
    "    top_index = tf.cast(top_index, tf.float32)\n",
    "    idxs_not_masked = tf.cast(tf.argmax(y_true, axis=-1), tf.int32)\n",
    "    for i in tf.range(n_samples):\n",
    "        ranked_indicies = tf.where(tf.equal(top_index[i, idxs_not_masked[i], :], y_true[i, :][:, tf.newaxis]))\n",
    "        if tf.shape(ranked_indicies)[0] > 0:\n",
    "            ranked_indicies = tf.cast(ranked_indicies[0], tf.int32)\n",
    "            #check that the prediction its not padding\n",
    "            if top_index[i, ranked_indicies[0], ranked_indicies[1]] != 0.0: \n",
    "                rr = tf.cast(1/(ranked_indicies[1]+1), tf.float32)\n",
    "            else:\n",
    "                rr = tf.constant(0.0)\n",
    "        else:\n",
    "            rr = tf.constant(0.0)\n",
    "        result+=rr\n",
    "    return result/(tf.reduce_sum(n_samples_mask) + 1e-8)\n",
    "  mrr.__name__ = f'mrr_{top_k}_categorical'\n",
    "  return mrr\n",
    "\n",
    "def recall_top_k(top_k=1, seq_len=10):\n",
    "    def recall(y_true, y_pred):\n",
    "        n_samples = tf.shape(y_pred)[0]\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), tf.int32)\n",
    "        _, top_index = tf.nn.top_k(y_pred, top_k) \n",
    "        top_index = tf.cast(top_index, tf.int64)\n",
    "        # cum_sum = tf.zeros(n_samples, tf.int32)\n",
    "        result = tf.constant(0, tf.int32)\n",
    "        for i in tf.range(seq_len):\n",
    "            indexes_i = top_index[:, i, :]\n",
    "            is_true = tf.reduce_sum(tf.reduce_max(tf.where(y_true[:, i:i+1]==indexes_i, 1, 0), -1) * mask[:, i])\n",
    "            result += is_true\n",
    "        return tf.cast(result, tf.float32) / (tf.cast(tf.reduce_sum(mask), tf.float32) + 1e-8)\n",
    "    recall.__name__ = f'recall_{top_k}'\n",
    "    return recall\n",
    "\n",
    "def create_folder_with_version(base_name, checkpoint_path):\n",
    "    if os.path.exists(os.path.join(checkpoint_path, base_name)):\n",
    "        version_ = base_name.split('_v')\n",
    "        if not version_ or len(version_)==1:\n",
    "            base_name_no_version = base_name\n",
    "            version_ = '_v1'\n",
    "        else:\n",
    "            base_name_no_version = '_'.join(base_name.split('_v')[:-1])\n",
    "            version_ = f'_v{int(version_[-1])+1}'\n",
    "        base_name = base_name_no_version + version_\n",
    "        return create_folder_with_version(base_name, checkpoint_path)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_path, base_name)\n",
    "        os.mkdir(checkpoint_path)\n",
    "        return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAAGwCAYAAABiu4tnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABegklEQVR4nO3deVxU9f4/8NfswzogCIggizvugiLk2oJbpWVJ3aK6fetGy3Xrdk2ra7d7S+3eluuv1GuR5q2r3kLNSks0Nc3JFXHDHQUVxGEbFmFg+Pz+QCZHFhlkOMzwej4e89A585lz3p9BnZef8zmfIxNCCBARERGRzeRSF0BERETkqBikiIiIiJqJQYqIiIiomRikiIiIiJqJQYqIiIiomRikiIiIiJqJQYqIiIiomZRSF+DMqqurcfnyZXh4eEAmk0ldDhERETWBEALFxcUIDAyEXN74mBODlB1dvnwZwcHBUpdBREREzZCVlYWgoKBG2zBI2ZGHhweAmh+Ep6enxNUQERFRUxiNRgQHB1u+xxvDIGVHtafzPD09GaSIiIgcTFOm5XCyOREREVEzMUgRERERNRODFBEREVEzMUgRERERNRODFBEREVEzMUgRERERNRODFBEREVEzMUgRERERNRODFBEREVEzMUgRERERNRODFBEREVEzMUgRERERNRODFNmsosoMc7WQugwiIiLJMUiRTcorzRjzj+2Y/PEvEIJhioiI2jcGKbLJuauluFxUjiOXinC5qFzqcoiIiCTFIEU2KSwzWX5/9FKRhJUQERFJj0GKbHK1pMLy+2MMUkRE1M4xSJFNrhb/FqSOMEgREVE7xyBFNrlxROroZaOElRAREUmPQYpscuOI1NXiCuQaOeGciIjaLwYpssmNQQrg6T0iImrfGKTIJrVBytddDQA4eomn94iIqP1ikCKbGK7PkRrT0w8AR6SIiKh9Y5CiJqsyVyOvtGYdqTG9aoLUscsMUkRE1H5JHqQWL16MsLAwaLVaREZGYufOnY2237FjByIjI6HVahEeHo6lS5fWaZOcnIyIiAhoNBpERERg3bp1zTpueno67r//fuh0Onh4eGDYsGHIzMxsfmcdXH6pCUIAchkwvLsvZDIgu6jcMkpFRETU3kgapNasWYMZM2bgtddeQ2pqKkaMGIHx48c3GFYyMjIwYcIEjBgxAqmpqZg7dy6mTZuG5ORkSxu9Xo/4+HgkJCQgLS0NCQkJmDp1Kvbs2WPTcc+ePYvhw4ejV69e2L59O9LS0vDGG29Aq9Xa7wNp43Kvz4/ycdfAU6tCmK8bAODIRY5KERFR+yQTEt55Njo6GoMHD8aSJUss23r37o3Jkydj/vz5ddrPnj0bGzZsQHp6umVbYmIi0tLSoNfrAQDx8fEwGo3YtGmTpc24cePg7e2NVatWNfm4jzzyCFQqFf7zn/80uT8VFRWoqPhtdMZoNCI4OBhFRUXw9PRs8n7aqu0nc/HU8n2I6OSJjdNHYNaaQ1ibegnT7+qOmff0kLo8IiKiFmE0GqHT6Zr0/S3ZiJTJZMKBAwcQFxdntT0uLg67d++u9z16vb5O+7Fjx2L//v2orKxstE3tPpty3Orqanz//ffo0aMHxo4dCz8/P0RHR2P9+vWN9mn+/PnQ6XSWR3BwcOMfgoOpvWKvo4cGADCoixcAIDWrUKKKiIiIpCVZkDIYDDCbzfD397fa7u/vj5ycnHrfk5OTU2/7qqoqGAyGRtvU7rMpx83NzUVJSQkWLFiAcePGYfPmzXjggQfw4IMPYseOHQ32ac6cOSgqKrI8srKymvBJOI7aVc1/C1LeAIBDmQWorpZsYJOIiEgySqkLkMlkVs+FEHW23ar9zdubss/G2lRXVwMAJk2ahJkzZwIABg4ciN27d2Pp0qUYNWpUvbVpNBpoNJoGa3d0v60hVdPHngEe0KrkMJZX4ZyhFN383KUsj4iIqNVJNiLl6+sLhUJRZ/QpNze3zmhRrYCAgHrbK5VK+Pj4NNqmdp9NOa6vry+USiUiIiKs2vTu3btdX7V386k9lUKO/p29AACpmQVSlUVERCQZyYKUWq1GZGQkUlJSrLanpKQgNja23vfExMTUab9582ZERUVBpVI12qZ2n005rlqtxpAhQ3Dy5EmrNqdOnUJISIiNPXUeNwcpgPOkiIiofZP01N6sWbOQkJCAqKgoxMTEYNmyZcjMzERiYiKAmjlHly5dwsqVKwHUXKH30UcfYdasWXj22Weh1+uRlJRkuRoPAKZPn46RI0di4cKFmDRpEr755hts2bIFu3btavJxAeCVV15BfHw8Ro4ciTFjxuCHH37At99+i+3bt7fOh9MGWeZIudcTpDILJaiIiIhIYkJiH3/8sQgJCRFqtVoMHjxY7Nixw/Lak08+KUaNGmXVfvv27WLQoEFCrVaL0NBQsWTJkjr7/Oqrr0TPnj2FSqUSvXr1EsnJyTYdt1ZSUpLo1q2b0Gq1YsCAAWL9+vU29a2oqEgAEEVFRTa9r63qO+8HETL7O3H6SrFlW3bhNREy+zsR9up3oqS8UsLqiIiIWoYt39+SriPl7GxZh6KtK680o9cbPwAA0ubFQeeisrwWM38rsovKsfoPwzAs3EeqEomIiFqEQ6wjRY6ldn6UWimHp9b6jDBP7xERUXvFIEVNYrhhftTNS0cMvr6e1P7z+a1eFxERkZQYpKhJ6rtir9aQ0A4AgP0XuDAnERG1LwxS1CQ3r2p+oz6BnnBVK1B0rRKncotbuzQiIiLJMEhRk9y8qvmNlAo5IkNqTu/tzeDpPSIiaj8YpKhJGju1BwBDr5/e28MgRURE7QiDFDXJrYLUkLCaILUvIx9cUYOIiNoLBilqkvpWNb/RwGAvqBVy5BZX4EJeWWuWRkREJBkGKWqSW41IaVUKDAjWAeA8KSIiaj8YpOiWhBCWIOXXQJACgKHXT+/t5XpSRETUTjBI0S0VV1ShoqoaQP1X7dUaYplwntcqdREREUmNQYpuyXB9NMpDo4SLWtFgu8gQbyjkMmTlX8PFAs6TIiIi58cgRbd0q/lRtTy0KgwIqpkntfsMR6WIiMj5MUjRLdVesed7iyAFAMO7+QIAdp0x2LUmIiKitoBBim7JMiLVyPyoWndcD1K/nDHwvntEROT0GKTolpp6ag8ABnXxhotKgbxSE05e4X33iIjIuTFI0S3ZEqTUSjmiw2uu3vuFp/eIiMjJMUjRLd1qVfOb3dH1t9N7REREzoxBim7JlhEp4Ld5Unsy8mG6vv4UERGRM2KQoluyNUj1CvCAj5saZSYzDmUV2rEyIiIiaTFIUaOqqwXySk0Amh6k5HIZYq+PSu08fdVutREREUmNQYoaVVBmgrlaQCYDOripm/y+UT06AgC2ncy1V2lERESSY5CiRtVONO/gqoZK0fQ/LrVB6uglI3KN5XapjYiISGoMUtQoW+dH1eroobHcLmb7SZ7eIyIi58QgRY2qDVK+TVz64Eaje/oB4Ok9IiJyXgxS1KjmjkgBwJ29aoLUztMGLoNAREROiUGKGnU7QapfZx183dUoqajC/gv5LV0aERGR5BikqFG2rmp+I7lchlE9rp/eO8HTe0RE5HwYpKhRtzMiBfx2em8bJ5wTEZETYpCiRt1ukBre3RdKuQxncktw3lDakqURERFJjkGKGmU5tdfMIKVzUSE6vAMA4MdjOS1WFxERUVvAIEUNMlVVo7CsEkDz5kjVGtcnAACDFBEROR8GKWpQXmnNaJRKIYPORdXs/dwTUROkDmYW4gpXOSciIifCIEUNunExTrlc1uz9BOi0GNTFCwCw+fiVliiNiIioTWCQogbdzqrmN7Oc3jvK03tEROQ8GKSoQbd7xd6Nxl4PUr+ey0Nhmem290dERNQWMEhRgyxBqgVGpEJ93dArwANV1QJb07k4JxEROQcGKWrQ7S59cLO466NSm3h6j4iInASDFDWoJU/tAcCEfjVB6udTV1F0rbJF9klERCQlBilqUEsHqV4Bnujh7w6TuZqTzomIyCkwSFGDWvrUHgDcPyAQALAh7XKL7ZOIiEgqDFLUIEMLTjavdd/1ILX7rAG5xVyck4iIHJvkQWrx4sUICwuDVqtFZGQkdu7c2Wj7HTt2IDIyElqtFuHh4Vi6dGmdNsnJyYiIiIBGo0FERATWrVtn83GfeuopyGQyq8ewYcNur7MOpLSiCqUmM4CWHZEK8XHDgGAvVAtg4+HsFtsvERGRFCQNUmvWrMGMGTPw2muvITU1FSNGjMD48eORmZlZb/uMjAxMmDABI0aMQGpqKubOnYtp06YhOTnZ0kav1yM+Ph4JCQlIS0tDQkICpk6dij179th83HHjxiE7O9vy2Lhxo30+iDbIcP20nqtaATeNskX3zdN7RETkLGRCCCHVwaOjozF48GAsWbLEsq13796YPHky5s+fX6f97NmzsWHDBqSnp1u2JSYmIi0tDXq9HgAQHx8Po9GITZs2WdqMGzcO3t7eWLVqVZOP+9RTT6GwsBDr169vdv+MRiN0Oh2Kiorg6enZ7P1IYf/5fDy0VI8uHVzx85/HtOi+rxjLMWz+VggB7PzzGAR3cG3R/RMREd0OW76/JRuRMplMOHDgAOLi4qy2x8XFYffu3fW+R6/X12k/duxY7N+/H5WVlY22qd2nLcfdvn07/Pz80KNHDzz77LPIzW18IcmKigoYjUarh6Nq6Sv2buTvqcWwMB8AHJUiIiLHJlmQMhgMMJvN8Pf3t9ru7++PnJz6L43Pycmpt31VVRUMBkOjbWr32dTjjh8/Hl9++SV++uknvPfee9i3bx/uvPNOVFRUNNin+fPnQ6fTWR7BwcG3+BTaLssVey040fxGDwzqDABIPnAREg6KEhER3RbJJ5vLZDKr50KIOttu1f7m7U3Z563axMfHY+LEiejbty/uu+8+bNq0CadOncL333/fYG1z5sxBUVGR5ZGVldVg27bOniNSADChfye4qBQ4ZyjFwcwCuxyDiIjI3iQLUr6+vlAoFHVGn3Jzc+uMFtUKCAiot71SqYSPj0+jbWr32ZzjAkCnTp0QEhKC06dPN9hGo9HA09PT6uGo7B2k3DVKTOjXCQDw1f6LdjkGERGRvUkWpNRqNSIjI5GSkmK1PSUlBbGxsfW+JyYmpk77zZs3IyoqCiqVqtE2tftsznEBIC8vD1lZWejUqVPTOujg7B2kAODhqCAAwHeHs1FmqrLbcYiIiOxF0lN7s2bNwqefforPPvsM6enpmDlzJjIzM5GYmAig5lTZE088YWmfmJiICxcuYNasWUhPT8dnn32GpKQk/OlPf7K0mT59OjZv3oyFCxfixIkTWLhwIbZs2YIZM2Y0+bglJSX405/+BL1ej/Pnz2P79u2477774OvriwceeKB1PhyJ2XuOFABEh3VAlw6uKKmowqYjvGUMERE5npZdIMhG8fHxyMvLw1tvvYXs7Gz07dsXGzduREhICAAgOzvbam2nsLAwbNy4ETNnzsTHH3+MwMBALFq0CFOmTLG0iY2NxerVq/H666/jjTfeQNeuXbFmzRpER0c3+bgKhQJHjhzBypUrUVhYiE6dOmHMmDFYs2YNPDw8WunTkZahFUakZDIZHooMwvspp/DVgSxMiQyy27GIiIjsQdJ1pJydo64jJYRAj9c3odIssPvVOxHo5WK3Y10qvIbhC3+CEMDPr4xBFx+uKUVERNJyiHWkqO0qulaJSnNNvvZxV9v1WJ29XDC8my8AYM3++le0JyIiaqsYpKiO2onmOhcVNEqF3Y/3u6FdAABr9mXBVFVt9+MRERG1FAYpqqM1rti70d0R/vD31MBQYsIPxzjpnIiIHAeDFNXRGlfs3UilkOPR66NSX/x6oVWOSURE1BIYpKiO1h6RAoBHh3aBQi7D3ox8nMwpbrXjEhER3Q4GKapDiiDl76lFXETNyvIclSIiIkfBIEV1SBGkACBhWM06XutSL6GkgiudExFR28cgRXW09hypWjFdfdC1oxtKKqqQfID33yMioraPQYrqkGpESiaT4anYUADAZ79kwFzNtWKJiKhtY5CiOgwl0gQpAHgoMhheripcyCtDyvErrX58IiIiWzBIkZUqczXySk0AAN9WPrUHAC5qBR6Prpkr9enOc61+fCIiIlswSJGV/FIThADkMqCDm31vD9OQJ2JCoFbIsf9CAVIzCySpgYiIqCkYpMhK7vX5UT7uGijkMklq8PPU4v6BgQCAT3dmSFIDERFRUzBIkRWprti72TMjwgAAm45mIyu/TNJaiIiIGsIgRVakumLvZr0CPDGiuy+qBbDsZ86VIiKitolBiqy0lSAFAC+M7gYAWLM/C1eM5RJXQ0REVBeDFFlpS0FqWHgHRIV4w1RVjU84KkVERG0QgxRZaStzpICaBTr/eFd3AMCXezKRd702IiKitoJBiqwY2tCIFACM7O6L/kE6XKs0I2kXr+AjIqK2hUGKrFyVcFXz+shkMrw0pmau1Er9BRSWmSSuiIiI6DcMUmSldo6UFKuaN+Tu3v7oFeCBkooqfPbLeanLISIismCQIovySjOKy6sAtJ0RKQCQy2WYdn2uVNLOc8gv5agUERG1DQxSZFE7GqVWyuGpVUpcjbVxfQLQt7MnSk1mLN52RupyiIiIADBI0Q1uvGJPJpPm9jANkctl+FNcTwDAyl8v4HLhNYkrIiIiYpCiG7SlNaTqM6pHRwwN6wBTVTUWbT0tdTlEREQMUvSbth6kZDIZZo+rGZX66sBFnLtaInFFRETU3jFIkUVbD1IAEBnSAXf18oO5WuC9lFNSl0NERO0cgxRZtKVVzRvzp7E9IZMB3x/OxoEL+VKXQ0RE7RiDFFm0tVXNG9K7kyfio4IBAG99l47qaiFxRURE1F4xSJFFW1vVvDGz4nrATa1AWlYhvkm7JHU5RETUTjFIkUVbXNW8IX4eWrx4Z82tYxZuOokyU5XEFRERUXvEIEUAACGEJUj5OcCIFAA8fUcYgrxdkGMsx7Kfz0ldDhERtUMMUgQAKK6oQkVVNQDHGJECAK1KgTnjewMA/r3jHBfpJCKiVscgRQB+O63noVHCRa2QuJqmm9AvAENCvXGt0oy/fXdc6nKIiKidYZAiAI6xhlR9ZDIZ3prUFwq5DJuO5mDbiVypSyIionaEQYoA3DDR3MGCFFCzHML/DQ8DAPxlw1FcM5klroiIiNoLBikC4LgjUrWm39UdgTotsvKv4aNtvA8fERG1DgYpAuA4q5o3xE2jxF/u6wMAWPbzOZzJLZa4IiIiag8YpAiA46xq3pixffxxZy8/VJoF5q49yhXPiYjI7hikCIBjrWreEJlMhr/e3weuagX2ns/Hf369IHVJRETk5BikCMANc6Qc9NRereAOrpg9rhcAYOEPJ5CVXyZxRURE5MwYpAiA4082v1HCsBAMDeuAMpMZs5MPQwie4iMiIvtgkCKYqwXySk0AnCNIyeUyvDulP7QqOXafzcN/92ZKXRIRETkpyYPU4sWLERYWBq1Wi8jISOzcubPR9jt27EBkZCS0Wi3Cw8OxdOnSOm2Sk5MREREBjUaDiIgIrFu37raO+9xzz0Emk+HDDz+0uX+OoKDMBHO1gEwGdHBTS11Oiwj1dcMrY2tO8b3zfTpP8RERkV1IGqTWrFmDGTNm4LXXXkNqaipGjBiB8ePHIzOz/hGEjIwMTJgwASNGjEBqairmzp2LadOmITk52dJGr9cjPj4eCQkJSEtLQ0JCAqZOnYo9e/Y067jr16/Hnj17EBgY2PIfQBtRe1qvg6saKoXk2brFPBUbiqgQb5SazJj1v0Mw8yo+IiJqYTIh4QSS6OhoDB48GEuWLLFs6927NyZPnoz58+fXaT979mxs2LAB6enplm2JiYlIS0uDXq8HAMTHx8NoNGLTpk2WNuPGjYO3tzdWrVpl03EvXbqE6Oho/Pjjj5g4cSJmzJiBGTNmNLl/RqMROp0ORUVF8PT0bPL7WtvPp67iic/2oleAB36YMVLqclpUZl4ZJizaiZKKKvwprgdeurO71CUREVEbZ8v3t2TDDyaTCQcOHEBcXJzV9ri4OOzevbve9+j1+jrtx44di/3796OysrLRNrX7bOpxq6urkZCQgFdeeQV9+vRpUp8qKipgNBqtHo7AmSaa36yLjyvemlTz8/tgy2kcyiqUtiAiInIqzQ5SJpMJJ0+eRFVVVbPebzAYYDab4e/vb7Xd398fOTk59b4nJyen3vZVVVUwGAyNtqndZ1OPu3DhQiiVSkybNq3JfZo/fz50Op3lERwc3OT3SsnRVzW/lQcGdcZ9AwJhrhaYvjoVpRXN+zNLRER0M5uDVFlZGf7v//4Prq6u6NOnj2Ve0bRp07BgwQKbC5DJZFbPhRB1tt2q/c3bm7LPxtocOHAA//rXv7BixYpGa7nZnDlzUFRUZHlkZWU1+b1ScoZVzRsjk8nw98l9EajT4kJeGeZtOCZ1SURE5CRsDlJz5sxBWloatm/fDq1Wa9l+9913Y82aNU3ej6+vLxQKRZ3Rp9zc3DqjRbUCAgLqba9UKuHj49Nom9p9NuW4O3fuRG5uLrp06QKlUgmlUokLFy7g5ZdfRmhoaIN90mg08PT0tHo4AmdY1fxWdC4qvB8/EDIZ8PWBi/hqv2OEXCIiattsDlLr16/HRx99hOHDh1uN1kRERODs2bNN3o9arUZkZCRSUlKstqekpCA2Nrbe98TExNRpv3nzZkRFRUGlUjXapnafTTluQkICDh8+jEOHDlkegYGBeOWVV/Djjz82uY+OonaOlK+TntqrNSzcBzPv7gEAeOOboziR4xhz2IiIqO1S2vqGq1evws/Pr8720tJSm06DAcCsWbOQkJCAqKgoxMTEYNmyZcjMzERiYiKAmtGvS5cuYeXKlQBqrtD76KOPMGvWLDz77LPQ6/VISkqyXI0HANOnT8fIkSOxcOFCTJo0Cd988w22bNmCXbt2Nfm4Pj4+lhGuWiqVCgEBAejZs6dNfXQEzjzZ/GYvjemG/RcK8POpq3j+i4PY8NId8NCqpC6LiIgclM0jUkOGDMH3339veV4bnj755BPExMTYtK/4+Hh8+OGHeOuttzBw4ED8/PPP2LhxI0JCQgAA2dnZVms7hYWFYePGjdi+fTsGDhyIv/3tb1i0aBGmTJliaRMbG4vVq1dj+fLl6N+/P1asWIE1a9YgOjq6ycdtb9rDqb1acrkMH8YPRCedFhmGUryafIS3kCEiomazeR2p3bt3Y9y4cXjsscewYsUKPPfcczh27Bj0er1l1XGq4QjrSFVUmdHz9R8AAKlv3ANvJ1nZ/FYOZhZg6lI9qqoF5t0Xgd/fESZ1SURE1EbYdR2p2NhY/PLLLygrK0PXrl2xefNm+Pv7Q6/XM0Q5oLySmnvsqRQy6FzazymuwV288drE3gCAv3+fjt1nDBJXREREjsjmOVIA0K9fP3z++ectXQtJ4MaJ5nK5bXPcHN1TsaE4fLEI61Iv4YX/HsQ3L96BEB83qcsiIiIHYvOIlEKhQG5ubp3teXl5UCgULVIUtZ72NNH8ZjKZDPMf7IcBwV4oLKvEM5/vR3F5pdRlERGRA7E5SDU0paqiogJqdfuYX+NMnH1V81vRqhRYlhAJf08NTueWYMZq3tyYiIiarsmn9hYtWgSg5n/xn376Kdzd3S2vmc1m/Pzzz+jVq1fLV0h25eyrmjeFv6cWyxKiMPXfemw9kYt/bj6J2eP4Z5mIiG6tyUHqgw8+AFAzIrV06VKr03hqtRqhoaFYunRpy1dIdtWelj5ozIBgL7z7UH9MX30IS7afRUgHVzwytIvUZRERURvX5CCVkZEBABgzZgzWrl0Lb29vuxVFrae9rGreFJMGdsbZ3BIs+ukMXlt/FP6eWozpVXfxWSIiolo2z5Hatm0bQ5QTac+Tzesz854emDI4COZqgRe+PIi0rEKpSyIiojasWcsfXLx4ERs2bEBmZiZMJpPVa++//36LFEatg6f2rMlkMiyY0g+5xeXYedqAp1fsw9oXYrksAhER1cvmILV161bcf//9CAsLw8mTJ9G3b1+cP38eQggMHjzYHjWSHVlGpHhqz0KlkGPJ45GYulSP49lGPLV8H/73XAzDJhER1WHzqb05c+bg5ZdfxtGjR6HVapGcnIysrCyMGjUKDz/8sD1qJDsprahCmckMgCNSN3PXKLHi90PQ2csFGYZSPPHZXhSVcY0pIiKyZnOQSk9Px5NPPgkAUCqVuHbtGtzd3fHWW29h4cKFLV4g2U/taJSrWgE3TbPO8jo1P08tvngmGr7uGqRnG/Hk8r0oqaiSuiwiImpDbA5Sbm5uqKio+QIODAzE2bNnLa8ZDLxfmSPh/KhbC/N1w5fPRMPLVYVDWYV45vN9KK80S10WERG1ETYHqWHDhuGXX34BAEycOBEvv/wy3n77bTz99NMYNmxYixdI9sP5UU3TM8ADK58eCneNEr+ey8fzXxyAqapa6rKIiKgNsDlIvf/++4iOjgYAvPnmm7jnnnuwZs0ahISEICkpqcULJPsxcESqyfoHeeGzp4ZAq5Jj28mrePG/BxmmiIjI9qv2wsPDLb93dXXF4sWLW7Qgaj1cQ8o2Q8M6YFlCFJ5ZuR8px6/g+S8OYPHjg6FR8mbdRETtlc0jUg1Zu3Yt+vfv31K7o1bAVc1tN7JHRyQ9GQWNUo6tJ3Lxh5UHOGeKiKgdsylIffLJJ3j44Yfxu9/9Dnv27AEA/PTTTxg0aBAef/xxxMTE2KVIsg+OSDXPiO4dsfypIXBRKbDj1FU8u3I/wxQRUTvV5CD1z3/+Ey+++CIyMjLwzTff4M4778Q777yDqVOnYvLkycjMzMS///1ve9ZKLcxy1R5HpGwW280Xy38/BK5qBXaeNuD3y/dxaQQionaoyUEqKSkJS5cuxf79+/H999/j2rVr+Omnn3DmzBnMmzcPvr6+9qyT7IAjUrdnWLgPPn96KNzUCujP5eF3n/yKvOvhlIiI2ocmB6kLFy7g7rvvBgCMHj0aKpUKb7/9Nry8vOxVG9lRdbXgVXstYEhoB/z32WHo4KbG4YtFeHipHhcLyqQui4iIWkmTg1R5eTm0Wq3luVqtRseOHe1SFNlf0bVKVJoFAMDHXS1xNY5tQLAXvkqMQWcvF5wzlOKhJXqculIsdVlERNQKbFr+4NNPP4W7uzsAoKqqCitWrKhzSm/atGktVx3ZTe38KC9XFS/fbwFdO7rj6+dj8ETSXpzOLcHDS/X47KkhiAzxlro0IiKyI5kQQjSlYWhoKGQyWeM7k8lw7ty5FinMGRiNRuh0OhQVFcHT01PqcqzsPmPA7z7dg+5+7kiZNUrqcpxGYZkJv1+xD6mZhdAo5fggfiAm9OskdVlERGQDW76/mzwidf78+duti9oQ3mfPPrxc1fjymWj88b+p2HoiFy98eRB/HtcTz4/qesv/iBARkeNpsQU5ybHwij37cVUrseyJKPz+jlAAwLs/nMSfvz7MW8oQETkhBql2iqua25dCLsO8+/rgrUl9IJcBXx24iCc+24PCMpPUpRERUQtikGqnOCLVOp6ICUXSU0Pgplbg13P5eGDxbpzmFX1ERE6DQaqd4qrmrWdMTz98/XwsOnu5IMNQiskf/4JNR7KlLouIiFoAg1Q7xRGp1tW7kyc2vHQHYsJ9UGoy4/kvD+LdH07AXN2ki2aJiKiNsjlIGY3Geh/FxcUwmTj/w1EwSLU+H3cN/vN/Q/HM8DAAwOLtZ/H7Ffs4b4qIyIHZHKS8vLzg7e1d5+Hl5QUXFxeEhIRg3rx5qK7mFUptVaW5GvnXv7wZpFqXUiHH6/dG4F+PDIRWJcfPp67i3v+3C4eyCqUujYiImsGmlc0BYMWKFXjttdfw1FNPYejQoRBCYN++ffj888/x+uuv4+rVq/jnP/8JjUaDuXPn2qNmuk35pSYIUXNlmbcrbw8jhUkDO6ObnzsSvziArPxreGjJbrw6vhf+b3gY15siInIgNgepzz//HO+99x6mTp1q2Xb//fejX79++Pe//42tW7eiS5cuePvttxmk2qja03o+bmoo5PzSlkqfQB2+++MIzFl7GBuP5ODv36dDfzYP/3x4ALzdGHCJiByBzaf29Ho9Bg0aVGf7oEGDoNfrAQDDhw9HZmbm7VdHdsFVzdsOnYsKH/9uMP42uS/USjm2nsjFxEU7sf98vtSlERFRE9gcpIKCgpCUlFRne1JSEoKDgwEAeXl58PbmzVrbKk40b1tkMhkShoVg3QuxCPN1w+WicsQv+xUfpJxCpZlzDYmI2jKbT+3985//xMMPP4xNmzZhyJAhkMlk2LdvH06cOIGvv/4aALBv3z7Ex8e3eLHUMriqedvUJ1CHb/84HK+vO4L1hy7jX1tPY/vJXLwfPxBdO7pLXR4REdXD5hGp+++/HydPnsT48eORn58Pg8GA8ePH48SJE7j33nsBAM8//zzef//9Fi+WWgZHpNoud40SHz4yCIseHQRPrRJpF4swcdFO/Ed/HkJwzSkiorbG5hEpAAgNDcWCBQtauhZqJVzVvO27f0AghoR6409fpeGXM3l445tj2JKei3cf6g9/T63U5RER0XXNClKFhYXYu3cvcnNz66wX9cQTT7RIYWQ/HJFyDJ10LvjP09H4XH8eCzadwI5TV3H3+zvwxsQIPBwVxGUSiIjaAJuD1LfffovHHnsMpaWl8PDwsPrHXCaTMUg5AAODlMOQy2X4/R1hGN7NFy9/lYbDF4vw5+TD+CbtEuY/0B9dfFylLpGIqF2zeY7Uyy+/jKeffhrFxcUoLCxEQUGB5ZGfz0u2HQFHpBxPd38PrH0+Fq9N6A2tSo5fzuRh7Ic/49Od53i/PiIiCdkcpC5duoRp06bB1ZX/E3ZE5ZVmFFdUAWCQcjRKhRzPjgzHD9NHYlh4B1yrNOPv36djypLdOJlTLHV5RETtks1BauzYsdi/f3+LFbB48WKEhYVBq9UiMjISO3fubLT9jh07EBkZCa1Wi/DwcCxdurROm+TkZERERECj0SAiIgLr1q2z+bhvvvkmevXqBTc3N3h7e+Puu+/Gnj17bq+zbUDtaJRGKYeHpllT5Ehiob5u+O8zw/DOA/3goVHiUFYhJizaibe/P46S6yGZiIhah81BauLEiXjllVfw5ptvIjk5GRs2bLB62GLNmjWYMWMGXnvtNaSmpmLEiBEYP358g6uiZ2RkYMKECRgxYgRSU1Mxd+5cTJs2DcnJyZY2er0e8fHxSEhIQFpaGhISEjB16lSrENSU4/bo0QMfffQRjhw5gl27diE0NBRxcXG4evWqjZ9Y23LjquacrOy45HIZfhfdBZtnjURchD/M1QKf7MzAXe9tx3eHL3OpBCKiViITNv6LK5c3nL1kMhnMZnOT9xUdHY3BgwdjyZIllm29e/fG5MmTMX/+/DrtZ8+ejQ0bNiA9Pd2yLTExEWlpaZbb08THx8NoNGLTpk2WNuPGjYO3tzdWrVrVrOMCgNFohE6nw5YtW3DXXXc1qX+17ykqKoKnp2eT3mNvPx7LwXP/OYBBXbyw7oU7pC6HWsi2E7l489tjuJBXBgAY3s0Xf53Uhwt5EhE1gy3f3zaPSFVXVzf4sCVEmUwmHDhwAHFxcVbb4+LisHv37nrfo9fr67SvPdVYWVnZaJvafTbnuCaTCcuWLYNOp8OAAQMa7FNFRQWMRqPVo63hqubOaUwvP/w4YyRm3N0daqUcu84YMO7Dn7HwhxM83UdEZEc2B6mWYjAYYDab4e/vb7Xd398fOTk59b4nJyen3vZVVVUwGAyNtqndpy3H/e677+Du7g6tVosPPvgAKSkp8PX1bbBP8+fPh06nszxq7z3YlvCKPeelVSkw4+4eSJk5EmN6dkSlWWDJ9rMY/Y/tWL03k1f3ERHZQZNmGy9atAh/+MMfoNVqsWjRokbbTps2zaYCbp6nI4RodO5Ofe1v3t6UfTalzZgxY3Do0CEYDAZ88sknlrlWfn5+9dY2Z84czJo1y/LcaDS2uTDFVc2dX4iPGz57agi2pOfinY3pyDCU4tW1R7Bi93m8PjECw7s3/J8BIiKyTZOC1AcffIDHHnvMMjLTEJlM1uQg5evrC4VCUWcUKDc3t85oUa2AgIB62yuVSvj4+DTapnafthzXzc0N3bp1Q7du3TBs2DB0794dSUlJmDNnTr31aTQaaDRtO6BwRKp9kMlkuCfCH6N6dMQXv17Av7aexomcYjyetAd39fLDnAm90c2P86eIiG5Xk07tZWRkWIJKRkZGg49z5841+cBqtRqRkZFISUmx2p6SkoLY2Nh63xMTE1On/ebNmxEVFQWVStVom9p9Nue4tYQQqKiouHXn2jAGqfZFrZTj6eFh2PHKaPz+jlAo5TJsPZGLsR/+jNfWHcEVY7nUJRIROTTJ5kgBwKxZs/Dpp5/is88+Q3p6OmbOnInMzEwkJiYCqDlVduMtZxITE3HhwgXMmjUL6enp+Oyzz5CUlIQ//elPljbTp0/H5s2bsXDhQpw4cQILFy7Eli1bMGPGjCYft7S0FHPnzsWvv/6KCxcu4ODBg3jmmWdw8eJFPPzww63z4dgJg1T75OWqxrz7+mDzzJG4u3fNcglf7snEqH9sw/xN6SgsM0ldIhGRQ7J5RUaz2YwVK1Zg69at9d60+KeffmryvuLj45GXl4e33noL2dnZ6Nu3LzZu3IiQkBAAQHZ2ttXaTmFhYdi4cSNmzpyJjz/+GIGBgVi0aBGmTJliaRMbG4vVq1fj9ddfxxtvvIGuXbtizZo1iI6ObvJxFQoFTpw4gc8//xwGgwE+Pj4YMmQIdu7ciT59+tj6kbUZQgjOkWrnwju649Mno7A3Ix/v/nAC+y8U4N87zuG/v2biuVHh+P0dYXDjQq1ERE1m8zpSL730ElasWIGJEyeiU6dOdSZoNzaHqr1pa+tIGcsr0f/NzQCAE38bB61KIXFFJCUhBLadzMW7P5zEieu3mPF1V+OF0d3wu+gu/PNBRO2WLd/fNv/Xc/Xq1fjf//6HCRMmNLtAkkbtaT0PrZJfkgSZTIY7e/ljdA8/fHv4Mt5POYULeWV467vjWLLjLJ4bGY7HokPgouafFSKihtg8R0qtVqNbt272qIXsjPOjqD5yuQyTBnbGllmj8M4D/dDZywVXiyvw9+/TMeLdn7Ds57Mo5aKeRET1sjlIvfzyy/jXv/7Fe3k5IK5qTo1RKeT4XXQXbPvTaCx4sB+CO7jAUGLCOxtPYMS727B4+xmukk5EdBObT+3t2rUL27Ztw6ZNm9CnTx/LsgO11q5d22LFUcviiBQ1hVopxyNDu2BKZBDWp17CR9vO4EJeGd794SSWbj+Lx4eF4Kk7QuHnoZW6VCIiydkcpLy8vPDAAw/YoxayM16xR7ZQKeR4OCoYDwzqjA1pl/HRtjM4d7UUi7efxac7MzAlsjOeHRGOcN4YmYjaMZuCVFVVFUaPHo2xY8ciICDAXjWRnXBEippDqZDjwcFBmDywM1LSr2DpjrNIzSzEqr1ZWL0vC3ER/nhuVFcM7uItdalERK3OpiClVCrx/PPPIz093V71kB0xSNHtkMtlGNsnAHER/tfXnzqLLem5+PHYFfx47AqGhnbA08PDcE+EPxTyhu+XSUTkTGw+tRcdHY3U1FTL4pXkOBikqCXIZDIMCe2AIaEdcPpKMT7ZeQ7rUi9h7/l87D2fj85eLngyNgTxUV2gc1XdeodERA7M5iD1wgsv4OWXX8bFixcRGRkJNzc3q9f79+/fYsVRy+IcKWpp3f098O5DAzDrnp5YqT+PVXszcanwGt7ZeALvp5zCA4OC8FRsKHoGeEhdKhGRXdi8srlcXnfFBJlMBiEEZDIZzGZzixXn6NrSyubmaoEer2+CuVpg79y74OfJK66o5ZVXmrHh0GUs330e6dlGy/bYrj54IiYUd/X2g0oh6S0+iYhuya4rm2dkZDS7MJJOQZkJ5moBmQzo4KaWuhxyUlqVAlOHBOPhqCDszcjHit3n8eOxHOw+m4fdZ/Pg56FB/JBgxA8JRpC3q9TlEhHdNpuDFOdGOaba+VE+bmooOSJAdiaTyRAd7oPocB9cLCjDF79m4qv9WcgtrsD/++kMPtp2BqN7dMTvokMwpmdH/pkkIofV7Nu8Hz9+HJmZmTCZTFbb77///tsuiloeVzUnqQR5u+LV8b0w654e2Hw8B//dk4ndZ/Ow7eRVbDt5FQGeWssoVaCXi9TlEhHZxOYgde7cOTzwwAM4cuSIZW4UUPM/UACcI9VG8Yo9kppaKce9/QNxb/9AZBhKsWpvJr4+cBE5xnL8a+tpLPrpNIZ388VDkUGIiwjgzZKJyCHYPJ4+ffp0hIWF4cqVK3B1dcWxY8fw888/IyoqCtu3b7dDidQSeMUetSVhvm6YO6E39HPuxKJHB2FYeAcIAew8bcD01Ycw9O0tmLP2MA5cyOd9PYmoTbN5REqv1+Onn35Cx44dIZfLIZfLMXz4cMyfPx/Tpk1DamqqPeqk28QRKWqLNEoF7h8QiPsHBCIzrwzJBy8i+eBFXCy4hlV7s7BqbxbCfN3wUGQQHhjUmaf+iKjNsXlEymw2w9295t5avr6+uHz5MoCaSegnT55s2eqoxTBIUVvXxccVM+/pgZ9fGYNVzw7DlMFBcFUrkGEoxT9+PIk7Fv6ER5f9ilV7M1FYZrr1DomIWoHNI1J9+/bF4cOHER4ejujoaLz77rtQq9VYtmwZwsPD7VEjtQAGKXIUcrkMMV19ENPVB29N6oNNR3Pw9YEs/HouH/pzedCfy8NfvjmKUT38MGlgIO7u7c/5VEQkGZuD1Ouvv47S0lIAwN///nfce++9GDFiBHx8fLBmzZoWL5BaBudIkSNy0yjxUGQQHooMwsWCMnyblo1vDl3CiZxibEm/gi3pV+CqViAuwh+TBnbG8O6+XPCTiFqVzSub1yc/Px/e3t6WK/eoRlta2XzgW5tRWFaJlJkj0d2ft+sgx3bqSjE2HLqMb9IuISv/mmW7l6sKcRH+GN+3E+7o5gu1kqGKiGxny/d3s4PUmTNncPbsWYwcORIuLi6WW8TQb9pKkKqoMqPn6z8AAA795R54uXJlc3IOQgikZhViw6HL+O5wNgzXR14BwEOrxD29/TG+XyeM6O4LrYqn/4ioaex6i5i8vDxMnToV27Ztg0wmw+nTpxEeHo5nnnkGXl5eeO+995pdONlHXknNxFyVQgadi0riaohajkwmw+Au3hjcxRtv3BuBvRn52HQ0G5uO5uBqcQXWpl7C2tRLcFMrcGdvf4zvG4DRPTvCVd3stYiJiKzY/K/JzJkzoVKpkJmZid69e1u2x8fHY+bMmQxSbdCNq5pz1JCcleKGSepv3tcHBzILsOlIDjYdzUZ2UTm+TbuMb9MuQ6uSY2T3jrg7wh939vLjav9EdFtsDlKbN2/Gjz/+iKCgIKvt3bt3x4ULF1qsMGo5vGKP2hu5XIYhoR0wJLQDXp/YG2kXC7HpaE2oysq/hs3Hr2Dz8SuQyYDBXbxxd29/3BPhh64d3fmfDSKyic1BqrS0FK6ude/abjAYoNHwi7ot4hV71J7J5TIM6uKNQV28MWd8Lxy7bLRc8Xf0khEHLhTgwIUCLPzhBEJ9XHFXb3/c3dsfQ0K9eTNlIrolm4PUyJEjsXLlSvztb38DUDNHobq6Gv/4xz8wZsyYFi+Qbh9HpIhqyGQy9O2sQ9/OOsy4uwcuF17D1hO52HL8CvRn83A+rwxJuzKQtCsDOhcVRvXoiNE9O2Jkj448BUhE9bI5SP3jH//A6NGjsX//fphMJvz5z3/GsWPHkJ+fj19++cUeNdJtYpAiql+glwsShoUgYVgISiqqsOv0VaQcz8VPJ66goKwSG9IuY0PaZchkQL/OOkuwGhDkxdEqIgLQjCAVERGBw4cPY8mSJVAoFCgtLcWDDz6IF198EZ06dbJHjXSbGKSIbs1do8S4vp0wrm8nmKsFDmYWYNuJXOw4dRXHLhtx+GIRDl8swv/76Qx0LioM7+5bE6x6dISfp1bq8olIIi2yICcAZGVlYd68efjss89aYndOoa2sIzVlyW4cuFCAJY8Nxvh+DLtEtso1luPn0wZsP5mLnacNKLpWafV6RCdPDO/uizu6+WJIqDeXVyBycK2yIOfN0tLSMHjwYJjN5pbYnVNoK0Fq1D+24UJeGb5OjEFUaAfJ6iByBuZqgUNZhdhx6ip2nMzF4UtFuPFfUZWiZnL78G6+uKObD/oHefG2NUQOxq4LcpLj4ak9opajkMsQGeKNyBBvzLqnB/JKKrDrjAG/nDHglzN5uFR4DXsz8rE3Ix/vp9ScMowO64DYbr4Y3s0XPfy5xAKRM2GQcnKlFVUoM9WMEvKqI6KW5+OuwaSBnTFpYGcIIXAhrwy/nDVg95k8/HLWgMKySmw9kYutJ3IB1Pw9jO3qg+jwDogO80HXjm4MVkQOjEHKydWORrmqFXDT8MdNZE8ymQyhvm4I9XXDY9EhqK4WOJ5trBmtOpuHvRl5MJRUWK4GBABfdzWGhtWEqqFhHdDT3wNyOYMVkaNo8jfrgw8+2OjrhYWFt1sL2YFlMU6e1iNqdXL5b+tWPTeqKyqqzEjNLIT+bB72ZOQhNbMQhhITNh7JwcYjOQAAnYsKQ0I7YNj1EavenTy41AJRG9bkIKXT6W75+hNPPHHbBVHLssyP4mk9IslplAoMC/fBsHAfAEBFlRmHLxZhz7k87MnIx4ELBSi6VmlZeR2omWNVOycrMsQbA4K94M7RZaI2o8l/G5cvX27POshOONGcqO3SKBWWewK+BKDSXI2jl4qwNyMfezLyse98PorLq2quEDx1FQAglwG9O3lagtXgLt4I8nbhPCsiifC/NU6OQYrIcagUcst9AZ8b1RXmaoH07N/uB3jgQgEuFV7DsctGHLtsxEp9zY3i/Tw0vwWrEG/0DdRBreTpQKLWwCDl5Hhqj8hxKW6YY/VkbCgAIKeo/LdglVmAY5eKkFtcgU1Hc7DpaM08K7VSjv6ddRgQ7IUBwV4YGOSF4A4ctSKyBwYpJ8fJ5kTOJUCnxcT+nTCxf81dCsora+ZZ1Yarg5kFyC81Yf+FAuy/UGB5n7erCgOCvdA/yAsDg3XoH+TFJVGIWgCDlJMzMEgROTWtSoGhYR0wNKzmrgVCCJzPK8PBCwU4fLEQhy4WIf2yEQVlldh+8iq2n7xqeW+QtwsGBHlhQLAOA4K80LezjsukENmIf2OcHOdIEbUvMpkMYb5uCPN1w5TIIAA1VweeyC5G2sVCHMoqxOGLRTiTW4KLBddwseAavj+SDaBmInt3Pw/06eyJvoE1pxQjAj15lSBRI/i3w4lVVwuOSBERNEqFZb7UEzE124zllTh6sQiHLhYi7Xq4yi4qx8krxTh5pRhrD14CAMhkQJiPGyICPWvmawXq0CfQE95uagl7RNR2MEg5saJrlag019xN1ceNQYqIfuOpVSG2my9iu/latl0xluPwxSIcu1yEo5eMOHa5JlydM5TinKEU3x3OtrTt7OWCPrXh6voIlp+nVoquEElK8utjFy9ejLCwMGi1WkRGRmLnzp2Ntt+xYwciIyOh1WoRHh6OpUuX1mmTnJyMiIgIaDQaREREYN26dTYdt7KyErNnz0a/fv3g5uaGwMBAPPHEE7h8+fLtd7gV1U4093JV8VJoIrolf08t7onwx4y7e+DTJ6Ogn3MX9r9+Nz5/eij+PK4nJvbrhBAfVwDApcJr2Hz8Ct5POYWnV+zH0He2IurvKUhI2oO3vz+O5AMXcexyESqqzBL3isi+JB2RWrNmDWbMmIHFixfjjjvuwL///W+MHz8ex48fR5cuXeq0z8jIwIQJE/Dss8/iiy++wC+//IIXXngBHTt2xJQpUwAAer0e8fHx+Nvf/oYHHngA69atw9SpU7Fr1y5ER0c36bhlZWU4ePAg3njjDQwYMAAFBQWYMWMG7r//fuzfv79VP6PbwaUPiOh2+bprMKpHR4zq0dGyrehaJY5frhmxOnbZiKOXinD2agkMJSbsPG3AztMGS1ulXIbwjm7o3ckTvQI80auTB3oHeMLfU8PlGMgpyIQQQqqDR0dHY/DgwViyZIllW+/evTF58mTMnz+/TvvZs2djw4YNSE9Pt2xLTExEWloa9Ho9ACA+Ph5GoxGbNm2ytBk3bhy8vb2xatWqZh0XAPbt24ehQ4fiwoUL9YY8AKioqEBFRYXludFoRHBwMIqKiuDp6dmUj6RFrU+9hBlrDiG2qw/+++ywVj8+EbUf10xmnMgx4kROMU5kG5F+/VdjeVW97b1cVegV4IFeAZ7o3anm1x7+HnBRK1q5cqK6jEYjdDpdk76/JRuRMplMOHDgAF599VWr7XFxcdi9e3e979Hr9YiLi7PaNnbsWCQlJaGyshIqlQp6vR4zZ86s0+bDDz9s9nEBoKioCDKZDF5eXg22mT9/Pv761782+Hpr4xV7RNRaXNQKy6rstYQQyC4qx4kcI9Kziy0h65yhFIVllfj1XD5+PZdvaS+T1SzJ0MPPA939PdDdzx09/D3Qzc+dAYvaLMmClMFggNlshr+/v9V2f39/5OTk1PuenJycettXVVXBYDCgU6dODbap3WdzjlteXo5XX30Vv/vd7xpNpnPmzMGsWbMsz2tHpKRiWYyTp/aISAIymQyBXi4I9HLBnb1++ze3vNKMM7kllmB1IqcYJ3KMMJSYkJV/DVn517D1RO4N+wGCvV3R3c8d3f090MPfHd39GLCobZD8qr2bz5ELIRo9b15f+5u3N2WfTT1uZWUlHnnkEVRXV2Px4sWN9ATQaDTQaNpOaOGIFBG1RVqVwnLrmxvllVTgdG4JTl8pxqkrJTh1pRhnckuQV2pCZn4ZMvPL6g1YPfzd0bXj9YefG8J93bk8A7UayYKUr68vFApFnVGg3NzcOqNFtQICAuptr1Qq4ePj02ib2n3actzKykpMnToVGRkZ+OmnnySZ53Q7uIYUETkSH3cNfNw1GBbuY7U9r6QCp66U4HRuMU5dKcbpKyU4nVuC/BsC1pb0XKv3dHBTo2vHmlBVG666+rkj2NsFSgWvYqaWI1mQUqvViIyMREpKCh544AHL9pSUFEyaNKne98TExODbb7+12rZ582ZERUVBpVJZ2qSkpFjNk9q8eTNiY2NtOm5tiDp9+jS2bdtmCWqOhCNSROQMfNw1iHHXIKar9b/DhpIKS7A6d7UEZ6+W4tzVElwuKkd+qQn5pSbsO19g9R6VQoYQHzd07eiGrh3dEd7RvSZwdXSHzkXVmt0iJyHpqb1Zs2YhISEBUVFRiImJwbJly5CZmYnExEQANXOOLl26hJUrVwKouULvo48+wqxZs/Dss89Cr9cjKSnJcjUeAEyfPh0jR47EwoULMWnSJHzzzTfYsmULdu3a1eTjVlVV4aGHHsLBgwfx3XffwWw2W0awOnToALXaMYaMGaSIyJn5umvg665BbFdfq+1lpiqcu1qKs1dLLL+evVqKDEMJyiurcSa3BGdySwBcqbO/cF83hPq6ItTXDaE+1x++rnBVSz4ThtooSf9kxMfHIy8vD2+99Rays7PRt29fbNy4ESEhIQCA7OxsZGZmWtqHhYVh48aNmDlzJj7++GMEBgZi0aJFljWkACA2NharV6/G66+/jjfeeANdu3bFmjVrLGtINeW4Fy9exIYNGwAAAwcOtKp527ZtGD16tJ0+kZZTaa5GfpkJAHiHdyJqV1zVynrnYFVXC1wuulZPyCrBFWMFDCU1j73n8+vs089Dg1BfN4T5uCHE1xVhPm4I9XVDiA9DVnsn6TpSzs6WdSha2hVjOaLf2QqFXIZTfx8PhZwL3xERNaSkogrnrpYgw1CK84YynM8rrXkYSlFQVtnoe/09NQjxcbOEq1AfVwR3cEUXH1d4anm60BE5xDpSZF+1p/V83NQMUUREt+CuUaJ/kBf6B3nVea2orNISrDIMpbiQV3b915qQdcVYgSvGCuzNqDuS5eWqQpcO14PVTY9OOi0nvjsBBiknxflRREQtQ+eqwgBXLwwI9qrzWmGZCefzynDhesg6byjF+bwyXCwog6HEhMKyShSWFeHwxaI671XIZejs5dJg0NK5cjTLETBIOSkGKSIi+/NyVWOgqxoD6wlZpRVVyCooQ2ZezRINWdeXasjML0NWwTWYqqotz+vjoVWiSwdXBHm7oLPX9V+9XRDk7YIgL1d4uih5v8I2gEHKSXFVcyIiablplDU3ag6oO8emulogt7jCEqQy88tw8Ybf5xZXoLi8CscuG3HssrHe/XtolJZg1dmrNmS5orNXzbYObmoGrVbAIOWkOCJFRNR2yeUyBOi0CNBpMTSsQ53Xr5nMuFhQhgt5ZbhUeA2XCq/hYkEZLhXU/N5QYkJxRdX12+sU13sMF5UCna1CVs3vA71c0Emnhb+nFirO0bptDFJO6ipXNSciclguakXNjZv9Pep9/ZrJ/Fu4KryGiwXXLCHrYkHNiNa16/c0rFkzqy65rOY7opPOBYFeWnTS1QSs2qAV6OWCju4ayHnBUqMYpJwUR6SIiJyXi1qBbn7u6ObnXu/rFVVmZBeW14xmFdSEq4vXf59dVI6conKYzNWWKw4PZdV/HKVcBn9P7W9By0uLwJsCV3s/hcgg5aQMxZwjRUTUXmmUipo1rXzd6n29ulogr9SE7KJruFxYfv3Xa7hcVI7swpqwdcVYjqpqYTm1CBTUuy+NUm45VRig0yLAUws/z5pfA3Qa+Htq4eehhVrpnKcRGaScVO2IlC9HpIiI6CZyuQwdPTTo6KFB/6D621SZq5FbXHFT2Kr5NbuoHJcLy2EoqUBFVTXO55XhfF79Vx/W8nFTw99TC39PDQKuBy//64Grdrsjjm4xSDmhayYziiuqAPDUHhERNY9SIUfg9cnpkSH1t6moMiPneqjKLa45ZVhzurAcOcaaUa1cYwVM5mrklZqQV2rC8eyGj6lWyOHnqakTsG4MXv6emjZ1W562Uwm1GMP1ieYapRweGv6IiYjIPjRKBUJ83BDiU/8pRAAQQiC/1FQnYF0xWgevvFITTOZqXCyomTzfGHeNEn4eGvh5ajBpYGc8OrRLS3etyfgt64Ryb5ho7mhDpERE5FxkMhl83DXwcdcgIrDh+9ZVVJlxtfh62Cqq+C1sWYJXBXKKynGt0oySiqqa+yMaSjEktO7yEa2JQcoJ8Yo9IiJyNBqlAkHergjydm2wjRACJRVVyC2uQK6xArnF5Q1eudhaGKScEFc1JyIiZySTyeChVcFDq0LXjtIGqFrOeS1iO8cRKSIiotbBIOWEDFzVnIiIqFUwSDkhjkgRERG1DgYpJ3SVq5oTERG1CgYpJ8RVzYmIiFoHg5STEULwqj0iIqJWwiDlZIzlVTBVVQPgHCkiIiJ7Y5ByMrWn9Ty0SmhVComrISIicm4MUk6GV+wRERG1HgYpJ8P5UURERK2HQcrJcESKiIio9TBIORmuak5ERNR6GKScDEekiIiIWg+DlJPhquZERESth0HKyXBVcyIiotbDIOVkeNUeERFR62GQciLmaoG860HKjyNSREREdscg5UTyS02oFoBMBnRwU0tdDhERkdNjkHIitfOjfNzUUCr4oyUiIrI3fts6kdr5Ub6cH0VERNQqGKScCNeQIiIial0MUk6Eq5oTERG1LgYpJ8IRKSIiotbFIOVEuKo5ERFR62KQciIckSIiImpdDFJOhKuaExERtS4GKSfCESkiIqLWxSDlJCqqzCi6VgmAQYqIiKi1MEg5CUOJCQCgUsigc1FJXA0REVH7IHmQWrx4McLCwqDVahEZGYmdO3c22n7Hjh2IjIyEVqtFeHg4li5dWqdNcnIyIiIioNFoEBERgXXr1tl83LVr12Ls2LHw9fWFTCbDoUOHbquf9nbjFXsymUziaoiIiNoHSYPUmjVrMGPGDLz22mtITU3FiBEjMH78eGRmZtbbPiMjAxMmTMCIESOQmpqKuXPnYtq0aUhOTra00ev1iI+PR0JCAtLS0pCQkICpU6diz549Nh23tLQUd9xxBxYsWGC/D6AFcX4UERFR65MJIYRUB4+OjsbgwYOxZMkSy7bevXtj8uTJmD9/fp32s2fPxoYNG5Cenm7ZlpiYiLS0NOj1egBAfHw8jEYjNm3aZGkzbtw4eHt7Y9WqVTYf9/z58wgLC0NqaioGDhzYaH8qKipQUVFheW40GhEcHIyioiJ4eno24RNpvlV7MzFn7RHc3dsPnz45xK7HIiIicmZGoxE6na5J39+SjUiZTCYcOHAAcXFxVtvj4uKwe/fuet+j1+vrtB87diz279+PysrKRtvU7rM5x22q+fPnQ6fTWR7BwcG3tT9bcESKiIio9UkWpAwGA8xmM/z9/a22+/v7Iycnp9735OTk1Nu+qqoKBoOh0Ta1+2zOcZtqzpw5KCoqsjyysrJua3+24KrmRERErU8pdQE3T4wWQjQ6Wbq+9jdvb8o+bT1uU2g0Gmg00gSZ2iDlyxEpIiKiViPZiJSvry8UCkWdUaDc3Nw6o0W1AgIC6m2vVCrh4+PTaJvafTbnuI6Aq5oTERG1PsmClFqtRmRkJFJSUqy2p6SkIDY2tt73xMTE1Gm/efNmREVFQaVSNdqmdp/NOa4j4BwpIiKi1ifpqb1Zs2YhISEBUVFRiImJwbJly5CZmYnExEQANXOOLl26hJUrVwKouULvo48+wqxZs/Dss89Cr9cjKSnJcjUeAEyfPh0jR47EwoULMWnSJHzzzTfYsmULdu3a1eTjAkB+fj4yMzNx+fJlAMDJkycB1Ix4BQQE2P2zsYUQgkGKiIhICkJiH3/8sQgJCRFqtVoMHjxY7Nixw/Lak08+KUaNGmXVfvv27WLQoEFCrVaL0NBQsWTJkjr7/Oqrr0TPnj2FSqUSvXr1EsnJyTYdVwghli9fLgDUecybN6/JfSsqKhIARFFRUZPf0xzF5ZUiZPZ3ImT2d6KkvNKuxyIiInJ2tnx/S7qOlLOzZR2K25FhKMWYf26Hm1qBY2+Ns9txiIiI2gOHWEeKWg5P6xEREUmDQcoJGEoYpIiIiKTAIOUEOCJFREQkDQYpJ8BVzYmIiKTBIOUELKuaM0gRERG1KgYpJ3CVc6SIiIgkwSDlBDhHioiISBoMUk6AQYqIiEgaDFIOrrpacPkDIiIiiTBIObjCa5Woqq5ZnN7HjUGKiIioNTFIObja0ShvVxXUSv44iYiIWhO/eR0c50cRERFJh0HKwTFIERERSYdBysFxVXMiIiLpMEg5uNrFOLmqORERUetjkHJwPLVHREQkHQYpB8cgRUREJB0GKQfHIEVERCQdBikHxxsWExERSYdByoFVmquRX2oCwKv2iIiIpMAg5cBqQ5RCLoO3q1riaoiIiNofBikHVjs/ytddDblcJnE1RERE7Q+DlAPjRHMiIiJpMUg5MK5qTkREJC0GKQfGVc2JiIikxSDlwHhqj4iISFoMUg6MQYqIiEhaDFIOjEGKiIhIWgxSDsyyqjnnSBEREUmCQcqBcUSKiIhIWgxSDuqayYySiioADFJERERSYZByUIbrp/W0KjncNUqJqyEiImqfGKQcVO4Np/VkMt4ehoiISAoMUg7qt/vs8bQeERGRVBikHBSv2CMiIpIeg5SD4hV7RERE0mOQclAMUkRERNJjkHJQDFJERETSY5ByUJwjRUREJD0GKQdl4IgUERGR5BikHJAQ4rcRKQYpIiIiyTBIOSBjeRVMVdUAuI4UERGRlCQPUosXL0ZYWBi0Wi0iIyOxc+fORtvv2LEDkZGR0Gq1CA8Px9KlS+u0SU5ORkREBDQaDSIiIrBu3TqbjyuEwJtvvonAwEC4uLhg9OjROHbs2O11toXUTjT31CqhVSkkroaIiKj9kjRIrVmzBjNmzMBrr72G1NRUjBgxAuPHj0dmZma97TMyMjBhwgSMGDECqampmDt3LqZNm4bk5GRLG71ej/j4eCQkJCAtLQ0JCQmYOnUq9uzZY9Nx3333Xbz//vv46KOPsG/fPgQEBOCee+5BcXGx/T6QJrKsas7TekRERNISEho6dKhITEy02tarVy/x6quv1tv+z3/+s+jVq5fVtueee04MGzbM8nzq1Kli3LhxVm3Gjh0rHnnkkSYft7q6WgQEBIgFCxZYXi8vLxc6nU4sXbq0yf0rKioSAERRUVGT39MU3xy6JEJmfyemLt3dovslIiIi276/JRuRMplMOHDgAOLi4qy2x8XFYffu3fW+R6/X12k/duxY7N+/H5WVlY22qd1nU46bkZGBnJwcqzYajQajRo1qsDYAqKiogNFotHrYA9eQIiIiahskC1IGgwFmsxn+/v5W2/39/ZGTk1Pve3JycuptX1VVBYPB0Gib2n025bi1v9pSGwDMnz8fOp3O8ggODm6w7e2oqDJDq5IzSBEREUlMKXUBMpnM6rkQos62W7W/eXtT9tlSbW40Z84czJo1y/LcaDTaJUy9MLobnh/VFeZq0eL7JiIioqaTLEj5+vpCoVDUGeHJzc2tMxJUKyAgoN72SqUSPj4+jbap3WdTjhsQEACgZmSqU6dOTaoNqDn9p9G0ziiRTCaDUtFwqCMiIiL7k+zUnlqtRmRkJFJSUqy2p6SkIDY2tt73xMTE1Gm/efNmREVFQaVSNdqmdp9NOW5YWBgCAgKs2phMJuzYsaPB2oiIiKgdsu+898atXr1aqFQqkZSUJI4fPy5mzJgh3NzcxPnz54UQQrz66qsiISHB0v7cuXPC1dVVzJw5Uxw/flwkJSUJlUolvv76a0ubX375RSgUCrFgwQKRnp4uFixYIJRKpfj111+bfFwhhFiwYIHQ6XRi7dq14siRI+LRRx8VnTp1Ekajscn9s9dVe0RERGQ/tnx/SxqkhBDi448/FiEhIUKtVovBgweLHTt2WF578sknxahRo6zab9++XQwaNEio1WoRGhoqlixZUmefX331lejZs6dQqVSiV69eIjk52abjClGzBMK8efNEQECA0Gg0YuTIkeLIkSM29Y1BioiIyPHY8v0tE0JwxrKdGI1G6HQ6FBUVwdPTU+pyiIiIqAls+f6W/BYxRERERI6KQYqIiIiomRikiIiIiJqJQYqIiIiomRikiIiIiJqJQYqIiIiomRikiIiIiJqJQYqIiIiomRikiIiIiJpJKXUBzqx20Xij0ShxJURERNRUtd/bTbn5C4OUHRUXFwMAgoODJa6EiIiIbFVcXAydTtdoG95rz46qq6tx+fJleHh4QCaTtei+jUYjgoODkZWV1a7u49de+w2w7+x7++p7e+03wL63hb4LIVBcXIzAwEDI5Y3PguKIlB3J5XIEBQXZ9Rienp7t7i8a0H77DbDv7Hv70l77DbDvUvf9ViNRtTjZnIiIiKiZGKSIiIiImolBykFpNBrMmzcPGo1G6lJaVXvtN8C+s+/tq+/ttd8A++5ofedkcyIiIqJm4ogUERERUTMxSBERERE1E4MUERERUTMxSBERERE1E4OUA1q8eDHCwsKg1WoRGRmJnTt3Sl1So37++Wfcd999CAwMhEwmw/r1661eF0LgzTffRGBgIFxcXDB69GgcO3bMqk1FRQX++Mc/wtfXF25ubrj//vtx8eJFqzYFBQVISEiATqeDTqdDQkICCgsLrdpkZmbivvvug5ubG3x9fTFt2jSYTCZ7dBvz58/HkCFD4OHhAT8/P0yePBknT55sF31fsmQJ+vfvb1lULyYmBps2bXL6ft9s/vz5kMlkmDFjhmWbs/b9zTffhEwms3oEBAQ4fb9rXbp0CY8//jh8fHzg6uqKgQMH4sCBA5bXnbX/oaGhdX7uMpkML774olP324ogh7J69WqhUqnEJ598Io4fPy6mT58u3NzcxIULF6QurUEbN24Ur732mkhOThYAxLp166xeX7BggfDw8BDJycniyJEjIj4+XnTq1EkYjUZLm8TERNG5c2eRkpIiDh48KMaMGSMGDBggqqqqLG3GjRsn+vbtK3bv3i12794t+vbtK+69917L61VVVaJv375izJgx4uDBgyIlJUUEBgaKl156yS79Hjt2rFi+fLk4evSoOHTokJg4caLo0qWLKCkpcfq+b9iwQXz//ffi5MmT4uTJk2Lu3LlCpVKJo0ePOnW/b7R3714RGhoq+vfvL6ZPn27Z7qx9nzdvnujTp4/Izs62PHJzc52+30IIkZ+fL0JCQsRTTz0l9uzZIzIyMsSWLVvEmTNnnL7/ubm5Vj/zlJQUAUBs27bNqft9IwYpBzN06FCRmJhota1Xr17i1Vdflagi29wcpKqrq0VAQIBYsGCBZVt5ebnQ6XRi6dKlQgghCgsLhUqlEqtXr7a0uXTpkpDL5eKHH34QQghx/PhxAUD8+uuvljZ6vV4AECdOnBBC1AQ6uVwuLl26ZGmzatUqodFoRFFRkV36e6Pc3FwBQOzYsUMI0b76LoQQ3t7e4tNPP20X/S4uLhbdu3cXKSkpYtSoUZYg5cx9nzdvnhgwYEC9rzlzv4UQYvbs2WL48OENvu7s/b/R9OnTRdeuXUV1dXW76TdP7TkQk8mEAwcOIC4uzmp7XFwcdu/eLVFVtycjIwM5OTlWfdJoNBg1apSlTwcOHEBlZaVVm8DAQPTt29fSRq/XQ6fTITo62tJm2LBh0Ol0Vm369u2LwMBAS5uxY8eioqLCagjeXoqKigAAHTp0ANB++m42m7F69WqUlpYiJiamXfT7xRdfxMSJE3H33XdbbXf2vp8+fRqBgYEICwvDI488gnPnzrWLfm/YsAFRUVF4+OGH4efnh0GDBuGTTz6xvO7s/a9lMpnwxRdf4Omnn4ZMJms3/WaQciAGgwFmsxn+/v5W2/39/ZGTkyNRVbentu7G+pSTkwO1Wg1vb+9G2/j5+dXZv5+fn1Wbm4/j7e0NtVpt989PCIFZs2Zh+PDh6Nu3r6UewHn7fuTIEbi7u0Oj0SAxMRHr1q1DRESE0/d79erVOHjwIObPn1/nNWfue3R0NFauXIkff/wRn3zyCXJychAbG4u8vDyn7jcAnDt3DkuWLEH37t3x448/IjExEdOmTcPKlSstNdX25UbO0v9a69evR2FhIZ566ilLLYDz91tp172TXchkMqvnQog62xxNc/p0c5v62jenjT289NJLOHz4MHbt2lXnNWfte8+ePXHo0CEUFhYiOTkZTz75JHbs2NFgPc7Q76ysLEyfPh2bN2+GVqttsJ0z9n38+PGW3/fr1w8xMTHo2rUrPv/8cwwbNqzeepyh3wBQXV2NqKgovPPOOwCAQYMG4dixY1iyZAmeeOKJButylv7XSkpKwvjx461Gheqrx9n6zREpB+Lr6wuFQlEnXefm5tZJ4o6i9qqexvoUEBAAk8mEgoKCRttcuXKlzv6vXr1q1ebm4xQUFKCystKun98f//hHbNiwAdu2bUNQUJBlu7P3Xa1Wo1u3boiKisL8+fMxYMAA/Otf/3Lqfh84cAC5ubmIjIyEUqmEUqnEjh07sGjRIiiVSssxnbHvN3Nzc0O/fv1w+vRpp/6ZA0CnTp0QERFhta13797IzMy01AQ4b/8B4MKFC9iyZQueeeYZy7b20G+AQcqhqNVqREZGIiUlxWp7SkoKYmNjJarq9oSFhSEgIMCqTyaTCTt27LD0KTIyEiqVyqpNdnY2jh49amkTExODoqIi7N2719Jmz549KCoqsmpz9OhRZGdnW9ps3rwZGo0GkZGRLd43IQReeuklrF27Fj/99BPCwsLaTd/rI4RARUWFU/f7rrvuwpEjR3Do0CHLIyoqCo899hgOHTqE8PBwp+37zSoqKpCeno5OnTo59c8cAO644446S5ucOnUKISEhANrH3/Xly5fDz88PEydOtGxrD/0GwOUPHE3t8gdJSUni+PHjYsaMGcLNzU2cP39e6tIaVFxcLFJTU0VqaqoAIN5//32RmppqWbJhwYIFQqfTibVr14ojR46IRx99tN7LY4OCgsSWLVvEwYMHxZ133lnv5bH9+/cXer1e6PV60a9fv3ovj73rrrvEwYMHxZYtW0RQUJDdLo99/vnnhU6nE9u3b7e6PLisrMzSxln7PmfOHPHzzz+LjIwMcfjwYTF37lwhl8vF5s2bnbrf9bnxqj0hnLfvL7/8sti+fbs4d+6c+PXXX8W9994rPDw8LP82OWu/hahZ6kKpVIq3335bnD59Wnz55ZfC1dVVfPHFF5Y2ztx/s9ksunTpImbPnl3nNWfudy0GKQf08ccfi5CQEKFWq8XgwYMtl9O3Vdu2bRMA6jyefPJJIUTNpcHz5s0TAQEBQqPRiJEjR4ojR45Y7ePatWvipZdeEh06dBAuLi7i3nvvFZmZmVZt8vLyxGOPPSY8PDyEh4eHeOyxx0RBQYFVmwsXLoiJEycKFxcX0aFDB/HSSy+J8vJyu/S7vj4DEMuXL7e0cda+P/3005Y/ox07dhR33XWXJUQ5c7/rc3OQcta+164PpFKpRGBgoHjwwQfFsWPHnL7ftb799lvRt29fodFoRK9evcSyZcusXnfm/v/4448CgDh58mSd15y537VkQghh3zEvIiIiIufEOVJEREREzcQgRURERNRMDFJEREREzcQgRURERNRMDFJEREREzcQgRURERNRMDFJEREREzcQgRURERNRMDFJERABGjx6NGTNmSF0GETkYBikicigymazRx1NPPdWs/a5duxZ/+9vfbqu23NxcPPfcc+jSpQs0Gg0CAgIwduxY6PV6q/rXr19/W8chorZDKXUBRES2uPHu7mvWrMFf/vIXnDx50rLNxcXFqn1lZSVUKtUt99uhQ4fbrm3KlCmorKzE559/jvDwcFy5cgVbt25Ffn7+be+biNomjkgRkUMJCAiwPHQ6HWQymeV5eXk5vLy88L///Q+jR4+GVqvFF198gby8PDz66KMICgqCq6sr+vXrh1WrVlnt9+ZTe6GhoXjnnXfw9NNPw8PDA126dMGyZcsarKuwsBC7du3CwoULMWbMGISEhGDo0KGYM2cOJk6caNknADzwwAOQyWSW5wDw7bffIjIyElqtFuHh4fjrX/+Kqqoqy+symQxLlizB+PHj4eLigrCwMHz11Ve3/4ES0W1hkCIipzN79mxMmzYN6enpGDt2LMrLyxEZGYnvvvsOR48exR/+8AckJCRgz549je7nvffeQ1RUFFJTU/HCCy/g+eefx4kTJ+pt6+7uDnd3d6xfvx4VFRX1ttm3bx8AYPny5cjOzrY8//HHH/H4449j2rRpOH78OP79739jxYoVePvtt63e/8Ybb2DKlClIS0vD448/jkcffRTp6em2fjxE1JIEEZGDWr58udDpdJbnGRkZAoD48MMPb/neCRMmiJdfftnyfNSoUWL69OmW5yEhIeLxxx+3PK+urhZ+fn5iyZIlDe7z66+/Ft7e3kKr1YrY2FgxZ84ckZaWZtUGgFi3bp3VthEjRoh33nnHatt//vMf0alTJ6v3JSYmWrWJjo4Wzz///C37SkT2wxEpInI6UVFRVs/NZjPefvtt9O/fHz4+PnB3d8fmzZuRmZnZ6H769+9v+X3tKcTc3NwG20+ZMgWXL1/Ghg0bMHbsWGzfvh2DBw/GihUrGj3OgQMH8NZbb1lGtdzd3fHss88iOzsbZWVllnYxMTFW74uJieGIFJHEONmciJyOm5ub1fP33nsPH3zwAT788EP069cPbm5umDFjBkwmU6P7uXmSukwmQ3V1daPv0Wq1uOeee3DPPffgL3/5C5555hnMmzev0asJq6ur8de//hUPPvhgvftrjEwma/R1IrIvBikicno7d+7EpEmT8PjjjwOoCS6nT59G79697X7siIgIq+UOVCoVzGazVZvBgwfj5MmT6NatW6P7+vXXX/HEE09YPR80aFCL1ktEtmGQIiKn161bNyQnJ2P37t3w9vbG+++/j5ycnBYNUnl5eXj44Yfx9NNPo3///vDw8MD+/fvx7rvvYtKkSZZ2oaGh2Lp1K+644w5oNBp4e3vjL3/5C+69914EBwfj4Ycfhlwux+HDh3HkyBH8/e9/t7z3q6++QlRUFIYPH44vv/wSe/fuRVJSUov1gYhsxzlSROT03njjDQwePBhjx47F6NGjERAQgMmTJ7foMdzd3REdHY0PPvgAI0eORN++ffHGG2/g2WefxUcffWRp99577yElJQXBwcGW0aSxY8fiu+++Q0pKCoYMGYJhw4bh/fffR0hIiNUx/vrXv2L16tXo378/Pv/8c3z55ZeIiIho0X4QkW1kQgghdRFERNQ4mUyGdevWtXgAJKLbwxEpIiIiomZikCIiIiJqJk42JyJyAJyFQdQ2cUSKiIiIqJkYpIiIiIiaiUGKiIiIqJkYpIiIiIiaiUGKiIiIqJkYpIiIiIiaiUGKiIiIqJkYpIiIiIia6f8D4VN/mTzgy28AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_lr = CustomSchedule(128, 10_000, weight_decay=None)\n",
    "finetune_lr = CustomSchedule(512, 5_000, weight_decay=None)\n",
    "plt.plot(tmp_lr(tf.range(10_000_000 // (32* 5), dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show();\n",
    "\n",
    "plt.plot(finetune_lr(tf.range(2_300_000 // (32), dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "\n",
    "def flat_gradients(grads_or_idx_slices: tf.Tensor) -> tf.Tensor:\n",
    "    '''Convert gradients if it's tf.IndexedSlices.\n",
    "    When computing gradients for operation concerning `tf.gather`, the type of gradients \n",
    "    '''\n",
    "    if type(grads_or_idx_slices) == tf.IndexedSlices:\n",
    "        return tf.scatter_nd(\n",
    "            tf.expand_dims(grads_or_idx_slices.indices, 1),\n",
    "            grads_or_idx_slices.values,\n",
    "            tf.cast(grads_or_idx_slices.dense_shape, tf.int64)\n",
    "        )\n",
    "    return grads_or_idx_slices\n",
    "\n",
    "def backward_optimization(num_grad_steps, global_gradients, step_gradients, step, model, optimizer):\n",
    "    if not global_gradients:\n",
    "        global_gradients = step_gradients\n",
    "    else:\n",
    "        for i, g in enumerate(step_gradients):\n",
    "            global_gradients[i] += flat_gradients(g)\n",
    "    if (step + 1) % num_grad_steps == 0:\n",
    "        global_gradients = zip(global_gradients, model.trainable_variables)\n",
    "        optimizer.apply_gradients(global_gradients)\n",
    "        global_gradients = []\n",
    "    return global_gradients\n",
    "\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def train_step(*inputs, target, **kwargs):\n",
    "    l_loss = kwargs['loss']\n",
    "    num_accum_steps = tf.cast(kwargs['num_accum_steps'], tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(*inputs, training=True)\n",
    "        loss = loss_function(target, predictions)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss / num_accum_steps)\n",
    "\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "    # gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    l_loss(loss)\n",
    "    return gradients\n",
    "  \n",
    "@tf.function\n",
    "def test_step(*inputs, target, **kwargs):\n",
    "    l_loss = kwargs['loss']\n",
    "    predictions = model(*inputs, training=False)\n",
    "    loss = loss_function(target, predictions)\n",
    "    l_loss(loss)\n",
    "\n",
    "\n",
    "def metrics_reset_states(*metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "\n",
    "def fancy_printer(loss_tracker, epoch, batch_num, start, step='train', dict_metrics={}, num_epochs=1, **kwargs):\n",
    "    num_step = kwargs['num_step']\n",
    "    dict_print_metrics = {' '.join(f\"{key}:{value:.6f}\" for key, value in dict_metrics.items())}\n",
    "    if step!='epoch':\n",
    "        printer = f'[{step} Epoch]{epoch + 1}/{num_epochs} [Time]{time.time() - start:.2f} [Step]{num_step} [Batch]{batch_num} [Speed]{((time.time() - start)/max(1, batch_num))*1000:.2f}ms/step '\n",
    "        printer += f'[Loss]{loss_tracker.result():.4f} ' + '[Metrics]' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "    else:\n",
    "        train_loss, val_loss = kwargs['train_loss'], kwargs['val_loss']\n",
    "        print(f'\\nTime taken for epoch {epoch+1}/{num_epochs}: {time.time() - start:.2f} secs')\n",
    "        printer = f'[Epoch]{epoch + 1}/{num_epochs} - [Train Loss]{train_loss.result():.4f} '\n",
    "        printer += f'- [Val Loss]{val_loss.result():.4f} ' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "\n",
    "\n",
    "def log_wandb_metrics(step='train', num_step=0, dict_metrics=None, gradients=None, plot_image=False, **kwargs):\n",
    "    # Scalar metrics\n",
    "    if step=='train' or step=='val':\n",
    "        wandb.log({name : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "    if step=='epoch':\n",
    "        wandb.log({f'epoch_{name}' : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "\n",
    "    # Gradients\n",
    "    if gradients:\n",
    "        wandb.log({\n",
    "            'mean_norm_gradients' : np.mean([tf.norm(x) for x in gradients]), \n",
    "            'max_norm_gradients': np.max([tf.norm(x) for x in gradients])\n",
    "        })\n",
    "\n",
    "def init_wandb(wandb_project='<your_project>', entity='', run_name='', dict_config=None):\n",
    "    wandb.init(project=wandb_project, entity=entity, name=run_name, settings=wandb.Settings(code_dir=\".\"),\n",
    "               config=dict_config)\n",
    "    wandb.run.log_code(\".\")\n",
    "\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menric1296\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/enric/SSD1TB/KAGGLE/025_Kaggle-OTTO Recsys-2022/1_Scripts/wandb/run-20221120_130654-tbae945o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/enric1296/otto-recsys/runs/tbae945o\" target=\"_blank\">model_bert4rec_complete_0.7_2022-11-20 13:06:53</a></strong> to <a href=\"https://wandb.ai/enric1296/otto-recsys\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n",
      "Latest checkpoint restored!!\n",
      "================================================================================\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 13:06:56.706221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/home/enric/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 167903104 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-11-20 13:06:57.750123: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1f3f8440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-20 13:06:57.750143: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2022-11-20 13:06:57.765797: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. model_bert4_rec/encoder_transformer_block/dropout_2/dropout/random_uniform/RandomUniform\n",
      "2022-11-20 13:06:57.768581: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-20 13:06:59.187835: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-11-20 13:06:59.617711: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch]1/1 [Time]2.94 [Step]0 [Batch]0 [Speed]2936.70ms/step [Loss]8.1226 [Metrics]{'train_loss:8.122582 lr:0.000000'}\n",
      "[Train Epoch]1/1 [Time]32.97 [Step]100 [Batch]500 [Speed]65.95ms/step [Loss]8.3068 [Metrics]{'train_loss:8.306808 lr:0.000009'}\n",
      "[Train Epoch]1/1 [Time]59.53 [Step]200 [Batch]1000 [Speed]59.53ms/step [Loss]8.3039 [Metrics]{'train_loss:8.303894 lr:0.000018'}\n",
      "[Train Epoch]1/1 [Time]86.05 [Step]300 [Batch]1500 [Speed]57.36ms/step [Loss]8.2933 [Metrics]{'train_loss:8.293296 lr:0.000027'}\n",
      "[Train Epoch]1/1 [Time]112.73 [Step]400 [Batch]2000 [Speed]56.37ms/step [Loss]8.2873 [Metrics]{'train_loss:8.287313 lr:0.000035'}\n",
      "[Train Epoch]1/1 [Time]139.47 [Step]500 [Batch]2500 [Speed]55.79ms/step [Loss]8.2812 [Metrics]{'train_loss:8.281203 lr:0.000044'}\n",
      "[Train Epoch]1/1 [Time]165.98 [Step]600 [Batch]3000 [Speed]55.33ms/step [Loss]8.2760 [Metrics]{'train_loss:8.275966 lr:0.000053'}\n",
      "[Train Epoch]1/1 [Time]192.52 [Step]700 [Batch]3500 [Speed]55.01ms/step [Loss]8.2839 [Metrics]{'train_loss:8.283860 lr:0.000062'}\n",
      "[Train Epoch]1/1 [Time]218.98 [Step]800 [Batch]4000 [Speed]54.74ms/step [Loss]8.2894 [Metrics]{'train_loss:8.289353 lr:0.000071'}\n",
      "[Train Epoch]1/1 [Time]245.45 [Step]900 [Batch]4500 [Speed]54.54ms/step [Loss]8.2944 [Metrics]{'train_loss:8.294362 lr:0.000080'}\n",
      "[Train Epoch]1/1 [Time]271.92 [Step]1000 [Batch]5000 [Speed]54.38ms/step [Loss]8.2961 [Metrics]{'train_loss:8.296144 lr:0.000088'}\n",
      "[Train Epoch]1/1 [Time]298.28 [Step]1100 [Batch]5500 [Speed]54.23ms/step [Loss]8.2971 [Metrics]{'train_loss:8.297080 lr:0.000097'}\n",
      "[Train Epoch]1/1 [Time]324.56 [Step]1200 [Batch]6000 [Speed]54.09ms/step [Loss]8.3001 [Metrics]{'train_loss:8.300092 lr:0.000106'}\n",
      "[Train Epoch]1/1 [Time]351.04 [Step]1300 [Batch]6500 [Speed]54.01ms/step [Loss]8.3051 [Metrics]{'train_loss:8.305102 lr:0.000115'}\n",
      "[Train Epoch]1/1 [Time]377.53 [Step]1400 [Batch]7000 [Speed]53.93ms/step [Loss]8.3104 [Metrics]{'train_loss:8.310383 lr:0.000124'}\n",
      "[Train Epoch]1/1 [Time]404.08 [Step]1500 [Batch]7500 [Speed]53.88ms/step [Loss]8.3132 [Metrics]{'train_loss:8.313167 lr:0.000133'}\n",
      "[Train Epoch]1/1 [Time]430.50 [Step]1600 [Batch]8000 [Speed]53.81ms/step [Loss]8.3204 [Metrics]{'train_loss:8.320436 lr:0.000141'}\n",
      "[Train Epoch]1/1 [Time]456.98 [Step]1700 [Batch]8500 [Speed]53.76ms/step [Loss]8.3260 [Metrics]{'train_loss:8.326024 lr:0.000150'}\n",
      "[Train Epoch]1/1 [Time]483.54 [Step]1800 [Batch]9000 [Speed]53.73ms/step [Loss]8.3325 [Metrics]{'train_loss:8.332511 lr:0.000159'}\n",
      "[Train Epoch]1/1 [Time]510.12 [Step]1900 [Batch]9500 [Speed]53.70ms/step [Loss]8.3364 [Metrics]{'train_loss:8.336411 lr:0.000168'}\n",
      "[Train Epoch]1/1 [Time]536.69 [Step]2000 [Batch]10000 [Speed]53.67ms/step [Loss]8.3359 [Metrics]{'train_loss:8.335917 lr:0.000177'}\n",
      "[Train Epoch]1/1 [Time]563.14 [Step]2100 [Batch]10500 [Speed]53.63ms/step [Loss]8.3382 [Metrics]{'train_loss:8.338160 lr:0.000186'}\n",
      "[Train Epoch]1/1 [Time]589.50 [Step]2200 [Batch]11000 [Speed]53.59ms/step [Loss]8.3363 [Metrics]{'train_loss:8.336338 lr:0.000194'}\n",
      "[Train Epoch]1/1 [Time]615.80 [Step]2300 [Batch]11500 [Speed]53.55ms/step [Loss]8.3375 [Metrics]{'train_loss:8.337451 lr:0.000203'}\n",
      "[Train Epoch]1/1 [Time]642.04 [Step]2400 [Batch]12000 [Speed]53.50ms/step [Loss]8.3374 [Metrics]{'train_loss:8.337360 lr:0.000212'}\n",
      "[Train Epoch]1/1 [Time]668.30 [Step]2500 [Batch]12500 [Speed]53.46ms/step [Loss]8.3366 [Metrics]{'train_loss:8.336621 lr:0.000221'}\n",
      "[Train Epoch]1/1 [Time]694.31 [Step]2600 [Batch]13000 [Speed]53.41ms/step [Loss]8.3355 [Metrics]{'train_loss:8.335513 lr:0.000230'}\n",
      "[Train Epoch]1/1 [Time]720.39 [Step]2700 [Batch]13500 [Speed]53.36ms/step [Loss]8.3352 [Metrics]{'train_loss:8.335223 lr:0.000239'}\n",
      "[Train Epoch]1/1 [Time]746.55 [Step]2800 [Batch]14000 [Speed]53.32ms/step [Loss]8.3357 [Metrics]{'train_loss:8.335707 lr:0.000247'}\n",
      "[Train Epoch]1/1 [Time]772.93 [Step]2900 [Batch]14500 [Speed]53.31ms/step [Loss]8.3344 [Metrics]{'train_loss:8.334445 lr:0.000256'}\n",
      "[Train Epoch]1/1 [Time]799.35 [Step]3000 [Batch]15000 [Speed]53.29ms/step [Loss]8.3349 [Metrics]{'train_loss:8.334891 lr:0.000265'}\n",
      "[Train Epoch]1/1 [Time]825.82 [Step]3100 [Batch]15500 [Speed]53.28ms/step [Loss]8.3330 [Metrics]{'train_loss:8.332966 lr:0.000274'}\n",
      "[Train Epoch]1/1 [Time]852.21 [Step]3200 [Batch]16000 [Speed]53.26ms/step [Loss]8.3298 [Metrics]{'train_loss:8.329840 lr:0.000283'}\n",
      "[Train Epoch]1/1 [Time]878.59 [Step]3300 [Batch]16500 [Speed]53.25ms/step [Loss]8.3285 [Metrics]{'train_loss:8.328547 lr:0.000292'}\n",
      "[Train Epoch]1/1 [Time]905.09 [Step]3400 [Batch]17000 [Speed]53.24ms/step [Loss]8.3271 [Metrics]{'train_loss:8.327118 lr:0.000301'}\n",
      "[Train Epoch]1/1 [Time]931.47 [Step]3500 [Batch]17500 [Speed]53.23ms/step [Loss]8.3259 [Metrics]{'train_loss:8.325908 lr:0.000309'}\n",
      "[Train Epoch]1/1 [Time]957.91 [Step]3600 [Batch]18000 [Speed]53.22ms/step [Loss]8.3267 [Metrics]{'train_loss:8.326703 lr:0.000318'}\n",
      "[Train Epoch]1/1 [Time]984.26 [Step]3700 [Batch]18500 [Speed]53.20ms/step [Loss]8.3276 [Metrics]{'train_loss:8.327575 lr:0.000327'}\n",
      "[Train Epoch]1/1 [Time]1010.41 [Step]3800 [Batch]19000 [Speed]53.18ms/step [Loss]8.3275 [Metrics]{'train_loss:8.327538 lr:0.000336'}\n",
      "[Train Epoch]1/1 [Time]1036.60 [Step]3900 [Batch]19500 [Speed]53.16ms/step [Loss]8.3250 [Metrics]{'train_loss:8.324973 lr:0.000345'}\n",
      "[Train Epoch]1/1 [Time]1062.66 [Step]4000 [Batch]20000 [Speed]53.13ms/step [Loss]8.3228 [Metrics]{'train_loss:8.322788 lr:0.000354'}\n",
      "[Train Epoch]1/1 [Time]1088.89 [Step]4100 [Batch]20500 [Speed]53.12ms/step [Loss]8.3226 [Metrics]{'train_loss:8.322574 lr:0.000362'}\n",
      "[Train Epoch]1/1 [Time]1114.99 [Step]4200 [Batch]21000 [Speed]53.09ms/step [Loss]8.3212 [Metrics]{'train_loss:8.321203 lr:0.000371'}\n",
      "[Train Epoch]1/1 [Time]1141.18 [Step]4300 [Batch]21500 [Speed]53.08ms/step [Loss]8.3203 [Metrics]{'train_loss:8.320279 lr:0.000380'}\n",
      "[Train Epoch]1/1 [Time]1167.46 [Step]4400 [Batch]22000 [Speed]53.07ms/step [Loss]8.3188 [Metrics]{'train_loss:8.318778 lr:0.000389'}\n",
      "[Train Epoch]1/1 [Time]1193.80 [Step]4500 [Batch]22500 [Speed]53.06ms/step [Loss]8.3172 [Metrics]{'train_loss:8.317234 lr:0.000398'}\n",
      "[Train Epoch]1/1 [Time]1220.25 [Step]4600 [Batch]23000 [Speed]53.05ms/step [Loss]8.3152 [Metrics]{'train_loss:8.315185 lr:0.000407'}\n",
      "[Train Epoch]1/1 [Time]1246.85 [Step]4700 [Batch]23500 [Speed]53.06ms/step [Loss]8.3142 [Metrics]{'train_loss:8.314242 lr:0.000415'}\n",
      "[Train Epoch]1/1 [Time]1273.33 [Step]4800 [Batch]24000 [Speed]53.06ms/step [Loss]8.3121 [Metrics]{'train_loss:8.312099 lr:0.000424'}\n",
      "[Train Epoch]1/1 [Time]1299.82 [Step]4900 [Batch]24500 [Speed]53.05ms/step [Loss]8.3114 [Metrics]{'train_loss:8.311375 lr:0.000433'}\n",
      "Saving checkpoint for epoch 1 at step 25000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]1331.84 [Step]5000 [Batch]25000 [Speed]53.27ms/step [Loss]8.3102 [Metrics]{'train_loss:8.310211 lr:0.000442'}\n",
      "[Train Epoch]1/1 [Time]1358.11 [Step]5100 [Batch]25500 [Speed]53.26ms/step [Loss]8.3113 [Metrics]{'train_loss:8.311346 lr:0.000451'}\n",
      "[Train Epoch]1/1 [Time]1383.61 [Step]5200 [Batch]26000 [Speed]53.22ms/step [Loss]8.3125 [Metrics]{'train_loss:8.312460 lr:0.000460'}\n",
      "[Train Epoch]1/1 [Time]1409.06 [Step]5300 [Batch]26500 [Speed]53.17ms/step [Loss]8.3125 [Metrics]{'train_loss:8.312526 lr:0.000468'}\n",
      "[Train Epoch]1/1 [Time]1434.45 [Step]5400 [Batch]27000 [Speed]53.13ms/step [Loss]8.3130 [Metrics]{'train_loss:8.312998 lr:0.000477'}\n",
      "[Train Epoch]1/1 [Time]1460.24 [Step]5500 [Batch]27500 [Speed]53.10ms/step [Loss]8.3126 [Metrics]{'train_loss:8.312560 lr:0.000486'}\n",
      "[Train Epoch]1/1 [Time]1486.27 [Step]5600 [Batch]28000 [Speed]53.08ms/step [Loss]8.3118 [Metrics]{'train_loss:8.311791 lr:0.000495'}\n",
      "[Train Epoch]1/1 [Time]1512.46 [Step]5700 [Batch]28500 [Speed]53.07ms/step [Loss]8.3112 [Metrics]{'train_loss:8.311161 lr:0.000504'}\n",
      "[Train Epoch]1/1 [Time]1538.65 [Step]5800 [Batch]29000 [Speed]53.06ms/step [Loss]8.3108 [Metrics]{'train_loss:8.310795 lr:0.000513'}\n",
      "[Train Epoch]1/1 [Time]1564.78 [Step]5900 [Batch]29500 [Speed]53.04ms/step [Loss]8.3091 [Metrics]{'train_loss:8.309146 lr:0.000521'}\n",
      "[Train Epoch]1/1 [Time]1591.24 [Step]6000 [Batch]30000 [Speed]53.04ms/step [Loss]8.3076 [Metrics]{'train_loss:8.307595 lr:0.000530'}\n",
      "[Train Epoch]1/1 [Time]1617.64 [Step]6100 [Batch]30500 [Speed]53.04ms/step [Loss]8.3069 [Metrics]{'train_loss:8.306892 lr:0.000539'}\n",
      "[Train Epoch]1/1 [Time]1644.11 [Step]6200 [Batch]31000 [Speed]53.04ms/step [Loss]8.3062 [Metrics]{'train_loss:8.306165 lr:0.000548'}\n",
      "[Train Epoch]1/1 [Time]1670.30 [Step]6300 [Batch]31500 [Speed]53.03ms/step [Loss]8.3057 [Metrics]{'train_loss:8.305689 lr:0.000557'}\n",
      "[Train Epoch]1/1 [Time]1696.56 [Step]6400 [Batch]32000 [Speed]53.02ms/step [Loss]8.3056 [Metrics]{'train_loss:8.305576 lr:0.000566'}\n",
      "[Train Epoch]1/1 [Time]1722.83 [Step]6500 [Batch]32500 [Speed]53.01ms/step [Loss]8.3052 [Metrics]{'train_loss:8.305243 lr:0.000575'}\n",
      "[Train Epoch]1/1 [Time]1749.20 [Step]6600 [Batch]33000 [Speed]53.01ms/step [Loss]8.3037 [Metrics]{'train_loss:8.303698 lr:0.000583'}\n",
      "[Train Epoch]1/1 [Time]1775.54 [Step]6700 [Batch]33500 [Speed]53.00ms/step [Loss]8.3028 [Metrics]{'train_loss:8.302803 lr:0.000592'}\n",
      "[Train Epoch]1/1 [Time]1801.84 [Step]6800 [Batch]34000 [Speed]53.00ms/step [Loss]8.3025 [Metrics]{'train_loss:8.302496 lr:0.000601'}\n",
      "[Train Epoch]1/1 [Time]1828.01 [Step]6900 [Batch]34500 [Speed]52.99ms/step [Loss]8.3015 [Metrics]{'train_loss:8.301518 lr:0.000610'}\n",
      "[Train Epoch]1/1 [Time]1854.18 [Step]7000 [Batch]35000 [Speed]52.98ms/step [Loss]8.3010 [Metrics]{'train_loss:8.301044 lr:0.000619'}\n",
      "[Train Epoch]1/1 [Time]1880.33 [Step]7100 [Batch]35500 [Speed]52.97ms/step [Loss]8.3008 [Metrics]{'train_loss:8.300845 lr:0.000628'}\n",
      "[Train Epoch]1/1 [Time]1906.54 [Step]7200 [Batch]36000 [Speed]52.96ms/step [Loss]8.2997 [Metrics]{'train_loss:8.299675 lr:0.000636'}\n",
      "[Train Epoch]1/1 [Time]1932.74 [Step]7300 [Batch]36500 [Speed]52.95ms/step [Loss]8.2984 [Metrics]{'train_loss:8.298399 lr:0.000645'}\n",
      "[Train Epoch]1/1 [Time]1959.07 [Step]7400 [Batch]37000 [Speed]52.95ms/step [Loss]8.2978 [Metrics]{'train_loss:8.297769 lr:0.000654'}\n",
      "[Train Epoch]1/1 [Time]1985.21 [Step]7500 [Batch]37500 [Speed]52.94ms/step [Loss]8.2976 [Metrics]{'train_loss:8.297639 lr:0.000663'}\n",
      "[Train Epoch]1/1 [Time]2011.59 [Step]7600 [Batch]38000 [Speed]52.94ms/step [Loss]8.2977 [Metrics]{'train_loss:8.297654 lr:0.000672'}\n",
      "[Train Epoch]1/1 [Time]2038.04 [Step]7700 [Batch]38500 [Speed]52.94ms/step [Loss]8.2970 [Metrics]{'train_loss:8.297013 lr:0.000681'}\n",
      "[Train Epoch]1/1 [Time]2064.44 [Step]7800 [Batch]39000 [Speed]52.93ms/step [Loss]8.2965 [Metrics]{'train_loss:8.296538 lr:0.000689'}\n",
      "[Train Epoch]1/1 [Time]2090.70 [Step]7900 [Batch]39500 [Speed]52.93ms/step [Loss]8.2962 [Metrics]{'train_loss:8.296221 lr:0.000698'}\n",
      "[Train Epoch]1/1 [Time]2117.05 [Step]8000 [Batch]40000 [Speed]52.93ms/step [Loss]8.2961 [Metrics]{'train_loss:8.296102 lr:0.000707'}\n",
      "[Train Epoch]1/1 [Time]2143.20 [Step]8100 [Batch]40500 [Speed]52.92ms/step [Loss]8.2958 [Metrics]{'train_loss:8.295791 lr:0.000716'}\n",
      "[Train Epoch]1/1 [Time]2169.48 [Step]8200 [Batch]41000 [Speed]52.91ms/step [Loss]8.2951 [Metrics]{'train_loss:8.295106 lr:0.000725'}\n",
      "[Train Epoch]1/1 [Time]2195.72 [Step]8300 [Batch]41500 [Speed]52.91ms/step [Loss]8.2949 [Metrics]{'train_loss:8.294901 lr:0.000734'}\n",
      "[Train Epoch]1/1 [Time]2221.83 [Step]8400 [Batch]42000 [Speed]52.90ms/step [Loss]8.2947 [Metrics]{'train_loss:8.294708 lr:0.000742'}\n",
      "[Train Epoch]1/1 [Time]2248.04 [Step]8500 [Batch]42500 [Speed]52.90ms/step [Loss]8.2943 [Metrics]{'train_loss:8.294274 lr:0.000751'}\n",
      "[Train Epoch]1/1 [Time]2274.36 [Step]8600 [Batch]43000 [Speed]52.89ms/step [Loss]8.2931 [Metrics]{'train_loss:8.293091 lr:0.000760'}\n",
      "[Train Epoch]1/1 [Time]2300.56 [Step]8700 [Batch]43500 [Speed]52.89ms/step [Loss]8.2923 [Metrics]{'train_loss:8.292302 lr:0.000769'}\n",
      "[Train Epoch]1/1 [Time]2326.70 [Step]8800 [Batch]44000 [Speed]52.88ms/step [Loss]8.2918 [Metrics]{'train_loss:8.291848 lr:0.000778'}\n",
      "[Train Epoch]1/1 [Time]2352.88 [Step]8900 [Batch]44500 [Speed]52.87ms/step [Loss]8.2912 [Metrics]{'train_loss:8.291174 lr:0.000787'}\n",
      "[Train Epoch]1/1 [Time]2379.00 [Step]9000 [Batch]45000 [Speed]52.87ms/step [Loss]8.2904 [Metrics]{'train_loss:8.290361 lr:0.000795'}\n",
      "[Train Epoch]1/1 [Time]2405.22 [Step]9100 [Batch]45500 [Speed]52.86ms/step [Loss]8.2893 [Metrics]{'train_loss:8.289318 lr:0.000804'}\n",
      "[Train Epoch]1/1 [Time]2431.64 [Step]9200 [Batch]46000 [Speed]52.86ms/step [Loss]8.2884 [Metrics]{'train_loss:8.288415 lr:0.000813'}\n",
      "[Train Epoch]1/1 [Time]2458.05 [Step]9300 [Batch]46500 [Speed]52.86ms/step [Loss]8.2878 [Metrics]{'train_loss:8.287814 lr:0.000822'}\n",
      "[Train Epoch]1/1 [Time]2484.46 [Step]9400 [Batch]47000 [Speed]52.86ms/step [Loss]8.2868 [Metrics]{'train_loss:8.286846 lr:0.000831'}\n",
      "[Train Epoch]1/1 [Time]2509.44 [Step]9500 [Batch]47500 [Speed]52.83ms/step [Loss]8.2861 [Metrics]{'train_loss:8.286085 lr:0.000840'}\n",
      "[Train Epoch]1/1 [Time]2533.54 [Step]9600 [Batch]48000 [Speed]52.78ms/step [Loss]8.2859 [Metrics]{'train_loss:8.285925 lr:0.000849'}\n",
      "[Train Epoch]1/1 [Time]2557.70 [Step]9700 [Batch]48500 [Speed]52.74ms/step [Loss]8.2853 [Metrics]{'train_loss:8.285253 lr:0.000857'}\n",
      "[Train Epoch]1/1 [Time]2581.82 [Step]9800 [Batch]49000 [Speed]52.69ms/step [Loss]8.2846 [Metrics]{'train_loss:8.284643 lr:0.000866'}\n",
      "[Train Epoch]1/1 [Time]2605.96 [Step]9900 [Batch]49500 [Speed]52.65ms/step [Loss]8.2842 [Metrics]{'train_loss:8.284205 lr:0.000875'}\n",
      "Saving checkpoint for epoch 1 at step 50000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]2634.15 [Step]10000 [Batch]50000 [Speed]52.68ms/step [Loss]8.2837 [Metrics]{'train_loss:8.283701 lr:0.000884'}\n",
      "[Train Epoch]1/1 [Time]2658.32 [Step]10100 [Batch]50500 [Speed]52.64ms/step [Loss]8.2832 [Metrics]{'train_loss:8.283170 lr:0.000879'}\n",
      "[Train Epoch]1/1 [Time]2682.43 [Step]10200 [Batch]51000 [Speed]52.60ms/step [Loss]8.2823 [Metrics]{'train_loss:8.282284 lr:0.000875'}\n",
      "[Train Epoch]1/1 [Time]2706.55 [Step]10300 [Batch]51500 [Speed]52.55ms/step [Loss]8.2816 [Metrics]{'train_loss:8.281632 lr:0.000871'}\n",
      "[Train Epoch]1/1 [Time]2730.69 [Step]10400 [Batch]52000 [Speed]52.51ms/step [Loss]8.2809 [Metrics]{'train_loss:8.280907 lr:0.000867'}\n",
      "[Train Epoch]1/1 [Time]2754.83 [Step]10500 [Batch]52500 [Speed]52.47ms/step [Loss]8.2806 [Metrics]{'train_loss:8.280607 lr:0.000863'}\n",
      "[Train Epoch]1/1 [Time]2778.94 [Step]10600 [Batch]53000 [Speed]52.43ms/step [Loss]8.2800 [Metrics]{'train_loss:8.280006 lr:0.000859'}\n",
      "[Train Epoch]1/1 [Time]2803.10 [Step]10700 [Batch]53500 [Speed]52.39ms/step [Loss]8.2794 [Metrics]{'train_loss:8.279354 lr:0.000854'}\n",
      "[Train Epoch]1/1 [Time]2827.17 [Step]10800 [Batch]54000 [Speed]52.35ms/step [Loss]8.2790 [Metrics]{'train_loss:8.279012 lr:0.000851'}\n",
      "[Train Epoch]1/1 [Time]2851.29 [Step]10900 [Batch]54500 [Speed]52.32ms/step [Loss]8.2781 [Metrics]{'train_loss:8.278094 lr:0.000847'}\n",
      "[Train Epoch]1/1 [Time]2875.43 [Step]11000 [Batch]55000 [Speed]52.28ms/step [Loss]8.2773 [Metrics]{'train_loss:8.277348 lr:0.000843'}\n",
      "[Train Epoch]1/1 [Time]2899.60 [Step]11100 [Batch]55500 [Speed]52.25ms/step [Loss]8.2766 [Metrics]{'train_loss:8.276621 lr:0.000839'}\n",
      "[Train Epoch]1/1 [Time]2923.71 [Step]11200 [Batch]56000 [Speed]52.21ms/step [Loss]8.2759 [Metrics]{'train_loss:8.275881 lr:0.000835'}\n",
      "[Train Epoch]1/1 [Time]2947.80 [Step]11300 [Batch]56500 [Speed]52.17ms/step [Loss]8.2750 [Metrics]{'train_loss:8.274981 lr:0.000831'}\n",
      "[Train Epoch]1/1 [Time]2971.92 [Step]11400 [Batch]57000 [Speed]52.14ms/step [Loss]8.2744 [Metrics]{'train_loss:8.274393 lr:0.000828'}\n",
      "[Train Epoch]1/1 [Time]2996.01 [Step]11500 [Batch]57500 [Speed]52.10ms/step [Loss]8.2737 [Metrics]{'train_loss:8.273677 lr:0.000824'}\n",
      "[Train Epoch]1/1 [Time]3020.17 [Step]11600 [Batch]58000 [Speed]52.07ms/step [Loss]8.2732 [Metrics]{'train_loss:8.273153 lr:0.000821'}\n",
      "[Train Epoch]1/1 [Time]3044.34 [Step]11700 [Batch]58500 [Speed]52.04ms/step [Loss]8.2727 [Metrics]{'train_loss:8.272733 lr:0.000817'}\n",
      "[Train Epoch]1/1 [Time]3068.41 [Step]11800 [Batch]59000 [Speed]52.01ms/step [Loss]8.2724 [Metrics]{'train_loss:8.272408 lr:0.000814'}\n",
      "[Train Epoch]1/1 [Time]3092.62 [Step]11900 [Batch]59500 [Speed]51.98ms/step [Loss]8.2725 [Metrics]{'train_loss:8.272536 lr:0.000810'}\n",
      "[Train Epoch]1/1 [Time]3116.74 [Step]12000 [Batch]60000 [Speed]51.95ms/step [Loss]8.2726 [Metrics]{'train_loss:8.272580 lr:0.000807'}\n",
      "[Train Epoch]1/1 [Time]3140.87 [Step]12100 [Batch]60500 [Speed]51.92ms/step [Loss]8.2724 [Metrics]{'train_loss:8.272394 lr:0.000804'}\n",
      "[Train Epoch]1/1 [Time]3164.98 [Step]12200 [Batch]61000 [Speed]51.88ms/step [Loss]8.2719 [Metrics]{'train_loss:8.271916 lr:0.000800'}\n",
      "[Train Epoch]1/1 [Time]3189.09 [Step]12300 [Batch]61500 [Speed]51.86ms/step [Loss]8.2711 [Metrics]{'train_loss:8.271062 lr:0.000797'}\n",
      "[Train Epoch]1/1 [Time]3213.20 [Step]12400 [Batch]62000 [Speed]51.83ms/step [Loss]8.2705 [Metrics]{'train_loss:8.270507 lr:0.000794'}\n",
      "[Train Epoch]1/1 [Time]3237.28 [Step]12500 [Batch]62500 [Speed]51.80ms/step [Loss]8.2703 [Metrics]{'train_loss:8.270269 lr:0.000791'}\n",
      "[Train Epoch]1/1 [Time]3261.40 [Step]12600 [Batch]63000 [Speed]51.77ms/step [Loss]8.2700 [Metrics]{'train_loss:8.269979 lr:0.000787'}\n",
      "[Train Epoch]1/1 [Time]3285.55 [Step]12700 [Batch]63500 [Speed]51.74ms/step [Loss]8.2696 [Metrics]{'train_loss:8.269574 lr:0.000784'}\n",
      "[Train Epoch]1/1 [Time]3309.67 [Step]12800 [Batch]64000 [Speed]51.71ms/step [Loss]8.2691 [Metrics]{'train_loss:8.269073 lr:0.000781'}\n",
      "[Train Epoch]1/1 [Time]3333.86 [Step]12900 [Batch]64500 [Speed]51.69ms/step [Loss]8.2685 [Metrics]{'train_loss:8.268520 lr:0.000778'}\n",
      "[Train Epoch]1/1 [Time]3358.02 [Step]13000 [Batch]65000 [Speed]51.66ms/step [Loss]8.2676 [Metrics]{'train_loss:8.267618 lr:0.000775'}\n",
      "[Train Epoch]1/1 [Time]3382.12 [Step]13100 [Batch]65500 [Speed]51.64ms/step [Loss]8.2670 [Metrics]{'train_loss:8.266996 lr:0.000772'}\n",
      "[Train Epoch]1/1 [Time]3406.22 [Step]13200 [Batch]66000 [Speed]51.61ms/step [Loss]8.2664 [Metrics]{'train_loss:8.266357 lr:0.000769'}\n",
      "[Train Epoch]1/1 [Time]3430.37 [Step]13300 [Batch]66500 [Speed]51.58ms/step [Loss]8.2656 [Metrics]{'train_loss:8.265561 lr:0.000766'}\n",
      "[Train Epoch]1/1 [Time]3454.51 [Step]13400 [Batch]67000 [Speed]51.56ms/step [Loss]8.2649 [Metrics]{'train_loss:8.264918 lr:0.000764'}\n",
      "[Train Epoch]1/1 [Time]3478.64 [Step]13500 [Batch]67500 [Speed]51.54ms/step [Loss]8.2640 [Metrics]{'train_loss:8.263994 lr:0.000761'}\n",
      "[Train Epoch]1/1 [Time]3502.76 [Step]13600 [Batch]68000 [Speed]51.51ms/step [Loss]8.2633 [Metrics]{'train_loss:8.263332 lr:0.000758'}\n",
      "[Train Epoch]1/1 [Time]3526.90 [Step]13700 [Batch]68500 [Speed]51.49ms/step [Loss]8.2629 [Metrics]{'train_loss:8.262938 lr:0.000755'}\n",
      "[Train Epoch]1/1 [Time]3551.03 [Step]13800 [Batch]69000 [Speed]51.46ms/step [Loss]8.2622 [Metrics]{'train_loss:8.262224 lr:0.000752'}\n",
      "[Train Epoch]1/1 [Time]3575.16 [Step]13900 [Batch]69500 [Speed]51.44ms/step [Loss]8.2616 [Metrics]{'train_loss:8.261558 lr:0.000750'}\n",
      "[Train Epoch]1/1 [Time]3599.25 [Step]14000 [Batch]70000 [Speed]51.42ms/step [Loss]8.2609 [Metrics]{'train_loss:8.260924 lr:0.000747'}\n",
      "[Train Epoch]1/1 [Time]3623.36 [Step]14100 [Batch]70500 [Speed]51.40ms/step [Loss]8.2600 [Metrics]{'train_loss:8.260006 lr:0.000744'}\n",
      "[Train Epoch]1/1 [Time]3647.45 [Step]14200 [Batch]71000 [Speed]51.37ms/step [Loss]8.2591 [Metrics]{'train_loss:8.259098 lr:0.000742'}\n",
      "[Train Epoch]1/1 [Time]3671.56 [Step]14300 [Batch]71500 [Speed]51.35ms/step [Loss]8.2582 [Metrics]{'train_loss:8.258230 lr:0.000739'}\n",
      "[Train Epoch]1/1 [Time]3695.68 [Step]14400 [Batch]72000 [Speed]51.33ms/step [Loss]8.2576 [Metrics]{'train_loss:8.257619 lr:0.000737'}\n",
      "[Train Epoch]1/1 [Time]3719.84 [Step]14500 [Batch]72500 [Speed]51.31ms/step [Loss]8.2569 [Metrics]{'train_loss:8.256942 lr:0.000734'}\n",
      "[Train Epoch]1/1 [Time]3743.97 [Step]14600 [Batch]73000 [Speed]51.29ms/step [Loss]8.2561 [Metrics]{'train_loss:8.256085 lr:0.000732'}\n",
      "[Train Epoch]1/1 [Time]3768.07 [Step]14700 [Batch]73500 [Speed]51.27ms/step [Loss]8.2557 [Metrics]{'train_loss:8.255684 lr:0.000729'}\n",
      "[Train Epoch]1/1 [Time]3792.23 [Step]14800 [Batch]74000 [Speed]51.25ms/step [Loss]8.2550 [Metrics]{'train_loss:8.254994 lr:0.000727'}\n",
      "[Train Epoch]1/1 [Time]3816.28 [Step]14900 [Batch]74500 [Speed]51.23ms/step [Loss]8.2548 [Metrics]{'train_loss:8.254809 lr:0.000724'}\n",
      "Saving checkpoint for epoch 1 at step 75000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]3844.59 [Step]15000 [Batch]75000 [Speed]51.26ms/step [Loss]8.2545 [Metrics]{'train_loss:8.254509 lr:0.000722'}\n",
      "[Train Epoch]1/1 [Time]3868.79 [Step]15100 [Batch]75500 [Speed]51.24ms/step [Loss]8.2543 [Metrics]{'train_loss:8.254306 lr:0.000719'}\n",
      "[Train Epoch]1/1 [Time]3892.88 [Step]15200 [Batch]76000 [Speed]51.22ms/step [Loss]8.2541 [Metrics]{'train_loss:8.254116 lr:0.000717'}\n",
      "[Train Epoch]1/1 [Time]3916.99 [Step]15300 [Batch]76500 [Speed]51.20ms/step [Loss]8.2538 [Metrics]{'train_loss:8.253804 lr:0.000715'}\n",
      "[Train Epoch]1/1 [Time]3941.09 [Step]15400 [Batch]77000 [Speed]51.18ms/step [Loss]8.2539 [Metrics]{'train_loss:8.253883 lr:0.000712'}\n",
      "[Train Epoch]1/1 [Time]3965.20 [Step]15500 [Batch]77500 [Speed]51.16ms/step [Loss]8.2541 [Metrics]{'train_loss:8.254053 lr:0.000710'}\n",
      "[Train Epoch]1/1 [Time]3989.27 [Step]15600 [Batch]78000 [Speed]51.14ms/step [Loss]8.2539 [Metrics]{'train_loss:8.253926 lr:0.000708'}\n",
      "[Train Epoch]1/1 [Time]4013.39 [Step]15700 [Batch]78500 [Speed]51.13ms/step [Loss]8.2539 [Metrics]{'train_loss:8.253888 lr:0.000705'}\n",
      "[Train Epoch]1/1 [Time]4037.50 [Step]15800 [Batch]79000 [Speed]51.11ms/step [Loss]8.2537 [Metrics]{'train_loss:8.253655 lr:0.000703'}\n",
      "[Train Epoch]1/1 [Time]4061.55 [Step]15900 [Batch]79500 [Speed]51.09ms/step [Loss]8.2534 [Metrics]{'train_loss:8.253414 lr:0.000701'}\n",
      "[Train Epoch]1/1 [Time]4085.65 [Step]16000 [Batch]80000 [Speed]51.07ms/step [Loss]8.2534 [Metrics]{'train_loss:8.253421 lr:0.000699'}\n",
      "[Train Epoch]1/1 [Time]4109.77 [Step]16100 [Batch]80500 [Speed]51.05ms/step [Loss]8.2539 [Metrics]{'train_loss:8.253903 lr:0.000697'}\n",
      "[Train Epoch]1/1 [Time]4133.92 [Step]16200 [Batch]81000 [Speed]51.04ms/step [Loss]8.2540 [Metrics]{'train_loss:8.253959 lr:0.000694'}\n",
      "[Train Epoch]1/1 [Time]4158.04 [Step]16300 [Batch]81500 [Speed]51.02ms/step [Loss]8.2535 [Metrics]{'train_loss:8.253460 lr:0.000692'}\n",
      "[Train Epoch]1/1 [Time]4182.16 [Step]16400 [Batch]82000 [Speed]51.00ms/step [Loss]8.2530 [Metrics]{'train_loss:8.252986 lr:0.000690'}\n",
      "[Train Epoch]1/1 [Time]4206.26 [Step]16500 [Batch]82500 [Speed]50.98ms/step [Loss]8.2528 [Metrics]{'train_loss:8.252777 lr:0.000688'}\n",
      "[Train Epoch]1/1 [Time]4230.31 [Step]16600 [Batch]83000 [Speed]50.97ms/step [Loss]8.2522 [Metrics]{'train_loss:8.252195 lr:0.000686'}\n",
      "[Train Epoch]1/1 [Time]4254.44 [Step]16700 [Batch]83500 [Speed]50.95ms/step [Loss]8.2520 [Metrics]{'train_loss:8.251978 lr:0.000684'}\n",
      "[Train Epoch]1/1 [Time]4278.58 [Step]16800 [Batch]84000 [Speed]50.94ms/step [Loss]8.2515 [Metrics]{'train_loss:8.251528 lr:0.000682'}\n",
      "[Train Epoch]1/1 [Time]4302.70 [Step]16900 [Batch]84500 [Speed]50.92ms/step [Loss]8.2510 [Metrics]{'train_loss:8.251019 lr:0.000680'}\n",
      "[Train Epoch]1/1 [Time]4326.83 [Step]17000 [Batch]85000 [Speed]50.90ms/step [Loss]8.2505 [Metrics]{'train_loss:8.250524 lr:0.000678'}\n",
      "[Train Epoch]1/1 [Time]4350.99 [Step]17100 [Batch]85500 [Speed]50.89ms/step [Loss]8.2501 [Metrics]{'train_loss:8.250080 lr:0.000676'}\n",
      "[Train Epoch]1/1 [Time]4375.14 [Step]17200 [Batch]86000 [Speed]50.87ms/step [Loss]8.2496 [Metrics]{'train_loss:8.249629 lr:0.000674'}\n",
      "[Train Epoch]1/1 [Time]4399.28 [Step]17300 [Batch]86500 [Speed]50.86ms/step [Loss]8.2496 [Metrics]{'train_loss:8.249552 lr:0.000672'}\n",
      "[Train Epoch]1/1 [Time]4423.38 [Step]17400 [Batch]87000 [Speed]50.84ms/step [Loss]8.2495 [Metrics]{'train_loss:8.249507 lr:0.000670'}\n",
      "[Train Epoch]1/1 [Time]4447.50 [Step]17500 [Batch]87500 [Speed]50.83ms/step [Loss]8.2495 [Metrics]{'train_loss:8.249500 lr:0.000668'}\n",
      "[Train Epoch]1/1 [Time]4471.62 [Step]17600 [Batch]88000 [Speed]50.81ms/step [Loss]8.2491 [Metrics]{'train_loss:8.249122 lr:0.000666'}\n",
      "[Train Epoch]1/1 [Time]4495.74 [Step]17700 [Batch]88500 [Speed]50.80ms/step [Loss]8.2486 [Metrics]{'train_loss:8.248630 lr:0.000664'}\n",
      "[Train Epoch]1/1 [Time]4519.85 [Step]17800 [Batch]89000 [Speed]50.78ms/step [Loss]8.2479 [Metrics]{'train_loss:8.247928 lr:0.000662'}\n",
      "[Train Epoch]1/1 [Time]4543.97 [Step]17900 [Batch]89500 [Speed]50.77ms/step [Loss]8.2474 [Metrics]{'train_loss:8.247398 lr:0.000661'}\n",
      "[Train Epoch]1/1 [Time]4568.08 [Step]18000 [Batch]90000 [Speed]50.76ms/step [Loss]8.2469 [Metrics]{'train_loss:8.246908 lr:0.000659'}\n",
      "[Train Epoch]1/1 [Time]4592.17 [Step]18100 [Batch]90500 [Speed]50.74ms/step [Loss]8.2465 [Metrics]{'train_loss:8.246469 lr:0.000657'}\n",
      "[Train Epoch]1/1 [Time]4616.31 [Step]18200 [Batch]91000 [Speed]50.73ms/step [Loss]8.2464 [Metrics]{'train_loss:8.246389 lr:0.000655'}\n",
      "[Train Epoch]1/1 [Time]4640.42 [Step]18300 [Batch]91500 [Speed]50.71ms/step [Loss]8.2461 [Metrics]{'train_loss:8.246056 lr:0.000653'}\n",
      "[Train Epoch]1/1 [Time]4664.58 [Step]18400 [Batch]92000 [Speed]50.70ms/step [Loss]8.2460 [Metrics]{'train_loss:8.245967 lr:0.000652'}\n",
      "[Train Epoch]1/1 [Time]4688.69 [Step]18500 [Batch]92500 [Speed]50.69ms/step [Loss]8.2457 [Metrics]{'train_loss:8.245723 lr:0.000650'}\n",
      "[Train Epoch]1/1 [Time]4712.82 [Step]18600 [Batch]93000 [Speed]50.68ms/step [Loss]8.2454 [Metrics]{'train_loss:8.245407 lr:0.000648'}\n",
      "[Train Epoch]1/1 [Time]4736.93 [Step]18700 [Batch]93500 [Speed]50.66ms/step [Loss]8.2450 [Metrics]{'train_loss:8.244950 lr:0.000646'}\n",
      "[Train Epoch]1/1 [Time]4761.07 [Step]18800 [Batch]94000 [Speed]50.65ms/step [Loss]8.2449 [Metrics]{'train_loss:8.244901 lr:0.000645'}\n",
      "[Train Epoch]1/1 [Time]4785.16 [Step]18900 [Batch]94500 [Speed]50.64ms/step [Loss]8.2446 [Metrics]{'train_loss:8.244575 lr:0.000643'}\n",
      "[Train Epoch]1/1 [Time]4809.26 [Step]19000 [Batch]95000 [Speed]50.62ms/step [Loss]8.2441 [Metrics]{'train_loss:8.244133 lr:0.000641'}\n",
      "[Train Epoch]1/1 [Time]4833.31 [Step]19100 [Batch]95500 [Speed]50.61ms/step [Loss]8.2440 [Metrics]{'train_loss:8.244011 lr:0.000640'}\n",
      "[Train Epoch]1/1 [Time]4857.39 [Step]19200 [Batch]96000 [Speed]50.60ms/step [Loss]8.2438 [Metrics]{'train_loss:8.243771 lr:0.000638'}\n",
      "[Train Epoch]1/1 [Time]4881.53 [Step]19300 [Batch]96500 [Speed]50.59ms/step [Loss]8.2435 [Metrics]{'train_loss:8.243460 lr:0.000636'}\n",
      "[Train Epoch]1/1 [Time]4905.61 [Step]19400 [Batch]97000 [Speed]50.57ms/step [Loss]8.2432 [Metrics]{'train_loss:8.243162 lr:0.000635'}\n",
      "[Train Epoch]1/1 [Time]4929.75 [Step]19500 [Batch]97500 [Speed]50.56ms/step [Loss]8.2431 [Metrics]{'train_loss:8.243142 lr:0.000633'}\n",
      "[Train Epoch]1/1 [Time]4953.91 [Step]19600 [Batch]98000 [Speed]50.55ms/step [Loss]8.2433 [Metrics]{'train_loss:8.243304 lr:0.000631'}\n",
      "[Train Epoch]1/1 [Time]4978.02 [Step]19700 [Batch]98500 [Speed]50.54ms/step [Loss]8.2433 [Metrics]{'train_loss:8.243256 lr:0.000630'}\n",
      "[Train Epoch]1/1 [Time]5002.12 [Step]19800 [Batch]99000 [Speed]50.53ms/step [Loss]8.2426 [Metrics]{'train_loss:8.242648 lr:0.000628'}\n",
      "[Train Epoch]1/1 [Time]5026.21 [Step]19900 [Batch]99500 [Speed]50.51ms/step [Loss]8.2426 [Metrics]{'train_loss:8.242590 lr:0.000627'}\n",
      "Saving checkpoint for epoch 1 at step 100000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]5054.39 [Step]20000 [Batch]100000 [Speed]50.54ms/step [Loss]8.2421 [Metrics]{'train_loss:8.242074 lr:0.000625'}\n",
      "[Train Epoch]1/1 [Time]5078.55 [Step]20100 [Batch]100500 [Speed]50.53ms/step [Loss]8.2419 [Metrics]{'train_loss:8.241852 lr:0.000623'}\n",
      "[Train Epoch]1/1 [Time]5102.65 [Step]20200 [Batch]101000 [Speed]50.52ms/step [Loss]8.2414 [Metrics]{'train_loss:8.241391 lr:0.000622'}\n",
      "[Train Epoch]1/1 [Time]5126.75 [Step]20300 [Batch]101500 [Speed]50.51ms/step [Loss]8.2411 [Metrics]{'train_loss:8.241082 lr:0.000620'}\n",
      "[Train Epoch]1/1 [Time]5150.88 [Step]20400 [Batch]102000 [Speed]50.50ms/step [Loss]8.2410 [Metrics]{'train_loss:8.241019 lr:0.000619'}\n",
      "[Train Epoch]1/1 [Time]5174.96 [Step]20500 [Batch]102500 [Speed]50.49ms/step [Loss]8.2407 [Metrics]{'train_loss:8.240701 lr:0.000617'}\n",
      "[Train Epoch]1/1 [Time]5199.09 [Step]20600 [Batch]103000 [Speed]50.48ms/step [Loss]8.2407 [Metrics]{'train_loss:8.240652 lr:0.000616'}\n",
      "[Train Epoch]1/1 [Time]5223.22 [Step]20700 [Batch]103500 [Speed]50.47ms/step [Loss]8.2404 [Metrics]{'train_loss:8.240407 lr:0.000614'}\n",
      "[Train Epoch]1/1 [Time]5247.32 [Step]20800 [Batch]104000 [Speed]50.46ms/step [Loss]8.2403 [Metrics]{'train_loss:8.240258 lr:0.000613'}\n",
      "[Train Epoch]1/1 [Time]5271.43 [Step]20900 [Batch]104500 [Speed]50.44ms/step [Loss]8.2402 [Metrics]{'train_loss:8.240202 lr:0.000611'}\n",
      "[Train Epoch]1/1 [Time]5295.56 [Step]21000 [Batch]105000 [Speed]50.43ms/step [Loss]8.2400 [Metrics]{'train_loss:8.239959 lr:0.000610'}\n",
      "[Train Epoch]1/1 [Time]5319.70 [Step]21100 [Batch]105500 [Speed]50.42ms/step [Loss]8.2398 [Metrics]{'train_loss:8.239801 lr:0.000608'}\n",
      "[Train Epoch]1/1 [Time]5343.84 [Step]21200 [Batch]106000 [Speed]50.41ms/step [Loss]8.2395 [Metrics]{'train_loss:8.239516 lr:0.000607'}\n",
      "[Train Epoch]1/1 [Time]5367.96 [Step]21300 [Batch]106500 [Speed]50.40ms/step [Loss]8.2393 [Metrics]{'train_loss:8.239257 lr:0.000606'}\n",
      "[Train Epoch]1/1 [Time]5392.11 [Step]21400 [Batch]107000 [Speed]50.39ms/step [Loss]8.2390 [Metrics]{'train_loss:8.238992 lr:0.000604'}\n",
      "[Train Epoch]1/1 [Time]5416.24 [Step]21500 [Batch]107500 [Speed]50.38ms/step [Loss]8.2387 [Metrics]{'train_loss:8.238732 lr:0.000603'}\n",
      "[Train Epoch]1/1 [Time]5440.37 [Step]21600 [Batch]108000 [Speed]50.37ms/step [Loss]8.2386 [Metrics]{'train_loss:8.238590 lr:0.000601'}\n",
      "[Train Epoch]1/1 [Time]5464.48 [Step]21700 [Batch]108500 [Speed]50.36ms/step [Loss]8.2386 [Metrics]{'train_loss:8.238587 lr:0.000600'}\n",
      "[Train Epoch]1/1 [Time]5488.58 [Step]21800 [Batch]109000 [Speed]50.35ms/step [Loss]8.2384 [Metrics]{'train_loss:8.238403 lr:0.000599'}\n",
      "[Train Epoch]1/1 [Time]5512.70 [Step]21900 [Batch]109500 [Speed]50.34ms/step [Loss]8.2383 [Metrics]{'train_loss:8.238255 lr:0.000597'}\n",
      "[Train Epoch]1/1 [Time]5536.81 [Step]22000 [Batch]110000 [Speed]50.33ms/step [Loss]8.2380 [Metrics]{'train_loss:8.238050 lr:0.000596'}\n",
      "[Train Epoch]1/1 [Time]5560.90 [Step]22100 [Batch]110500 [Speed]50.32ms/step [Loss]8.2381 [Metrics]{'train_loss:8.238108 lr:0.000595'}\n",
      "[Train Epoch]1/1 [Time]5585.01 [Step]22200 [Batch]111000 [Speed]50.32ms/step [Loss]8.2381 [Metrics]{'train_loss:8.238075 lr:0.000593'}\n",
      "[Train Epoch]1/1 [Time]5609.17 [Step]22300 [Batch]111500 [Speed]50.31ms/step [Loss]8.2377 [Metrics]{'train_loss:8.237745 lr:0.000592'}\n",
      "[Train Epoch]1/1 [Time]5633.26 [Step]22400 [Batch]112000 [Speed]50.30ms/step [Loss]8.2374 [Metrics]{'train_loss:8.237370 lr:0.000591'}\n",
      "[Train Epoch]1/1 [Time]5657.37 [Step]22500 [Batch]112500 [Speed]50.29ms/step [Loss]8.2370 [Metrics]{'train_loss:8.237035 lr:0.000589'}\n",
      "[Train Epoch]1/1 [Time]5681.46 [Step]22600 [Batch]113000 [Speed]50.28ms/step [Loss]8.2369 [Metrics]{'train_loss:8.236852 lr:0.000588'}\n",
      "[Train Epoch]1/1 [Time]5705.58 [Step]22700 [Batch]113500 [Speed]50.27ms/step [Loss]8.2366 [Metrics]{'train_loss:8.236606 lr:0.000587'}\n",
      "[Train Epoch]1/1 [Time]5729.65 [Step]22800 [Batch]114000 [Speed]50.26ms/step [Loss]8.2363 [Metrics]{'train_loss:8.236320 lr:0.000585'}\n",
      "[Train Epoch]1/1 [Time]5753.76 [Step]22900 [Batch]114500 [Speed]50.25ms/step [Loss]8.2360 [Metrics]{'train_loss:8.235979 lr:0.000584'}\n",
      "[Train Epoch]1/1 [Time]5777.90 [Step]23000 [Batch]115000 [Speed]50.24ms/step [Loss]8.2360 [Metrics]{'train_loss:8.235971 lr:0.000583'}\n",
      "[Train Epoch]1/1 [Time]5801.99 [Step]23100 [Batch]115500 [Speed]50.23ms/step [Loss]8.2357 [Metrics]{'train_loss:8.235674 lr:0.000582'}\n",
      "[Train Epoch]1/1 [Time]5826.11 [Step]23200 [Batch]116000 [Speed]50.23ms/step [Loss]8.2354 [Metrics]{'train_loss:8.235434 lr:0.000580'}\n",
      "[Train Epoch]1/1 [Time]5850.25 [Step]23300 [Batch]116500 [Speed]50.22ms/step [Loss]8.2353 [Metrics]{'train_loss:8.235338 lr:0.000579'}\n",
      "[Train Epoch]1/1 [Time]5874.39 [Step]23400 [Batch]117000 [Speed]50.21ms/step [Loss]8.2351 [Metrics]{'train_loss:8.235123 lr:0.000578'}\n",
      "[Train Epoch]1/1 [Time]5898.51 [Step]23500 [Batch]117500 [Speed]50.20ms/step [Loss]8.2350 [Metrics]{'train_loss:8.234983 lr:0.000577'}\n",
      "[Train Epoch]1/1 [Time]5922.60 [Step]23600 [Batch]118000 [Speed]50.19ms/step [Loss]8.2350 [Metrics]{'train_loss:8.235041 lr:0.000575'}\n",
      "[Train Epoch]1/1 [Time]5946.73 [Step]23700 [Batch]118500 [Speed]50.18ms/step [Loss]8.2349 [Metrics]{'train_loss:8.234927 lr:0.000574'}\n",
      "[Train Epoch]1/1 [Time]5970.85 [Step]23800 [Batch]119000 [Speed]50.18ms/step [Loss]8.2346 [Metrics]{'train_loss:8.234561 lr:0.000573'}\n",
      "[Train Epoch]1/1 [Time]5994.95 [Step]23900 [Batch]119500 [Speed]50.17ms/step [Loss]8.2344 [Metrics]{'train_loss:8.234409 lr:0.000572'}\n",
      "[Train Epoch]1/1 [Time]6019.11 [Step]24000 [Batch]120000 [Speed]50.16ms/step [Loss]8.2344 [Metrics]{'train_loss:8.234353 lr:0.000571'}\n",
      "[Train Epoch]1/1 [Time]6043.26 [Step]24100 [Batch]120500 [Speed]50.15ms/step [Loss]8.2343 [Metrics]{'train_loss:8.234252 lr:0.000569'}\n",
      "[Train Epoch]1/1 [Time]6069.90 [Step]24200 [Batch]121000 [Speed]50.16ms/step [Loss]8.2343 [Metrics]{'train_loss:8.234282 lr:0.000568'}\n",
      "[Train Epoch]1/1 [Time]6096.07 [Step]24300 [Batch]121500 [Speed]50.17ms/step [Loss]8.2340 [Metrics]{'train_loss:8.234036 lr:0.000567'}\n",
      "[Train Epoch]1/1 [Time]6121.05 [Step]24400 [Batch]122000 [Speed]50.17ms/step [Loss]8.2338 [Metrics]{'train_loss:8.233788 lr:0.000566'}\n",
      "[Train Epoch]1/1 [Time]6145.13 [Step]24500 [Batch]122500 [Speed]50.16ms/step [Loss]8.2336 [Metrics]{'train_loss:8.233614 lr:0.000565'}\n",
      "[Train Epoch]1/1 [Time]6169.24 [Step]24600 [Batch]123000 [Speed]50.16ms/step [Loss]8.2338 [Metrics]{'train_loss:8.233788 lr:0.000564'}\n",
      "[Train Epoch]1/1 [Time]6193.38 [Step]24700 [Batch]123500 [Speed]50.15ms/step [Loss]8.2337 [Metrics]{'train_loss:8.233729 lr:0.000562'}\n",
      "[Train Epoch]1/1 [Time]6217.50 [Step]24800 [Batch]124000 [Speed]50.14ms/step [Loss]8.2339 [Metrics]{'train_loss:8.233889 lr:0.000561'}\n",
      "[Train Epoch]1/1 [Time]6241.64 [Step]24900 [Batch]124500 [Speed]50.13ms/step [Loss]8.2337 [Metrics]{'train_loss:8.233734 lr:0.000560'}\n",
      "Saving checkpoint for epoch 1 at step 125000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]6269.81 [Step]25000 [Batch]125000 [Speed]50.16ms/step [Loss]8.2336 [Metrics]{'train_loss:8.233624 lr:0.000559'}\n",
      "[Train Epoch]1/1 [Time]6294.05 [Step]25100 [Batch]125500 [Speed]50.15ms/step [Loss]8.2334 [Metrics]{'train_loss:8.233417 lr:0.000558'}\n",
      "[Train Epoch]1/1 [Time]6318.25 [Step]25200 [Batch]126000 [Speed]50.14ms/step [Loss]8.2332 [Metrics]{'train_loss:8.233191 lr:0.000557'}\n",
      "[Train Epoch]1/1 [Time]6342.41 [Step]25300 [Batch]126500 [Speed]50.14ms/step [Loss]8.2330 [Metrics]{'train_loss:8.233027 lr:0.000556'}\n",
      "[Train Epoch]1/1 [Time]6366.54 [Step]25400 [Batch]127000 [Speed]50.13ms/step [Loss]8.2329 [Metrics]{'train_loss:8.232934 lr:0.000555'}\n",
      "[Train Epoch]1/1 [Time]6390.67 [Step]25500 [Batch]127500 [Speed]50.12ms/step [Loss]8.2328 [Metrics]{'train_loss:8.232826 lr:0.000554'}\n",
      "[Train Epoch]1/1 [Time]6414.81 [Step]25600 [Batch]128000 [Speed]50.12ms/step [Loss]8.2327 [Metrics]{'train_loss:8.232664 lr:0.000552'}\n",
      "[Train Epoch]1/1 [Time]6438.93 [Step]25700 [Batch]128500 [Speed]50.11ms/step [Loss]8.2326 [Metrics]{'train_loss:8.232588 lr:0.000551'}\n",
      "[Train Epoch]1/1 [Time]6463.06 [Step]25800 [Batch]129000 [Speed]50.10ms/step [Loss]8.2326 [Metrics]{'train_loss:8.232575 lr:0.000550'}\n",
      "[Train Epoch]1/1 [Time]6487.24 [Step]25900 [Batch]129500 [Speed]50.09ms/step [Loss]8.2324 [Metrics]{'train_loss:8.232387 lr:0.000549'}\n",
      "[Train Epoch]1/1 [Time]6511.34 [Step]26000 [Batch]130000 [Speed]50.09ms/step [Loss]8.2321 [Metrics]{'train_loss:8.232088 lr:0.000548'}\n",
      "[Train Epoch]1/1 [Time]6535.45 [Step]26100 [Batch]130500 [Speed]50.08ms/step [Loss]8.2319 [Metrics]{'train_loss:8.231882 lr:0.000547'}\n",
      "[Train Epoch]1/1 [Time]6559.58 [Step]26200 [Batch]131000 [Speed]50.07ms/step [Loss]8.2320 [Metrics]{'train_loss:8.231995 lr:0.000546'}\n",
      "[Train Epoch]1/1 [Time]6583.72 [Step]26300 [Batch]131500 [Speed]50.07ms/step [Loss]8.2317 [Metrics]{'train_loss:8.231744 lr:0.000545'}\n",
      "[Train Epoch]1/1 [Time]6607.86 [Step]26400 [Batch]132000 [Speed]50.06ms/step [Loss]8.2316 [Metrics]{'train_loss:8.231589 lr:0.000544'}\n",
      "[Train Epoch]1/1 [Time]6631.99 [Step]26500 [Batch]132500 [Speed]50.05ms/step [Loss]8.2316 [Metrics]{'train_loss:8.231565 lr:0.000543'}\n",
      "[Train Epoch]1/1 [Time]6656.11 [Step]26600 [Batch]133000 [Speed]50.05ms/step [Loss]8.2313 [Metrics]{'train_loss:8.231302 lr:0.000542'}\n",
      "[Train Epoch]1/1 [Time]6680.21 [Step]26700 [Batch]133500 [Speed]50.04ms/step [Loss]8.2311 [Metrics]{'train_loss:8.231116 lr:0.000541'}\n",
      "[Train Epoch]1/1 [Time]6704.31 [Step]26800 [Batch]134000 [Speed]50.03ms/step [Loss]8.2309 [Metrics]{'train_loss:8.230867 lr:0.000540'}\n",
      "[Train Epoch]1/1 [Time]6728.42 [Step]26900 [Batch]134500 [Speed]50.03ms/step [Loss]8.2307 [Metrics]{'train_loss:8.230721 lr:0.000539'}\n",
      "[Train Epoch]1/1 [Time]6752.53 [Step]27000 [Batch]135000 [Speed]50.02ms/step [Loss]8.2307 [Metrics]{'train_loss:8.230687 lr:0.000538'}\n",
      "[Train Epoch]1/1 [Time]6776.66 [Step]27100 [Batch]135500 [Speed]50.01ms/step [Loss]8.2307 [Metrics]{'train_loss:8.230659 lr:0.000537'}\n",
      "[Train Epoch]1/1 [Time]6800.78 [Step]27200 [Batch]136000 [Speed]50.01ms/step [Loss]8.2307 [Metrics]{'train_loss:8.230670 lr:0.000536'}\n",
      "[Train Epoch]1/1 [Time]6824.90 [Step]27300 [Batch]136500 [Speed]50.00ms/step [Loss]8.2304 [Metrics]{'train_loss:8.230412 lr:0.000535'}\n",
      "[Train Epoch]1/1 [Time]6849.04 [Step]27400 [Batch]137000 [Speed]49.99ms/step [Loss]8.2301 [Metrics]{'train_loss:8.230090 lr:0.000534'}\n",
      "[Train Epoch]1/1 [Time]6873.14 [Step]27500 [Batch]137500 [Speed]49.99ms/step [Loss]8.2300 [Metrics]{'train_loss:8.229969 lr:0.000533'}\n",
      "[Train Epoch]1/1 [Time]6897.32 [Step]27600 [Batch]138000 [Speed]49.98ms/step [Loss]8.2298 [Metrics]{'train_loss:8.229775 lr:0.000532'}\n",
      "[Train Epoch]1/1 [Time]6921.42 [Step]27700 [Batch]138500 [Speed]49.97ms/step [Loss]8.2296 [Metrics]{'train_loss:8.229630 lr:0.000531'}\n",
      "[Train Epoch]1/1 [Time]6945.59 [Step]27800 [Batch]139000 [Speed]49.97ms/step [Loss]8.2296 [Metrics]{'train_loss:8.229613 lr:0.000530'}\n",
      "[Train Epoch]1/1 [Time]6969.67 [Step]27900 [Batch]139500 [Speed]49.96ms/step [Loss]8.2296 [Metrics]{'train_loss:8.229566 lr:0.000529'}\n",
      "[Train Epoch]1/1 [Time]6993.75 [Step]28000 [Batch]140000 [Speed]49.96ms/step [Loss]8.2294 [Metrics]{'train_loss:8.229356 lr:0.000528'}\n",
      "[Train Epoch]1/1 [Time]7017.90 [Step]28100 [Batch]140500 [Speed]49.95ms/step [Loss]8.2293 [Metrics]{'train_loss:8.229303 lr:0.000527'}\n",
      "[Train Epoch]1/1 [Time]7042.06 [Step]28200 [Batch]141000 [Speed]49.94ms/step [Loss]8.2291 [Metrics]{'train_loss:8.229145 lr:0.000526'}\n",
      "[Train Epoch]1/1 [Time]7066.19 [Step]28300 [Batch]141500 [Speed]49.94ms/step [Loss]8.2289 [Metrics]{'train_loss:8.228892 lr:0.000525'}\n",
      "[Train Epoch]1/1 [Time]7090.30 [Step]28400 [Batch]142000 [Speed]49.93ms/step [Loss]8.2288 [Metrics]{'train_loss:8.228833 lr:0.000524'}\n",
      "[Train Epoch]1/1 [Time]7114.47 [Step]28500 [Batch]142500 [Speed]49.93ms/step [Loss]8.2287 [Metrics]{'train_loss:8.228697 lr:0.000524'}\n",
      "[Train Epoch]1/1 [Time]7138.58 [Step]28600 [Batch]143000 [Speed]49.92ms/step [Loss]8.2288 [Metrics]{'train_loss:8.228768 lr:0.000523'}\n",
      "[Train Epoch]1/1 [Time]7162.71 [Step]28700 [Batch]143500 [Speed]49.91ms/step [Loss]8.2286 [Metrics]{'train_loss:8.228621 lr:0.000522'}\n",
      "[Train Epoch]1/1 [Time]7186.83 [Step]28800 [Batch]144000 [Speed]49.91ms/step [Loss]8.2284 [Metrics]{'train_loss:8.228387 lr:0.000521'}\n",
      "[Train Epoch]1/1 [Time]7210.95 [Step]28900 [Batch]144500 [Speed]49.90ms/step [Loss]8.2282 [Metrics]{'train_loss:8.228249 lr:0.000520'}\n",
      "[Train Epoch]1/1 [Time]7235.07 [Step]29000 [Batch]145000 [Speed]49.90ms/step [Loss]8.2281 [Metrics]{'train_loss:8.228141 lr:0.000519'}\n",
      "[Train Epoch]1/1 [Time]7259.23 [Step]29100 [Batch]145500 [Speed]49.89ms/step [Loss]8.2280 [Metrics]{'train_loss:8.228007 lr:0.000518'}\n",
      "[Train Epoch]1/1 [Time]7283.42 [Step]29200 [Batch]146000 [Speed]49.89ms/step [Loss]8.2280 [Metrics]{'train_loss:8.227969 lr:0.000517'}\n",
      "[Train Epoch]1/1 [Time]7307.52 [Step]29300 [Batch]146500 [Speed]49.88ms/step [Loss]8.2278 [Metrics]{'train_loss:8.227768 lr:0.000516'}\n",
      "[Train Epoch]1/1 [Time]7331.60 [Step]29400 [Batch]147000 [Speed]49.87ms/step [Loss]8.2277 [Metrics]{'train_loss:8.227669 lr:0.000515'}\n",
      "[Train Epoch]1/1 [Time]7355.69 [Step]29500 [Batch]147500 [Speed]49.87ms/step [Loss]8.2276 [Metrics]{'train_loss:8.227560 lr:0.000515'}\n",
      "[Train Epoch]1/1 [Time]7379.84 [Step]29600 [Batch]148000 [Speed]49.86ms/step [Loss]8.2278 [Metrics]{'train_loss:8.227808 lr:0.000514'}\n",
      "[Train Epoch]1/1 [Time]7403.94 [Step]29700 [Batch]148500 [Speed]49.86ms/step [Loss]8.2278 [Metrics]{'train_loss:8.227847 lr:0.000513'}\n",
      "[Train Epoch]1/1 [Time]7428.11 [Step]29800 [Batch]149000 [Speed]49.85ms/step [Loss]8.2278 [Metrics]{'train_loss:8.227789 lr:0.000512'}\n",
      "[Train Epoch]1/1 [Time]7452.23 [Step]29900 [Batch]149500 [Speed]49.85ms/step [Loss]8.2279 [Metrics]{'train_loss:8.227898 lr:0.000511'}\n",
      "Saving checkpoint for epoch 1 at step 150000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]7480.41 [Step]30000 [Batch]150000 [Speed]49.87ms/step [Loss]8.2278 [Metrics]{'train_loss:8.227835 lr:0.000510'}\n",
      "[Train Epoch]1/1 [Time]7504.58 [Step]30100 [Batch]150500 [Speed]49.86ms/step [Loss]8.2278 [Metrics]{'train_loss:8.227793 lr:0.000509'}\n",
      "[Train Epoch]1/1 [Time]7528.72 [Step]30200 [Batch]151000 [Speed]49.86ms/step [Loss]8.2278 [Metrics]{'train_loss:8.227774 lr:0.000509'}\n",
      "[Train Epoch]1/1 [Time]7552.83 [Step]30300 [Batch]151500 [Speed]49.85ms/step [Loss]8.2277 [Metrics]{'train_loss:8.227713 lr:0.000508'}\n",
      "[Train Epoch]1/1 [Time]7576.99 [Step]30400 [Batch]152000 [Speed]49.85ms/step [Loss]8.2277 [Metrics]{'train_loss:8.227715 lr:0.000507'}\n",
      "[Train Epoch]1/1 [Time]7601.14 [Step]30500 [Batch]152500 [Speed]49.84ms/step [Loss]8.2275 [Metrics]{'train_loss:8.227540 lr:0.000506'}\n",
      "[Train Epoch]1/1 [Time]7625.31 [Step]30600 [Batch]153000 [Speed]49.84ms/step [Loss]8.2275 [Metrics]{'train_loss:8.227504 lr:0.000505'}\n",
      "[Train Epoch]1/1 [Time]7649.46 [Step]30700 [Batch]153500 [Speed]49.83ms/step [Loss]8.2273 [Metrics]{'train_loss:8.227258 lr:0.000504'}\n",
      "[Train Epoch]1/1 [Time]7673.60 [Step]30800 [Batch]154000 [Speed]49.83ms/step [Loss]8.2271 [Metrics]{'train_loss:8.227093 lr:0.000504'}\n",
      "[Train Epoch]1/1 [Time]7697.75 [Step]30900 [Batch]154500 [Speed]49.82ms/step [Loss]8.2270 [Metrics]{'train_loss:8.226951 lr:0.000503'}\n",
      "[Train Epoch]1/1 [Time]7721.89 [Step]31000 [Batch]155000 [Speed]49.82ms/step [Loss]8.2268 [Metrics]{'train_loss:8.226836 lr:0.000502'}\n",
      "[Train Epoch]1/1 [Time]7746.03 [Step]31100 [Batch]155500 [Speed]49.81ms/step [Loss]8.2267 [Metrics]{'train_loss:8.226685 lr:0.000501'}\n",
      "[Train Epoch]1/1 [Time]7770.15 [Step]31200 [Batch]156000 [Speed]49.81ms/step [Loss]8.2267 [Metrics]{'train_loss:8.226688 lr:0.000500'}\n",
      "[Train Epoch]1/1 [Time]7794.25 [Step]31300 [Batch]156500 [Speed]49.80ms/step [Loss]8.2266 [Metrics]{'train_loss:8.226571 lr:0.000500'}\n",
      "[Train Epoch]1/1 [Time]7818.35 [Step]31400 [Batch]157000 [Speed]49.80ms/step [Loss]8.2265 [Metrics]{'train_loss:8.226502 lr:0.000499'}\n",
      "[Train Epoch]1/1 [Time]7842.45 [Step]31500 [Batch]157500 [Speed]49.79ms/step [Loss]8.2265 [Metrics]{'train_loss:8.226546 lr:0.000498'}\n",
      "[Train Epoch]1/1 [Time]7866.63 [Step]31600 [Batch]158000 [Speed]49.79ms/step [Loss]8.2265 [Metrics]{'train_loss:8.226534 lr:0.000497'}\n",
      "[Train Epoch]1/1 [Time]7890.80 [Step]31700 [Batch]158500 [Speed]49.78ms/step [Loss]8.2266 [Metrics]{'train_loss:8.226592 lr:0.000496'}\n",
      "[Train Epoch]1/1 [Time]7914.95 [Step]31800 [Batch]159000 [Speed]49.78ms/step [Loss]8.2269 [Metrics]{'train_loss:8.226851 lr:0.000496'}\n",
      "[Train Epoch]1/1 [Time]7939.08 [Step]31900 [Batch]159500 [Speed]49.77ms/step [Loss]8.2270 [Metrics]{'train_loss:8.227037 lr:0.000495'}\n",
      "[Train Epoch]1/1 [Time]7963.19 [Step]32000 [Batch]160000 [Speed]49.77ms/step [Loss]8.2271 [Metrics]{'train_loss:8.227077 lr:0.000494'}\n",
      "[Train Epoch]1/1 [Time]7987.30 [Step]32100 [Batch]160500 [Speed]49.77ms/step [Loss]8.2271 [Metrics]{'train_loss:8.227084 lr:0.000493'}\n",
      "[Train Epoch]1/1 [Time]8011.45 [Step]32200 [Batch]161000 [Speed]49.76ms/step [Loss]8.2269 [Metrics]{'train_loss:8.226851 lr:0.000493'}\n",
      "[Train Epoch]1/1 [Time]8035.58 [Step]32300 [Batch]161500 [Speed]49.76ms/step [Loss]8.2267 [Metrics]{'train_loss:8.226732 lr:0.000492'}\n",
      "[Train Epoch]1/1 [Time]8059.69 [Step]32400 [Batch]162000 [Speed]49.75ms/step [Loss]8.2266 [Metrics]{'train_loss:8.226558 lr:0.000491'}\n",
      "[Train Epoch]1/1 [Time]8083.83 [Step]32500 [Batch]162500 [Speed]49.75ms/step [Loss]8.2264 [Metrics]{'train_loss:8.226432 lr:0.000490'}\n",
      "[Train Epoch]1/1 [Time]8107.99 [Step]32600 [Batch]163000 [Speed]49.74ms/step [Loss]8.2263 [Metrics]{'train_loss:8.226260 lr:0.000490'}\n",
      "[Train Epoch]1/1 [Time]8132.10 [Step]32700 [Batch]163500 [Speed]49.74ms/step [Loss]8.2261 [Metrics]{'train_loss:8.226138 lr:0.000489'}\n",
      "[Train Epoch]1/1 [Time]8156.20 [Step]32800 [Batch]164000 [Speed]49.73ms/step [Loss]8.2261 [Metrics]{'train_loss:8.226149 lr:0.000488'}\n",
      "[Train Epoch]1/1 [Time]8180.29 [Step]32900 [Batch]164500 [Speed]49.73ms/step [Loss]8.2259 [Metrics]{'train_loss:8.225891 lr:0.000487'}\n",
      "[Train Epoch]1/1 [Time]8204.42 [Step]33000 [Batch]165000 [Speed]49.72ms/step [Loss]8.2257 [Metrics]{'train_loss:8.225686 lr:0.000487'}\n",
      "[Train Epoch]1/1 [Time]8228.51 [Step]33100 [Batch]165500 [Speed]49.72ms/step [Loss]8.2254 [Metrics]{'train_loss:8.225399 lr:0.000486'}\n",
      "[Train Epoch]1/1 [Time]8252.62 [Step]33200 [Batch]166000 [Speed]49.71ms/step [Loss]8.2253 [Metrics]{'train_loss:8.225302 lr:0.000485'}\n",
      "[Train Epoch]1/1 [Time]8276.78 [Step]33300 [Batch]166500 [Speed]49.71ms/step [Loss]8.2252 [Metrics]{'train_loss:8.225162 lr:0.000484'}\n",
      "[Train Epoch]1/1 [Time]8300.92 [Step]33400 [Batch]167000 [Speed]49.71ms/step [Loss]8.2248 [Metrics]{'train_loss:8.224825 lr:0.000484'}\n",
      "[Train Epoch]1/1 [Time]8325.03 [Step]33500 [Batch]167500 [Speed]49.70ms/step [Loss]8.2246 [Metrics]{'train_loss:8.224563 lr:0.000483'}\n",
      "[Train Epoch]1/1 [Time]8349.12 [Step]33600 [Batch]168000 [Speed]49.70ms/step [Loss]8.2244 [Metrics]{'train_loss:8.224376 lr:0.000482'}\n",
      "[Train Epoch]1/1 [Time]8373.32 [Step]33700 [Batch]168500 [Speed]49.69ms/step [Loss]8.2242 [Metrics]{'train_loss:8.224231 lr:0.000481'}\n",
      "[Train Epoch]1/1 [Time]8397.41 [Step]33800 [Batch]169000 [Speed]49.69ms/step [Loss]8.2242 [Metrics]{'train_loss:8.224151 lr:0.000481'}\n",
      "[Train Epoch]1/1 [Time]8421.54 [Step]33900 [Batch]169500 [Speed]49.68ms/step [Loss]8.2241 [Metrics]{'train_loss:8.224053 lr:0.000480'}\n",
      "[Train Epoch]1/1 [Time]8445.65 [Step]34000 [Batch]170000 [Speed]49.68ms/step [Loss]8.2240 [Metrics]{'train_loss:8.223968 lr:0.000479'}\n",
      "[Train Epoch]1/1 [Time]8469.80 [Step]34100 [Batch]170500 [Speed]49.68ms/step [Loss]8.2238 [Metrics]{'train_loss:8.223809 lr:0.000479'}\n",
      "[Train Epoch]1/1 [Time]8493.94 [Step]34200 [Batch]171000 [Speed]49.67ms/step [Loss]8.2237 [Metrics]{'train_loss:8.223659 lr:0.000478'}\n",
      "[Train Epoch]1/1 [Time]8518.05 [Step]34300 [Batch]171500 [Speed]49.67ms/step [Loss]8.2235 [Metrics]{'train_loss:8.223514 lr:0.000477'}\n",
      "[Train Epoch]1/1 [Time]8542.17 [Step]34400 [Batch]172000 [Speed]49.66ms/step [Loss]8.2234 [Metrics]{'train_loss:8.223435 lr:0.000477'}\n",
      "[Train Epoch]1/1 [Time]8566.26 [Step]34500 [Batch]172500 [Speed]49.66ms/step [Loss]8.2234 [Metrics]{'train_loss:8.223397 lr:0.000476'}\n",
      "[Train Epoch]1/1 [Time]8590.41 [Step]34600 [Batch]173000 [Speed]49.66ms/step [Loss]8.2234 [Metrics]{'train_loss:8.223373 lr:0.000475'}\n",
      "[Train Epoch]1/1 [Time]8614.53 [Step]34700 [Batch]173500 [Speed]49.65ms/step [Loss]8.2233 [Metrics]{'train_loss:8.223277 lr:0.000474'}\n",
      "[Train Epoch]1/1 [Time]8638.68 [Step]34800 [Batch]174000 [Speed]49.65ms/step [Loss]8.2235 [Metrics]{'train_loss:8.223494 lr:0.000474'}\n",
      "[Train Epoch]1/1 [Time]8662.80 [Step]34900 [Batch]174500 [Speed]49.64ms/step [Loss]8.2234 [Metrics]{'train_loss:8.223400 lr:0.000473'}\n",
      "Saving checkpoint for epoch 1 at step 175000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]8691.03 [Step]35000 [Batch]175000 [Speed]49.66ms/step [Loss]8.2233 [Metrics]{'train_loss:8.223316 lr:0.000472'}\n",
      "[Train Epoch]1/1 [Time]8715.18 [Step]35100 [Batch]175500 [Speed]49.66ms/step [Loss]8.2232 [Metrics]{'train_loss:8.223243 lr:0.000472'}\n",
      "[Train Epoch]1/1 [Time]8739.34 [Step]35200 [Batch]176000 [Speed]49.66ms/step [Loss]8.2233 [Metrics]{'train_loss:8.223296 lr:0.000471'}\n",
      "[Train Epoch]1/1 [Time]8763.48 [Step]35300 [Batch]176500 [Speed]49.65ms/step [Loss]8.2232 [Metrics]{'train_loss:8.223181 lr:0.000470'}\n",
      "[Train Epoch]1/1 [Time]8787.58 [Step]35400 [Batch]177000 [Speed]49.65ms/step [Loss]8.2232 [Metrics]{'train_loss:8.223182 lr:0.000470'}\n",
      "[Train Epoch]1/1 [Time]8811.71 [Step]35500 [Batch]177500 [Speed]49.64ms/step [Loss]8.2232 [Metrics]{'train_loss:8.223173 lr:0.000469'}\n",
      "[Train Epoch]1/1 [Time]8835.84 [Step]35600 [Batch]178000 [Speed]49.64ms/step [Loss]8.2232 [Metrics]{'train_loss:8.223238 lr:0.000468'}\n",
      "[Train Epoch]1/1 [Time]8859.99 [Step]35700 [Batch]178500 [Speed]49.64ms/step [Loss]8.2234 [Metrics]{'train_loss:8.223413 lr:0.000468'}\n",
      "[Train Epoch]1/1 [Time]8884.09 [Step]35800 [Batch]179000 [Speed]49.63ms/step [Loss]8.2235 [Metrics]{'train_loss:8.223477 lr:0.000467'}\n",
      "[Train Epoch]1/1 [Time]8908.22 [Step]35900 [Batch]179500 [Speed]49.63ms/step [Loss]8.2235 [Metrics]{'train_loss:8.223490 lr:0.000466'}\n",
      "[Train Epoch]1/1 [Time]8932.36 [Step]36000 [Batch]180000 [Speed]49.62ms/step [Loss]8.2235 [Metrics]{'train_loss:8.223482 lr:0.000466'}\n",
      "[Train Epoch]1/1 [Time]8956.49 [Step]36100 [Batch]180500 [Speed]49.62ms/step [Loss]8.2235 [Metrics]{'train_loss:8.223549 lr:0.000465'}\n",
      "[Train Epoch]1/1 [Time]8980.62 [Step]36200 [Batch]181000 [Speed]49.62ms/step [Loss]8.2236 [Metrics]{'train_loss:8.223595 lr:0.000465'}\n",
      "[Train Epoch]1/1 [Time]9004.73 [Step]36300 [Batch]181500 [Speed]49.61ms/step [Loss]8.2235 [Metrics]{'train_loss:8.223461 lr:0.000464'}\n",
      "[Train Epoch]1/1 [Time]9028.84 [Step]36400 [Batch]182000 [Speed]49.61ms/step [Loss]8.2234 [Metrics]{'train_loss:8.223351 lr:0.000463'}\n",
      "[Train Epoch]1/1 [Time]9052.98 [Step]36500 [Batch]182500 [Speed]49.61ms/step [Loss]8.2232 [Metrics]{'train_loss:8.223241 lr:0.000463'}\n",
      "[Train Epoch]1/1 [Time]9077.11 [Step]36600 [Batch]183000 [Speed]49.60ms/step [Loss]8.2231 [Metrics]{'train_loss:8.223148 lr:0.000462'}\n",
      "[Train Epoch]1/1 [Time]9101.25 [Step]36700 [Batch]183500 [Speed]49.60ms/step [Loss]8.2230 [Metrics]{'train_loss:8.223037 lr:0.000461'}\n",
      "[Train Epoch]1/1 [Time]9125.43 [Step]36800 [Batch]184000 [Speed]49.59ms/step [Loss]8.2229 [Metrics]{'train_loss:8.222865 lr:0.000461'}\n",
      "[Train Epoch]1/1 [Time]9149.55 [Step]36900 [Batch]184500 [Speed]49.59ms/step [Loss]8.2228 [Metrics]{'train_loss:8.222775 lr:0.000460'}\n",
      "[Train Epoch]1/1 [Time]9173.65 [Step]37000 [Batch]185000 [Speed]49.59ms/step [Loss]8.2227 [Metrics]{'train_loss:8.222711 lr:0.000460'}\n",
      "[Train Epoch]1/1 [Time]9197.76 [Step]37100 [Batch]185500 [Speed]49.58ms/step [Loss]8.2227 [Metrics]{'train_loss:8.222658 lr:0.000459'}\n",
      "[Train Epoch]1/1 [Time]9221.86 [Step]37200 [Batch]186000 [Speed]49.58ms/step [Loss]8.2227 [Metrics]{'train_loss:8.222710 lr:0.000458'}\n",
      "[Train Epoch]1/1 [Time]9245.96 [Step]37300 [Batch]186500 [Speed]49.58ms/step [Loss]8.2225 [Metrics]{'train_loss:8.222526 lr:0.000458'}\n",
      "[Train Epoch]1/1 [Time]9270.11 [Step]37400 [Batch]187000 [Speed]49.57ms/step [Loss]8.2224 [Metrics]{'train_loss:8.222429 lr:0.000457'}\n",
      "[Train Epoch]1/1 [Time]9294.24 [Step]37500 [Batch]187500 [Speed]49.57ms/step [Loss]8.2224 [Metrics]{'train_loss:8.222397 lr:0.000456'}\n",
      "[Train Epoch]1/1 [Time]9318.38 [Step]37600 [Batch]188000 [Speed]49.57ms/step [Loss]8.2222 [Metrics]{'train_loss:8.222187 lr:0.000456'}\n",
      "[Train Epoch]1/1 [Time]9342.52 [Step]37700 [Batch]188500 [Speed]49.56ms/step [Loss]8.2221 [Metrics]{'train_loss:8.222124 lr:0.000455'}\n",
      "[Train Epoch]1/1 [Time]9366.67 [Step]37800 [Batch]189000 [Speed]49.56ms/step [Loss]8.2221 [Metrics]{'train_loss:8.222133 lr:0.000455'}\n",
      "[Train Epoch]1/1 [Time]9390.79 [Step]37900 [Batch]189500 [Speed]49.56ms/step [Loss]8.2221 [Metrics]{'train_loss:8.222148 lr:0.000454'}\n",
      "[Train Epoch]1/1 [Time]9414.89 [Step]38000 [Batch]190000 [Speed]49.55ms/step [Loss]8.2220 [Metrics]{'train_loss:8.221970 lr:0.000453'}\n",
      "[Train Epoch]1/1 [Time]9439.00 [Step]38100 [Batch]190500 [Speed]49.55ms/step [Loss]8.2218 [Metrics]{'train_loss:8.221823 lr:0.000453'}\n",
      "[Train Epoch]1/1 [Time]9463.11 [Step]38200 [Batch]191000 [Speed]49.55ms/step [Loss]8.2217 [Metrics]{'train_loss:8.221743 lr:0.000452'}\n",
      "[Train Epoch]1/1 [Time]9487.23 [Step]38300 [Batch]191500 [Speed]49.54ms/step [Loss]8.2216 [Metrics]{'train_loss:8.221618 lr:0.000452'}\n",
      "[Train Epoch]1/1 [Time]9511.35 [Step]38400 [Batch]192000 [Speed]49.54ms/step [Loss]8.2214 [Metrics]{'train_loss:8.221437 lr:0.000451'}\n",
      "[Train Epoch]1/1 [Time]9535.47 [Step]38500 [Batch]192500 [Speed]49.53ms/step [Loss]8.2214 [Metrics]{'train_loss:8.221357 lr:0.000450'}\n",
      "[Train Epoch]1/1 [Time]9559.58 [Step]38600 [Batch]193000 [Speed]49.53ms/step [Loss]8.2214 [Metrics]{'train_loss:8.221352 lr:0.000450'}\n",
      "[Train Epoch]1/1 [Time]9583.69 [Step]38700 [Batch]193500 [Speed]49.53ms/step [Loss]8.2212 [Metrics]{'train_loss:8.221233 lr:0.000449'}\n",
      "[Train Epoch]1/1 [Time]9607.78 [Step]38800 [Batch]194000 [Speed]49.52ms/step [Loss]8.2212 [Metrics]{'train_loss:8.221175 lr:0.000449'}\n",
      "[Train Epoch]1/1 [Time]9631.88 [Step]38900 [Batch]194500 [Speed]49.52ms/step [Loss]8.2211 [Metrics]{'train_loss:8.221060 lr:0.000448'}\n",
      "[Train Epoch]1/1 [Time]9656.01 [Step]39000 [Batch]195000 [Speed]49.52ms/step [Loss]8.2210 [Metrics]{'train_loss:8.220975 lr:0.000448'}\n",
      "[Train Epoch]1/1 [Time]9680.21 [Step]39100 [Batch]195500 [Speed]49.52ms/step [Loss]8.2210 [Metrics]{'train_loss:8.221018 lr:0.000447'}\n",
      "[Train Epoch]1/1 [Time]9704.32 [Step]39200 [Batch]196000 [Speed]49.51ms/step [Loss]8.2209 [Metrics]{'train_loss:8.220921 lr:0.000446'}\n",
      "[Train Epoch]1/1 [Time]9728.48 [Step]39300 [Batch]196500 [Speed]49.51ms/step [Loss]8.2209 [Metrics]{'train_loss:8.220859 lr:0.000446'}\n",
      "[Train Epoch]1/1 [Time]9752.61 [Step]39400 [Batch]197000 [Speed]49.51ms/step [Loss]8.2209 [Metrics]{'train_loss:8.220860 lr:0.000445'}\n",
      "[Train Epoch]1/1 [Time]9776.74 [Step]39500 [Batch]197500 [Speed]49.50ms/step [Loss]8.2207 [Metrics]{'train_loss:8.220707 lr:0.000445'}\n",
      "[Train Epoch]1/1 [Time]9800.86 [Step]39600 [Batch]198000 [Speed]49.50ms/step [Loss]8.2207 [Metrics]{'train_loss:8.220676 lr:0.000444'}\n",
      "[Train Epoch]1/1 [Time]9824.98 [Step]39700 [Batch]198500 [Speed]49.50ms/step [Loss]8.2206 [Metrics]{'train_loss:8.220602 lr:0.000444'}\n",
      "[Train Epoch]1/1 [Time]9849.11 [Step]39800 [Batch]199000 [Speed]49.49ms/step [Loss]8.2206 [Metrics]{'train_loss:8.220612 lr:0.000443'}\n",
      "[Train Epoch]1/1 [Time]9873.21 [Step]39900 [Batch]199500 [Speed]49.49ms/step [Loss]8.2205 [Metrics]{'train_loss:8.220529 lr:0.000442'}\n",
      "Saving checkpoint for epoch 1 at step 200000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]9901.42 [Step]40000 [Batch]200000 [Speed]49.51ms/step [Loss]8.2204 [Metrics]{'train_loss:8.220368 lr:0.000442'}\n",
      "[Train Epoch]1/1 [Time]9925.65 [Step]40100 [Batch]200500 [Speed]49.50ms/step [Loss]8.2203 [Metrics]{'train_loss:8.220319 lr:0.000441'}\n",
      "[Train Epoch]1/1 [Time]9949.77 [Step]40200 [Batch]201000 [Speed]49.50ms/step [Loss]8.2203 [Metrics]{'train_loss:8.220296 lr:0.000441'}\n",
      "[Train Epoch]1/1 [Time]9973.88 [Step]40300 [Batch]201500 [Speed]49.50ms/step [Loss]8.2204 [Metrics]{'train_loss:8.220395 lr:0.000440'}\n",
      "[Train Epoch]1/1 [Time]9998.02 [Step]40400 [Batch]202000 [Speed]49.50ms/step [Loss]8.2205 [Metrics]{'train_loss:8.220479 lr:0.000440'}\n",
      "[Train Epoch]1/1 [Time]10022.12 [Step]40500 [Batch]202500 [Speed]49.49ms/step [Loss]8.2206 [Metrics]{'train_loss:8.220639 lr:0.000439'}\n",
      "[Train Epoch]1/1 [Time]10046.22 [Step]40600 [Batch]203000 [Speed]49.49ms/step [Loss]8.2206 [Metrics]{'train_loss:8.220648 lr:0.000439'}\n",
      "[Train Epoch]1/1 [Time]10070.34 [Step]40700 [Batch]203500 [Speed]49.49ms/step [Loss]8.2206 [Metrics]{'train_loss:8.220562 lr:0.000438'}\n",
      "[Train Epoch]1/1 [Time]10094.47 [Step]40800 [Batch]204000 [Speed]49.48ms/step [Loss]8.2205 [Metrics]{'train_loss:8.220460 lr:0.000438'}\n",
      "[Train Epoch]1/1 [Time]10118.63 [Step]40900 [Batch]204500 [Speed]49.48ms/step [Loss]8.2204 [Metrics]{'train_loss:8.220372 lr:0.000437'}\n",
      "[Train Epoch]1/1 [Time]10142.75 [Step]41000 [Batch]205000 [Speed]49.48ms/step [Loss]8.2203 [Metrics]{'train_loss:8.220313 lr:0.000437'}\n",
      "[Train Epoch]1/1 [Time]10166.88 [Step]41100 [Batch]205500 [Speed]49.47ms/step [Loss]8.2204 [Metrics]{'train_loss:8.220402 lr:0.000436'}\n",
      "[Train Epoch]1/1 [Time]10191.01 [Step]41200 [Batch]206000 [Speed]49.47ms/step [Loss]8.2202 [Metrics]{'train_loss:8.220230 lr:0.000435'}\n",
      "[Train Epoch]1/1 [Time]10215.12 [Step]41300 [Batch]206500 [Speed]49.47ms/step [Loss]8.2201 [Metrics]{'train_loss:8.220104 lr:0.000435'}\n",
      "[Train Epoch]1/1 [Time]10239.21 [Step]41400 [Batch]207000 [Speed]49.46ms/step [Loss]8.2199 [Metrics]{'train_loss:8.219950 lr:0.000434'}\n",
      "[Train Epoch]1/1 [Time]10263.30 [Step]41500 [Batch]207500 [Speed]49.46ms/step [Loss]8.2199 [Metrics]{'train_loss:8.219905 lr:0.000434'}\n",
      "[Train Epoch]1/1 [Time]10287.41 [Step]41600 [Batch]208000 [Speed]49.46ms/step [Loss]8.2198 [Metrics]{'train_loss:8.219750 lr:0.000433'}\n",
      "[Train Epoch]1/1 [Time]10311.51 [Step]41700 [Batch]208500 [Speed]49.46ms/step [Loss]8.2196 [Metrics]{'train_loss:8.219598 lr:0.000433'}\n",
      "[Train Epoch]1/1 [Time]10335.62 [Step]41800 [Batch]209000 [Speed]49.45ms/step [Loss]8.2196 [Metrics]{'train_loss:8.219602 lr:0.000432'}\n",
      "[Train Epoch]1/1 [Time]10359.74 [Step]41900 [Batch]209500 [Speed]49.45ms/step [Loss]8.2195 [Metrics]{'train_loss:8.219510 lr:0.000432'}\n",
      "[Train Epoch]1/1 [Time]10383.87 [Step]42000 [Batch]210000 [Speed]49.45ms/step [Loss]8.2193 [Metrics]{'train_loss:8.219268 lr:0.000431'}\n",
      "[Train Epoch]1/1 [Time]10407.99 [Step]42100 [Batch]210500 [Speed]49.44ms/step [Loss]8.2191 [Metrics]{'train_loss:8.219135 lr:0.000431'}\n",
      "[Train Epoch]1/1 [Time]10432.10 [Step]42200 [Batch]211000 [Speed]49.44ms/step [Loss]8.2191 [Metrics]{'train_loss:8.219085 lr:0.000430'}\n",
      "[Train Epoch]1/1 [Time]10456.20 [Step]42300 [Batch]211500 [Speed]49.44ms/step [Loss]8.2189 [Metrics]{'train_loss:8.218905 lr:0.000430'}\n",
      "[Train Epoch]1/1 [Time]10480.30 [Step]42400 [Batch]212000 [Speed]49.44ms/step [Loss]8.2187 [Metrics]{'train_loss:8.218680 lr:0.000429'}\n",
      "[Train Epoch]1/1 [Time]10504.44 [Step]42500 [Batch]212500 [Speed]49.43ms/step [Loss]8.2185 [Metrics]{'train_loss:8.218534 lr:0.000429'}\n",
      "[Train Epoch]1/1 [Time]10528.61 [Step]42600 [Batch]213000 [Speed]49.43ms/step [Loss]8.2184 [Metrics]{'train_loss:8.218445 lr:0.000428'}\n",
      "[Train Epoch]1/1 [Time]10552.71 [Step]42700 [Batch]213500 [Speed]49.43ms/step [Loss]8.2183 [Metrics]{'train_loss:8.218303 lr:0.000428'}\n",
      "[Train Epoch]1/1 [Time]10576.87 [Step]42800 [Batch]214000 [Speed]49.42ms/step [Loss]8.2181 [Metrics]{'train_loss:8.218119 lr:0.000427'}\n",
      "[Train Epoch]1/1 [Time]10601.06 [Step]42900 [Batch]214500 [Speed]49.42ms/step [Loss]8.2181 [Metrics]{'train_loss:8.218116 lr:0.000427'}\n",
      "[Train Epoch]1/1 [Time]10625.16 [Step]43000 [Batch]215000 [Speed]49.42ms/step [Loss]8.2179 [Metrics]{'train_loss:8.217920 lr:0.000426'}\n",
      "[Train Epoch]1/1 [Time]10649.24 [Step]43100 [Batch]215500 [Speed]49.42ms/step [Loss]8.2180 [Metrics]{'train_loss:8.218019 lr:0.000426'}\n",
      "[Train Epoch]1/1 [Time]10673.34 [Step]43200 [Batch]216000 [Speed]49.41ms/step [Loss]8.2181 [Metrics]{'train_loss:8.218113 lr:0.000425'}\n",
      "[Train Epoch]1/1 [Time]10697.46 [Step]43300 [Batch]216500 [Speed]49.41ms/step [Loss]8.2181 [Metrics]{'train_loss:8.218126 lr:0.000425'}\n",
      "[Train Epoch]1/1 [Time]10721.55 [Step]43400 [Batch]217000 [Speed]49.41ms/step [Loss]8.2182 [Metrics]{'train_loss:8.218167 lr:0.000424'}\n",
      "[Train Epoch]1/1 [Time]10745.71 [Step]43500 [Batch]217500 [Speed]49.41ms/step [Loss]8.2181 [Metrics]{'train_loss:8.218124 lr:0.000424'}\n",
      "[Train Epoch]1/1 [Time]10769.83 [Step]43600 [Batch]218000 [Speed]49.40ms/step [Loss]8.2181 [Metrics]{'train_loss:8.218145 lr:0.000423'}\n",
      "[Train Epoch]1/1 [Time]10793.97 [Step]43700 [Batch]218500 [Speed]49.40ms/step [Loss]8.2183 [Metrics]{'train_loss:8.218266 lr:0.000423'}\n",
      "[Train Epoch]1/1 [Time]10818.10 [Step]43800 [Batch]219000 [Speed]49.40ms/step [Loss]8.2182 [Metrics]{'train_loss:8.218237 lr:0.000422'}\n",
      "[Train Epoch]1/1 [Time]10842.25 [Step]43900 [Batch]219500 [Speed]49.40ms/step [Loss]8.2182 [Metrics]{'train_loss:8.218153 lr:0.000422'}\n",
      "[Train Epoch]1/1 [Time]10866.34 [Step]44000 [Batch]220000 [Speed]49.39ms/step [Loss]8.2179 [Metrics]{'train_loss:8.217949 lr:0.000421'}\n",
      "[Train Epoch]1/1 [Time]10890.45 [Step]44100 [Batch]220500 [Speed]49.39ms/step [Loss]8.2178 [Metrics]{'train_loss:8.217787 lr:0.000421'}\n",
      "[Train Epoch]1/1 [Time]10914.55 [Step]44200 [Batch]221000 [Speed]49.39ms/step [Loss]8.2176 [Metrics]{'train_loss:8.217628 lr:0.000420'}\n",
      "[Train Epoch]1/1 [Time]10938.71 [Step]44300 [Batch]221500 [Speed]49.38ms/step [Loss]8.2176 [Metrics]{'train_loss:8.217621 lr:0.000420'}\n",
      "[Train Epoch]1/1 [Time]10962.85 [Step]44400 [Batch]222000 [Speed]49.38ms/step [Loss]8.2176 [Metrics]{'train_loss:8.217627 lr:0.000419'}\n",
      "[Train Epoch]1/1 [Time]10986.98 [Step]44500 [Batch]222500 [Speed]49.38ms/step [Loss]8.2175 [Metrics]{'train_loss:8.217525 lr:0.000419'}\n",
      "[Train Epoch]1/1 [Time]11011.12 [Step]44600 [Batch]223000 [Speed]49.38ms/step [Loss]8.2175 [Metrics]{'train_loss:8.217515 lr:0.000419'}\n",
      "[Train Epoch]1/1 [Time]11035.26 [Step]44700 [Batch]223500 [Speed]49.37ms/step [Loss]8.2176 [Metrics]{'train_loss:8.217615 lr:0.000418'}\n",
      "[Train Epoch]1/1 [Time]11059.36 [Step]44800 [Batch]224000 [Speed]49.37ms/step [Loss]8.2178 [Metrics]{'train_loss:8.217771 lr:0.000418'}\n",
      "[Train Epoch]1/1 [Time]11083.46 [Step]44900 [Batch]224500 [Speed]49.37ms/step [Loss]8.2177 [Metrics]{'train_loss:8.217734 lr:0.000417'}\n",
      "Saving checkpoint for epoch 1 at step 225000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]11111.69 [Step]45000 [Batch]225000 [Speed]49.39ms/step [Loss]8.2178 [Metrics]{'train_loss:8.217766 lr:0.000417'}\n",
      "[Train Epoch]1/1 [Time]11135.84 [Step]45100 [Batch]225500 [Speed]49.38ms/step [Loss]8.2176 [Metrics]{'train_loss:8.217636 lr:0.000416'}\n",
      "[Train Epoch]1/1 [Time]11159.93 [Step]45200 [Batch]226000 [Speed]49.38ms/step [Loss]8.2176 [Metrics]{'train_loss:8.217582 lr:0.000416'}\n",
      "[Train Epoch]1/1 [Time]11184.04 [Step]45300 [Batch]226500 [Speed]49.38ms/step [Loss]8.2175 [Metrics]{'train_loss:8.217525 lr:0.000415'}\n",
      "[Train Epoch]1/1 [Time]11208.15 [Step]45400 [Batch]227000 [Speed]49.38ms/step [Loss]8.2174 [Metrics]{'train_loss:8.217427 lr:0.000415'}\n",
      "[Train Epoch]1/1 [Time]11232.28 [Step]45500 [Batch]227500 [Speed]49.37ms/step [Loss]8.2174 [Metrics]{'train_loss:8.217430 lr:0.000414'}\n",
      "[Train Epoch]1/1 [Time]11256.38 [Step]45600 [Batch]228000 [Speed]49.37ms/step [Loss]8.2173 [Metrics]{'train_loss:8.217305 lr:0.000414'}\n",
      "[Train Epoch]1/1 [Time]11280.49 [Step]45700 [Batch]228500 [Speed]49.37ms/step [Loss]8.2172 [Metrics]{'train_loss:8.217190 lr:0.000413'}\n",
      "[Train Epoch]1/1 [Time]11304.55 [Step]45800 [Batch]229000 [Speed]49.36ms/step [Loss]8.2171 [Metrics]{'train_loss:8.217142 lr:0.000413'}\n",
      "[Train Epoch]1/1 [Time]11328.64 [Step]45900 [Batch]229500 [Speed]49.36ms/step [Loss]8.2170 [Metrics]{'train_loss:8.217031 lr:0.000413'}\n",
      "[Train Epoch]1/1 [Time]11352.76 [Step]46000 [Batch]230000 [Speed]49.36ms/step [Loss]8.2170 [Metrics]{'train_loss:8.216951 lr:0.000412'}\n",
      "[Train Epoch]1/1 [Time]11376.92 [Step]46100 [Batch]230500 [Speed]49.36ms/step [Loss]8.2170 [Metrics]{'train_loss:8.216961 lr:0.000412'}\n",
      "[Train Epoch]1/1 [Time]11401.06 [Step]46200 [Batch]231000 [Speed]49.36ms/step [Loss]8.2169 [Metrics]{'train_loss:8.216888 lr:0.000411'}\n",
      "[Train Epoch]1/1 [Time]11425.14 [Step]46300 [Batch]231500 [Speed]49.35ms/step [Loss]8.2169 [Metrics]{'train_loss:8.216854 lr:0.000411'}\n",
      "[Train Epoch]1/1 [Time]11449.23 [Step]46400 [Batch]232000 [Speed]49.35ms/step [Loss]8.2167 [Metrics]{'train_loss:8.216674 lr:0.000410'}\n",
      "[Train Epoch]1/1 [Time]11473.39 [Step]46500 [Batch]232500 [Speed]49.35ms/step [Loss]8.2167 [Metrics]{'train_loss:8.216662 lr:0.000410'}\n",
      "[Train Epoch]1/1 [Time]11497.51 [Step]46600 [Batch]233000 [Speed]49.35ms/step [Loss]8.2167 [Metrics]{'train_loss:8.216669 lr:0.000409'}\n",
      "[Train Epoch]1/1 [Time]11521.60 [Step]46700 [Batch]233500 [Speed]49.34ms/step [Loss]8.2166 [Metrics]{'train_loss:8.216628 lr:0.000409'}\n",
      "[Train Epoch]1/1 [Time]11545.71 [Step]46800 [Batch]234000 [Speed]49.34ms/step [Loss]8.2166 [Metrics]{'train_loss:8.216615 lr:0.000409'}\n",
      "[Train Epoch]1/1 [Time]11569.79 [Step]46900 [Batch]234500 [Speed]49.34ms/step [Loss]8.2164 [Metrics]{'train_loss:8.216441 lr:0.000408'}\n",
      "[Train Epoch]1/1 [Time]11593.96 [Step]47000 [Batch]235000 [Speed]49.34ms/step [Loss]8.2164 [Metrics]{'train_loss:8.216424 lr:0.000408'}\n",
      "[Train Epoch]1/1 [Time]11618.11 [Step]47100 [Batch]235500 [Speed]49.33ms/step [Loss]8.2164 [Metrics]{'train_loss:8.216433 lr:0.000407'}\n",
      "[Train Epoch]1/1 [Time]11642.28 [Step]47200 [Batch]236000 [Speed]49.33ms/step [Loss]8.2165 [Metrics]{'train_loss:8.216458 lr:0.000407'}\n",
      "[Train Epoch]1/1 [Time]11666.40 [Step]47300 [Batch]236500 [Speed]49.33ms/step [Loss]8.2164 [Metrics]{'train_loss:8.216380 lr:0.000406'}\n",
      "[Train Epoch]1/1 [Time]11690.50 [Step]47400 [Batch]237000 [Speed]49.33ms/step [Loss]8.2164 [Metrics]{'train_loss:8.216383 lr:0.000406'}\n",
      "[Train Epoch]1/1 [Time]11714.59 [Step]47500 [Batch]237500 [Speed]49.32ms/step [Loss]8.2164 [Metrics]{'train_loss:8.216376 lr:0.000406'}\n",
      "[Train Epoch]1/1 [Time]11738.71 [Step]47600 [Batch]238000 [Speed]49.32ms/step [Loss]8.2163 [Metrics]{'train_loss:8.216288 lr:0.000405'}\n",
      "[Train Epoch]1/1 [Time]11762.81 [Step]47700 [Batch]238500 [Speed]49.32ms/step [Loss]8.2162 [Metrics]{'train_loss:8.216248 lr:0.000405'}\n",
      "[Train Epoch]1/1 [Time]11786.90 [Step]47800 [Batch]239000 [Speed]49.32ms/step [Loss]8.2163 [Metrics]{'train_loss:8.216319 lr:0.000404'}\n",
      "[Train Epoch]1/1 [Time]11811.04 [Step]47900 [Batch]239500 [Speed]49.32ms/step [Loss]8.2162 [Metrics]{'train_loss:8.216215 lr:0.000404'}\n",
      "[Train Epoch]1/1 [Time]11835.19 [Step]48000 [Batch]240000 [Speed]49.31ms/step [Loss]8.2162 [Metrics]{'train_loss:8.216204 lr:0.000403'}\n",
      "[Train Epoch]1/1 [Time]11859.29 [Step]48100 [Batch]240500 [Speed]49.31ms/step [Loss]8.2161 [Metrics]{'train_loss:8.216137 lr:0.000403'}\n",
      "[Train Epoch]1/1 [Time]11883.39 [Step]48200 [Batch]241000 [Speed]49.31ms/step [Loss]8.2160 [Metrics]{'train_loss:8.215999 lr:0.000403'}\n",
      "[Train Epoch]1/1 [Time]11907.51 [Step]48300 [Batch]241500 [Speed]49.31ms/step [Loss]8.2158 [Metrics]{'train_loss:8.215837 lr:0.000402'}\n",
      "[Train Epoch]1/1 [Time]11931.57 [Step]48400 [Batch]242000 [Speed]49.30ms/step [Loss]8.2158 [Metrics]{'train_loss:8.215837 lr:0.000402'}\n",
      "[Train Epoch]1/1 [Time]11955.74 [Step]48500 [Batch]242500 [Speed]49.30ms/step [Loss]8.2157 [Metrics]{'train_loss:8.215694 lr:0.000401'}\n",
      "[Train Epoch]1/1 [Time]11979.83 [Step]48600 [Batch]243000 [Speed]49.30ms/step [Loss]8.2157 [Metrics]{'train_loss:8.215651 lr:0.000401'}\n",
      "[Train Epoch]1/1 [Time]12003.95 [Step]48700 [Batch]243500 [Speed]49.30ms/step [Loss]8.2156 [Metrics]{'train_loss:8.215568 lr:0.000401'}\n",
      "[Train Epoch]1/1 [Time]12028.07 [Step]48800 [Batch]244000 [Speed]49.30ms/step [Loss]8.2156 [Metrics]{'train_loss:8.215560 lr:0.000400'}\n",
      "[Train Epoch]1/1 [Time]12052.21 [Step]48900 [Batch]244500 [Speed]49.29ms/step [Loss]8.2155 [Metrics]{'train_loss:8.215521 lr:0.000400'}\n",
      "[Train Epoch]1/1 [Time]12076.36 [Step]49000 [Batch]245000 [Speed]49.29ms/step [Loss]8.2153 [Metrics]{'train_loss:8.215336 lr:0.000399'}\n",
      "[Train Epoch]1/1 [Time]12100.45 [Step]49100 [Batch]245500 [Speed]49.29ms/step [Loss]8.2152 [Metrics]{'train_loss:8.215238 lr:0.000399'}\n",
      "[Train Epoch]1/1 [Time]12124.53 [Step]49200 [Batch]246000 [Speed]49.29ms/step [Loss]8.2153 [Metrics]{'train_loss:8.215255 lr:0.000398'}\n",
      "[Train Epoch]1/1 [Time]12148.62 [Step]49300 [Batch]246500 [Speed]49.28ms/step [Loss]8.2153 [Metrics]{'train_loss:8.215281 lr:0.000398'}\n",
      "[Train Epoch]1/1 [Time]12172.74 [Step]49400 [Batch]247000 [Speed]49.28ms/step [Loss]8.2153 [Metrics]{'train_loss:8.215334 lr:0.000398'}\n",
      "[Train Epoch]1/1 [Time]12196.93 [Step]49500 [Batch]247500 [Speed]49.28ms/step [Loss]8.2153 [Metrics]{'train_loss:8.215312 lr:0.000397'}\n",
      "[Train Epoch]1/1 [Time]12221.02 [Step]49600 [Batch]248000 [Speed]49.28ms/step [Loss]8.2153 [Metrics]{'train_loss:8.215317 lr:0.000397'}\n",
      "[Train Epoch]1/1 [Time]12245.14 [Step]49700 [Batch]248500 [Speed]49.28ms/step [Loss]8.2154 [Metrics]{'train_loss:8.215408 lr:0.000396'}\n",
      "[Train Epoch]1/1 [Time]12269.26 [Step]49800 [Batch]249000 [Speed]49.27ms/step [Loss]8.2155 [Metrics]{'train_loss:8.215450 lr:0.000396'}\n",
      "[Train Epoch]1/1 [Time]12293.36 [Step]49900 [Batch]249500 [Speed]49.27ms/step [Loss]8.2154 [Metrics]{'train_loss:8.215449 lr:0.000396'}\n",
      "Saving checkpoint for epoch 1 at step 250000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]12321.58 [Step]50000 [Batch]250000 [Speed]49.29ms/step [Loss]8.2153 [Metrics]{'train_loss:8.215333 lr:0.000395'}\n",
      "[Train Epoch]1/1 [Time]12345.72 [Step]50100 [Batch]250500 [Speed]49.28ms/step [Loss]8.2153 [Metrics]{'train_loss:8.215259 lr:0.000395'}\n",
      "[Train Epoch]1/1 [Time]12369.81 [Step]50200 [Batch]251000 [Speed]49.28ms/step [Loss]8.2152 [Metrics]{'train_loss:8.215176 lr:0.000394'}\n",
      "[Train Epoch]1/1 [Time]12393.90 [Step]50300 [Batch]251500 [Speed]49.28ms/step [Loss]8.2151 [Metrics]{'train_loss:8.215109 lr:0.000394'}\n",
      "[Train Epoch]1/1 [Time]12418.03 [Step]50400 [Batch]252000 [Speed]49.28ms/step [Loss]8.2150 [Metrics]{'train_loss:8.215005 lr:0.000394'}\n",
      "[Train Epoch]1/1 [Time]12442.21 [Step]50500 [Batch]252500 [Speed]49.28ms/step [Loss]8.2149 [Metrics]{'train_loss:8.214856 lr:0.000393'}\n",
      "[Train Epoch]1/1 [Time]12466.36 [Step]50600 [Batch]253000 [Speed]49.27ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214804 lr:0.000393'}\n",
      "[Train Epoch]1/1 [Time]12490.49 [Step]50700 [Batch]253500 [Speed]49.27ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214753 lr:0.000393'}\n",
      "[Train Epoch]1/1 [Time]12514.58 [Step]50800 [Batch]254000 [Speed]49.27ms/step [Loss]8.2147 [Metrics]{'train_loss:8.214734 lr:0.000392'}\n",
      "[Train Epoch]1/1 [Time]12538.68 [Step]50900 [Batch]254500 [Speed]49.27ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214750 lr:0.000392'}\n",
      "[Train Epoch]1/1 [Time]12562.78 [Step]51000 [Batch]255000 [Speed]49.27ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214829 lr:0.000391'}\n",
      "[Train Epoch]1/1 [Time]12586.94 [Step]51100 [Batch]255500 [Speed]49.26ms/step [Loss]8.2149 [Metrics]{'train_loss:8.214914 lr:0.000391'}\n",
      "[Train Epoch]1/1 [Time]12611.08 [Step]51200 [Batch]256000 [Speed]49.26ms/step [Loss]8.2150 [Metrics]{'train_loss:8.214961 lr:0.000391'}\n",
      "[Train Epoch]1/1 [Time]12635.21 [Step]51300 [Batch]256500 [Speed]49.26ms/step [Loss]8.2149 [Metrics]{'train_loss:8.214876 lr:0.000390'}\n",
      "[Train Epoch]1/1 [Time]12659.31 [Step]51400 [Batch]257000 [Speed]49.26ms/step [Loss]8.2149 [Metrics]{'train_loss:8.214893 lr:0.000390'}\n",
      "[Train Epoch]1/1 [Time]12683.42 [Step]51500 [Batch]257500 [Speed]49.26ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214813 lr:0.000389'}\n",
      "[Train Epoch]1/1 [Time]12707.53 [Step]51600 [Batch]258000 [Speed]49.25ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214774 lr:0.000389'}\n",
      "[Train Epoch]1/1 [Time]12731.58 [Step]51700 [Batch]258500 [Speed]49.25ms/step [Loss]8.2147 [Metrics]{'train_loss:8.214744 lr:0.000389'}\n",
      "[Train Epoch]1/1 [Time]12755.67 [Step]51800 [Batch]259000 [Speed]49.25ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214782 lr:0.000388'}\n",
      "[Train Epoch]1/1 [Time]12779.77 [Step]51900 [Batch]259500 [Speed]49.25ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214811 lr:0.000388'}\n",
      "[Train Epoch]1/1 [Time]12803.85 [Step]52000 [Batch]260000 [Speed]49.25ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214777 lr:0.000388'}\n",
      "[Train Epoch]1/1 [Time]12827.97 [Step]52100 [Batch]260500 [Speed]49.24ms/step [Loss]8.2147 [Metrics]{'train_loss:8.214676 lr:0.000387'}\n",
      "[Train Epoch]1/1 [Time]12852.11 [Step]52200 [Batch]261000 [Speed]49.24ms/step [Loss]8.2147 [Metrics]{'train_loss:8.214671 lr:0.000387'}\n",
      "[Train Epoch]1/1 [Time]12876.25 [Step]52300 [Batch]261500 [Speed]49.24ms/step [Loss]8.2148 [Metrics]{'train_loss:8.214751 lr:0.000386'}\n",
      "[Train Epoch]1/1 [Time]12900.38 [Step]52400 [Batch]262000 [Speed]49.24ms/step [Loss]8.2147 [Metrics]{'train_loss:8.214658 lr:0.000386'}\n",
      "[Train Epoch]1/1 [Time]12924.50 [Step]52500 [Batch]262500 [Speed]49.24ms/step [Loss]8.2146 [Metrics]{'train_loss:8.214646 lr:0.000386'}\n",
      "[Train Epoch]1/1 [Time]12948.57 [Step]52600 [Batch]263000 [Speed]49.23ms/step [Loss]8.2146 [Metrics]{'train_loss:8.214625 lr:0.000385'}\n",
      "[Train Epoch]1/1 [Time]12972.65 [Step]52700 [Batch]263500 [Speed]49.23ms/step [Loss]8.2146 [Metrics]{'train_loss:8.214575 lr:0.000385'}\n",
      "[Train Epoch]1/1 [Time]12996.76 [Step]52800 [Batch]264000 [Speed]49.23ms/step [Loss]8.2145 [Metrics]{'train_loss:8.214466 lr:0.000385'}\n",
      "[Train Epoch]1/1 [Time]13020.91 [Step]52900 [Batch]264500 [Speed]49.23ms/step [Loss]8.2144 [Metrics]{'train_loss:8.214448 lr:0.000384'}\n",
      "[Train Epoch]1/1 [Time]13045.03 [Step]53000 [Batch]265000 [Speed]49.23ms/step [Loss]8.2144 [Metrics]{'train_loss:8.214379 lr:0.000384'}\n",
      "[Train Epoch]1/1 [Time]13069.16 [Step]53100 [Batch]265500 [Speed]49.22ms/step [Loss]8.2143 [Metrics]{'train_loss:8.214310 lr:0.000384'}\n",
      "[Train Epoch]1/1 [Time]13093.30 [Step]53200 [Batch]266000 [Speed]49.22ms/step [Loss]8.2142 [Metrics]{'train_loss:8.214190 lr:0.000383'}\n",
      "[Train Epoch]1/1 [Time]13117.41 [Step]53300 [Batch]266500 [Speed]49.22ms/step [Loss]8.2142 [Metrics]{'train_loss:8.214199 lr:0.000383'}\n",
      "[Train Epoch]1/1 [Time]13141.48 [Step]53400 [Batch]267000 [Speed]49.22ms/step [Loss]8.2141 [Metrics]{'train_loss:8.214123 lr:0.000382'}\n",
      "[Train Epoch]1/1 [Time]13165.53 [Step]53500 [Batch]267500 [Speed]49.22ms/step [Loss]8.2141 [Metrics]{'train_loss:8.214135 lr:0.000382'}\n",
      "[Train Epoch]1/1 [Time]13189.66 [Step]53600 [Batch]268000 [Speed]49.22ms/step [Loss]8.2142 [Metrics]{'train_loss:8.214163 lr:0.000382'}\n",
      "[Train Epoch]1/1 [Time]13213.77 [Step]53700 [Batch]268500 [Speed]49.21ms/step [Loss]8.2141 [Metrics]{'train_loss:8.214135 lr:0.000381'}\n",
      "[Train Epoch]1/1 [Time]13237.87 [Step]53800 [Batch]269000 [Speed]49.21ms/step [Loss]8.2141 [Metrics]{'train_loss:8.214100 lr:0.000381'}\n",
      "[Train Epoch]1/1 [Time]13262.01 [Step]53900 [Batch]269500 [Speed]49.21ms/step [Loss]8.2141 [Metrics]{'train_loss:8.214080 lr:0.000381'}\n",
      "[Train Epoch]1/1 [Time]13286.13 [Step]54000 [Batch]270000 [Speed]49.21ms/step [Loss]8.2140 [Metrics]{'train_loss:8.213984 lr:0.000380'}\n",
      "[Train Epoch]1/1 [Time]13310.25 [Step]54100 [Batch]270500 [Speed]49.21ms/step [Loss]8.2139 [Metrics]{'train_loss:8.213851 lr:0.000380'}\n",
      "[Train Epoch]1/1 [Time]13334.39 [Step]54200 [Batch]271000 [Speed]49.20ms/step [Loss]8.2139 [Metrics]{'train_loss:8.213878 lr:0.000380'}\n",
      "[Train Epoch]1/1 [Time]13358.49 [Step]54300 [Batch]271500 [Speed]49.20ms/step [Loss]8.2139 [Metrics]{'train_loss:8.213943 lr:0.000379'}\n",
      "[Train Epoch]1/1 [Time]13382.59 [Step]54400 [Batch]272000 [Speed]49.20ms/step [Loss]8.2140 [Metrics]{'train_loss:8.214002 lr:0.000379'}\n",
      "[Train Epoch]1/1 [Time]13406.71 [Step]54500 [Batch]272500 [Speed]49.20ms/step [Loss]8.2141 [Metrics]{'train_loss:8.214052 lr:0.000379'}\n",
      "[Train Epoch]1/1 [Time]13430.82 [Step]54600 [Batch]273000 [Speed]49.20ms/step [Loss]8.2139 [Metrics]{'train_loss:8.213905 lr:0.000378'}\n",
      "[Train Epoch]1/1 [Time]13454.95 [Step]54700 [Batch]273500 [Speed]49.20ms/step [Loss]8.2138 [Metrics]{'train_loss:8.213811 lr:0.000378'}\n",
      "[Train Epoch]1/1 [Time]13479.08 [Step]54800 [Batch]274000 [Speed]49.19ms/step [Loss]8.2138 [Metrics]{'train_loss:8.213760 lr:0.000378'}\n",
      "[Train Epoch]1/1 [Time]13503.21 [Step]54900 [Batch]274500 [Speed]49.19ms/step [Loss]8.2137 [Metrics]{'train_loss:8.213674 lr:0.000377'}\n",
      "Saving checkpoint for epoch 1 at step 275000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]13532.37 [Step]55000 [Batch]275000 [Speed]49.21ms/step [Loss]8.2137 [Metrics]{'train_loss:8.213662 lr:0.000377'}\n",
      "[Train Epoch]1/1 [Time]13556.45 [Step]55100 [Batch]275500 [Speed]49.21ms/step [Loss]8.2136 [Metrics]{'train_loss:8.213626 lr:0.000377'}\n",
      "[Train Epoch]1/1 [Time]13580.55 [Step]55200 [Batch]276000 [Speed]49.20ms/step [Loss]8.2135 [Metrics]{'train_loss:8.213543 lr:0.000376'}\n",
      "[Train Epoch]1/1 [Time]13604.64 [Step]55300 [Batch]276500 [Speed]49.20ms/step [Loss]8.2134 [Metrics]{'train_loss:8.213375 lr:0.000376'}\n",
      "[Train Epoch]1/1 [Time]13628.72 [Step]55400 [Batch]277000 [Speed]49.20ms/step [Loss]8.2134 [Metrics]{'train_loss:8.213362 lr:0.000376'}\n",
      "[Train Epoch]1/1 [Time]13652.83 [Step]55500 [Batch]277500 [Speed]49.20ms/step [Loss]8.2134 [Metrics]{'train_loss:8.213419 lr:0.000375'}\n",
      "[Train Epoch]1/1 [Time]13676.97 [Step]55600 [Batch]278000 [Speed]49.20ms/step [Loss]8.2134 [Metrics]{'train_loss:8.213365 lr:0.000375'}\n",
      "[Train Epoch]1/1 [Time]13701.08 [Step]55700 [Batch]278500 [Speed]49.20ms/step [Loss]8.2133 [Metrics]{'train_loss:8.213275 lr:0.000375'}\n",
      "[Train Epoch]1/1 [Time]13725.21 [Step]55800 [Batch]279000 [Speed]49.19ms/step [Loss]8.2132 [Metrics]{'train_loss:8.213177 lr:0.000374'}\n",
      "[Train Epoch]1/1 [Time]13749.32 [Step]55900 [Batch]279500 [Speed]49.19ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213130 lr:0.000374'}\n",
      "[Train Epoch]1/1 [Time]13773.42 [Step]56000 [Batch]280000 [Speed]49.19ms/step [Loss]8.2130 [Metrics]{'train_loss:8.213002 lr:0.000374'}\n",
      "[Train Epoch]1/1 [Time]13797.53 [Step]56100 [Batch]280500 [Speed]49.19ms/step [Loss]8.2129 [Metrics]{'train_loss:8.212946 lr:0.000373'}\n",
      "[Train Epoch]1/1 [Time]13821.62 [Step]56200 [Batch]281000 [Speed]49.19ms/step [Loss]8.2130 [Metrics]{'train_loss:8.213041 lr:0.000373'}\n",
      "[Train Epoch]1/1 [Time]13845.74 [Step]56300 [Batch]281500 [Speed]49.19ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213114 lr:0.000373'}\n",
      "[Train Epoch]1/1 [Time]13869.86 [Step]56400 [Batch]282000 [Speed]49.18ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213097 lr:0.000372'}\n",
      "[Train Epoch]1/1 [Time]13893.98 [Step]56500 [Batch]282500 [Speed]49.18ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213092 lr:0.000372'}\n",
      "[Train Epoch]1/1 [Time]13918.09 [Step]56600 [Batch]283000 [Speed]49.18ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213104 lr:0.000372'}\n",
      "[Train Epoch]1/1 [Time]13942.19 [Step]56700 [Batch]283500 [Speed]49.18ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213068 lr:0.000371'}\n",
      "[Train Epoch]1/1 [Time]13966.28 [Step]56800 [Batch]284000 [Speed]49.18ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213144 lr:0.000371'}\n",
      "[Train Epoch]1/1 [Time]13990.40 [Step]56900 [Batch]284500 [Speed]49.18ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213146 lr:0.000371'}\n",
      "[Train Epoch]1/1 [Time]14014.53 [Step]57000 [Batch]285000 [Speed]49.17ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213137 lr:0.000370'}\n",
      "[Train Epoch]1/1 [Time]14038.63 [Step]57100 [Batch]285500 [Speed]49.17ms/step [Loss]8.2132 [Metrics]{'train_loss:8.213204 lr:0.000370'}\n",
      "[Train Epoch]1/1 [Time]14062.71 [Step]57200 [Batch]286000 [Speed]49.17ms/step [Loss]8.2132 [Metrics]{'train_loss:8.213235 lr:0.000370'}\n",
      "[Train Epoch]1/1 [Time]14086.86 [Step]57300 [Batch]286500 [Speed]49.17ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213120 lr:0.000369'}\n",
      "[Train Epoch]1/1 [Time]14111.06 [Step]57400 [Batch]287000 [Speed]49.17ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213062 lr:0.000369'}\n",
      "[Train Epoch]1/1 [Time]14135.17 [Step]57500 [Batch]287500 [Speed]49.17ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213066 lr:0.000369'}\n",
      "[Train Epoch]1/1 [Time]14159.32 [Step]57600 [Batch]288000 [Speed]49.16ms/step [Loss]8.2130 [Metrics]{'train_loss:8.213039 lr:0.000368'}\n",
      "[Train Epoch]1/1 [Time]14183.43 [Step]57700 [Batch]288500 [Speed]49.16ms/step [Loss]8.2131 [Metrics]{'train_loss:8.213053 lr:0.000368'}\n",
      "[Train Epoch]1/1 [Time]14207.51 [Step]57800 [Batch]289000 [Speed]49.16ms/step [Loss]8.2130 [Metrics]{'train_loss:8.212968 lr:0.000368'}\n",
      "[Train Epoch]1/1 [Time]14231.63 [Step]57900 [Batch]289500 [Speed]49.16ms/step [Loss]8.2129 [Metrics]{'train_loss:8.212867 lr:0.000367'}\n",
      "[Train Epoch]1/1 [Time]14255.82 [Step]58000 [Batch]290000 [Speed]49.16ms/step [Loss]8.2128 [Metrics]{'train_loss:8.212837 lr:0.000367'}\n",
      "[Train Epoch]1/1 [Time]14279.94 [Step]58100 [Batch]290500 [Speed]49.16ms/step [Loss]8.2127 [Metrics]{'train_loss:8.212732 lr:0.000367'}\n",
      "[Train Epoch]1/1 [Time]14304.06 [Step]58200 [Batch]291000 [Speed]49.15ms/step [Loss]8.2126 [Metrics]{'train_loss:8.212623 lr:0.000366'}\n",
      "[Train Epoch]1/1 [Time]14328.17 [Step]58300 [Batch]291500 [Speed]49.15ms/step [Loss]8.2125 [Metrics]{'train_loss:8.212508 lr:0.000366'}\n",
      "[Train Epoch]1/1 [Time]14352.28 [Step]58400 [Batch]292000 [Speed]49.15ms/step [Loss]8.2124 [Metrics]{'train_loss:8.212431 lr:0.000366'}\n",
      "[Train Epoch]1/1 [Time]14376.46 [Step]58500 [Batch]292500 [Speed]49.15ms/step [Loss]8.2124 [Metrics]{'train_loss:8.212382 lr:0.000365'}\n",
      "[Train Epoch]1/1 [Time]14400.56 [Step]58600 [Batch]293000 [Speed]49.15ms/step [Loss]8.2123 [Metrics]{'train_loss:8.212284 lr:0.000365'}\n",
      "[Train Epoch]1/1 [Time]14424.65 [Step]58700 [Batch]293500 [Speed]49.15ms/step [Loss]8.2122 [Metrics]{'train_loss:8.212211 lr:0.000365'}\n",
      "[Train Epoch]1/1 [Time]14448.74 [Step]58800 [Batch]294000 [Speed]49.15ms/step [Loss]8.2122 [Metrics]{'train_loss:8.212171 lr:0.000365'}\n",
      "[Train Epoch]1/1 [Time]14472.82 [Step]58900 [Batch]294500 [Speed]49.14ms/step [Loss]8.2121 [Metrics]{'train_loss:8.212119 lr:0.000364'}\n",
      "[Train Epoch]1/1 [Time]14496.92 [Step]59000 [Batch]295000 [Speed]49.14ms/step [Loss]8.2121 [Metrics]{'train_loss:8.212123 lr:0.000364'}\n",
      "[Train Epoch]1/1 [Time]14521.09 [Step]59100 [Batch]295500 [Speed]49.14ms/step [Loss]8.2120 [Metrics]{'train_loss:8.212040 lr:0.000364'}\n",
      "[Train Epoch]1/1 [Time]14545.20 [Step]59200 [Batch]296000 [Speed]49.14ms/step [Loss]8.2119 [Metrics]{'train_loss:8.211940 lr:0.000363'}\n",
      "[Train Epoch]1/1 [Time]14569.31 [Step]59300 [Batch]296500 [Speed]49.14ms/step [Loss]8.2119 [Metrics]{'train_loss:8.211862 lr:0.000363'}\n",
      "[Train Epoch]1/1 [Time]14593.41 [Step]59400 [Batch]297000 [Speed]49.14ms/step [Loss]8.2118 [Metrics]{'train_loss:8.211825 lr:0.000363'}\n",
      "[Train Epoch]1/1 [Time]14617.49 [Step]59500 [Batch]297500 [Speed]49.13ms/step [Loss]8.2117 [Metrics]{'train_loss:8.211722 lr:0.000362'}\n",
      "[Train Epoch]1/1 [Time]14641.59 [Step]59600 [Batch]298000 [Speed]49.13ms/step [Loss]8.2117 [Metrics]{'train_loss:8.211732 lr:0.000362'}\n",
      "[Train Epoch]1/1 [Time]14665.70 [Step]59700 [Batch]298500 [Speed]49.13ms/step [Loss]8.2117 [Metrics]{'train_loss:8.211709 lr:0.000362'}\n",
      "[Train Epoch]1/1 [Time]14689.83 [Step]59800 [Batch]299000 [Speed]49.13ms/step [Loss]8.2116 [Metrics]{'train_loss:8.211607 lr:0.000361'}\n",
      "[Train Epoch]1/1 [Time]14714.00 [Step]59900 [Batch]299500 [Speed]49.13ms/step [Loss]8.2115 [Metrics]{'train_loss:8.211523 lr:0.000361'}\n",
      "Saving checkpoint for epoch 1 at step 300000 on path ../2_Models/model_bert4rec_complete_0.7/checkpoints/\n",
      "[Train Epoch]1/1 [Time]14742.30 [Step]60000 [Batch]300000 [Speed]49.14ms/step [Loss]8.2115 [Metrics]{'train_loss:8.211484 lr:0.000361'}\n",
      "[Train Epoch]1/1 [Time]14766.51 [Step]60100 [Batch]300500 [Speed]49.14ms/step [Loss]8.2115 [Metrics]{'train_loss:8.211485 lr:0.000361'}\n",
      "[Train Epoch]1/1 [Time]14790.61 [Step]60200 [Batch]301000 [Speed]49.14ms/step [Loss]8.2114 [Metrics]{'train_loss:8.211411 lr:0.000360'}\n",
      "[Train Epoch]1/1 [Time]14814.75 [Step]60300 [Batch]301500 [Speed]49.14ms/step [Loss]8.2113 [Metrics]{'train_loss:8.211264 lr:0.000360'}\n",
      "[Train Epoch]1/1 [Time]14838.84 [Step]60400 [Batch]302000 [Speed]49.14ms/step [Loss]8.2112 [Metrics]{'train_loss:8.211173 lr:0.000360'}\n",
      "[Train Epoch]1/1 [Time]14862.94 [Step]60500 [Batch]302500 [Speed]49.13ms/step [Loss]8.2110 [Metrics]{'train_loss:8.211032 lr:0.000359'}\n",
      "[Train Epoch]1/1 [Time]14887.06 [Step]60600 [Batch]303000 [Speed]49.13ms/step [Loss]8.2110 [Metrics]{'train_loss:8.210999 lr:0.000359'}\n",
      "[Train Epoch]1/1 [Time]14911.15 [Step]60700 [Batch]303500 [Speed]49.13ms/step [Loss]8.2110 [Metrics]{'train_loss:8.210977 lr:0.000359'}\n",
      "[Train Epoch]1/1 [Time]14935.24 [Step]60800 [Batch]304000 [Speed]49.13ms/step [Loss]8.2110 [Metrics]{'train_loss:8.211002 lr:0.000358'}\n",
      "[Train Epoch]1/1 [Time]14959.38 [Step]60900 [Batch]304500 [Speed]49.13ms/step [Loss]8.2110 [Metrics]{'train_loss:8.211027 lr:0.000358'}\n",
      "[Train Epoch]1/1 [Time]14983.49 [Step]61000 [Batch]305000 [Speed]49.13ms/step [Loss]8.2111 [Metrics]{'train_loss:8.211097 lr:0.000358'}\n",
      "[Train Epoch]1/1 [Time]15007.62 [Step]61100 [Batch]305500 [Speed]49.12ms/step [Loss]8.2111 [Metrics]{'train_loss:8.211105 lr:0.000358'}\n",
      "[Train Epoch]1/1 [Time]15031.73 [Step]61200 [Batch]306000 [Speed]49.12ms/step [Loss]8.2110 [Metrics]{'train_loss:8.211033 lr:0.000357'}\n",
      "[Train Epoch]1/1 [Time]15055.80 [Step]61300 [Batch]306500 [Speed]49.12ms/step [Loss]8.2109 [Metrics]{'train_loss:8.210938 lr:0.000357'}\n",
      "[Train Epoch]1/1 [Time]15079.94 [Step]61400 [Batch]307000 [Speed]49.12ms/step [Loss]8.2109 [Metrics]{'train_loss:8.210877 lr:0.000357'}\n",
      "[Train Epoch]1/1 [Time]15104.03 [Step]61500 [Batch]307500 [Speed]49.12ms/step [Loss]8.2108 [Metrics]{'train_loss:8.210822 lr:0.000356'}\n",
      "[Train Epoch]1/1 [Time]15128.13 [Step]61600 [Batch]308000 [Speed]49.12ms/step [Loss]8.2108 [Metrics]{'train_loss:8.210769 lr:0.000356'}\n",
      "[Train Epoch]1/1 [Time]15152.24 [Step]61700 [Batch]308500 [Speed]49.12ms/step [Loss]8.2108 [Metrics]{'train_loss:8.210796 lr:0.000356'}\n",
      "[Train Epoch]1/1 [Time]15176.40 [Step]61800 [Batch]309000 [Speed]49.11ms/step [Loss]8.2108 [Metrics]{'train_loss:8.210770 lr:0.000356'}\n",
      "[Train Epoch]1/1 [Time]15200.51 [Step]61900 [Batch]309500 [Speed]49.11ms/step [Loss]8.2108 [Metrics]{'train_loss:8.210785 lr:0.000355'}\n",
      "[Train Epoch]1/1 [Time]15224.61 [Step]62000 [Batch]310000 [Speed]49.11ms/step [Loss]8.2109 [Metrics]{'train_loss:8.210873 lr:0.000355'}\n",
      "[Train Epoch]1/1 [Time]15248.73 [Step]62100 [Batch]310500 [Speed]49.11ms/step [Loss]8.2109 [Metrics]{'train_loss:8.210913 lr:0.000355'}\n",
      "[Train Epoch]1/1 [Time]15272.85 [Step]62200 [Batch]311000 [Speed]49.11ms/step [Loss]8.2110 [Metrics]{'train_loss:8.211023 lr:0.000354'}\n",
      "[Train Epoch]1/1 [Time]15296.91 [Step]62300 [Batch]311500 [Speed]49.11ms/step [Loss]8.2111 [Metrics]{'train_loss:8.211094 lr:0.000354'}\n",
      "[Train Epoch]1/1 [Time]15321.01 [Step]62400 [Batch]312000 [Speed]49.11ms/step [Loss]8.2111 [Metrics]{'train_loss:8.211059 lr:0.000354'}\n",
      "[Train Epoch]1/1 [Time]15345.11 [Step]62500 [Batch]312500 [Speed]49.10ms/step [Loss]8.2110 [Metrics]{'train_loss:8.211033 lr:0.000354'}\n",
      "[Train Epoch]1/1 [Time]15369.20 [Step]62600 [Batch]313000 [Speed]49.10ms/step [Loss]8.2113 [Metrics]{'train_loss:8.211305 lr:0.000353'}\n",
      "[Train Epoch]1/1 [Time]15393.35 [Step]62700 [Batch]313500 [Speed]49.10ms/step [Loss]8.2113 [Metrics]{'train_loss:8.211284 lr:0.000353'}\n",
      "[Train Epoch]1/1 [Time]15417.48 [Step]62800 [Batch]314000 [Speed]49.10ms/step [Loss]8.2112 [Metrics]{'train_loss:8.211193 lr:0.000353'}\n",
      "[Train Epoch]1/1 [Time]15441.60 [Step]62900 [Batch]314500 [Speed]49.10ms/step [Loss]8.2111 [Metrics]{'train_loss:8.211067 lr:0.000352'}\n",
      "[Train Epoch]1/1 [Time]15465.68 [Step]63000 [Batch]315000 [Speed]49.10ms/step [Loss]8.2110 [Metrics]{'train_loss:8.211004 lr:0.000352'}\n",
      "[Train Epoch]1/1 [Time]15489.77 [Step]63100 [Batch]315500 [Speed]49.10ms/step [Loss]8.2109 [Metrics]{'train_loss:8.210936 lr:0.000352'}\n",
      "[Train Epoch]1/1 [Time]15513.90 [Step]63200 [Batch]316000 [Speed]49.09ms/step [Loss]8.2110 [Metrics]{'train_loss:8.210959 lr:0.000352'}\n",
      "[Train Epoch]1/1 [Time]15538.02 [Step]63300 [Batch]316500 [Speed]49.09ms/step [Loss]8.2109 [Metrics]{'train_loss:8.210950 lr:0.000351'}\n",
      "[Val Epoch]1/1 [Time]15538.67 [Step]1 [Batch]0 [Speed]15538669.45ms/step [Loss]7.4549 [Metrics]{'val_loss:7.454852'}\n",
      "[Val Epoch]1/1 [Time]15549.38 [Step]201 [Batch]200 [Speed]77746.90ms/step [Loss]7.3105 [Metrics]{'val_loss:7.310544'}\n",
      "[Val Epoch]1/1 [Time]15560.10 [Step]401 [Batch]400 [Speed]38900.26ms/step [Loss]7.3174 [Metrics]{'val_loss:7.317386'}\n",
      "[Val Epoch]1/1 [Time]15570.82 [Step]601 [Batch]600 [Speed]25951.36ms/step [Loss]7.3354 [Metrics]{'val_loss:7.335413'}\n",
      "[Val Epoch]1/1 [Time]15581.54 [Step]801 [Batch]800 [Speed]19476.92ms/step [Loss]7.3526 [Metrics]{'val_loss:7.352639'}\n",
      "[Val Epoch]1/1 [Time]15592.27 [Step]1001 [Batch]1000 [Speed]15592.27ms/step [Loss]7.3493 [Metrics]{'val_loss:7.349319'}\n",
      "[Val Epoch]1/1 [Time]15602.99 [Step]1201 [Batch]1200 [Speed]13002.49ms/step [Loss]7.3488 [Metrics]{'val_loss:7.348761'}\n",
      "[Val Epoch]1/1 [Time]15613.71 [Step]1401 [Batch]1400 [Speed]11152.65ms/step [Loss]7.3535 [Metrics]{'val_loss:7.353471'}\n",
      "[Val Epoch]1/1 [Time]15624.42 [Step]1601 [Batch]1600 [Speed]9765.26ms/step [Loss]7.3549 [Metrics]{'val_loss:7.354931'}\n",
      "[Val Epoch]1/1 [Time]15635.13 [Step]1801 [Batch]1800 [Speed]8686.18ms/step [Loss]7.3538 [Metrics]{'val_loss:7.353800'}\n",
      "[Val Epoch]1/1 [Time]15645.84 [Step]2001 [Batch]2000 [Speed]7822.92ms/step [Loss]7.3575 [Metrics]{'val_loss:7.357495'}\n",
      "[Val Epoch]1/1 [Time]15656.56 [Step]2201 [Batch]2200 [Speed]7116.62ms/step [Loss]7.3593 [Metrics]{'val_loss:7.359302'}\n",
      "[Val Epoch]1/1 [Time]15667.29 [Step]2401 [Batch]2400 [Speed]6528.04ms/step [Loss]7.3626 [Metrics]{'val_loss:7.362577'}\n",
      "[Val Epoch]1/1 [Time]15678.01 [Step]2601 [Batch]2600 [Speed]6030.00ms/step [Loss]7.3701 [Metrics]{'val_loss:7.370117'}\n",
      "[Val Epoch]1/1 [Time]15688.73 [Step]2801 [Batch]2800 [Speed]5603.12ms/step [Loss]7.3743 [Metrics]{'val_loss:7.374296'}\n",
      "[Val Epoch]1/1 [Time]15699.47 [Step]3001 [Batch]3000 [Speed]5233.16ms/step [Loss]7.3763 [Metrics]{'val_loss:7.376328'}\n",
      "[Val Epoch]1/1 [Time]15710.21 [Step]3201 [Batch]3200 [Speed]4909.44ms/step [Loss]7.3767 [Metrics]{'val_loss:7.376678'}\n",
      "[Val Epoch]1/1 [Time]15720.93 [Step]3401 [Batch]3400 [Speed]4623.80ms/step [Loss]7.3720 [Metrics]{'val_loss:7.371974'}\n",
      "[Val Epoch]1/1 [Time]15731.63 [Step]3601 [Batch]3600 [Speed]4369.90ms/step [Loss]7.3706 [Metrics]{'val_loss:7.370556'}\n",
      "[Val Epoch]1/1 [Time]15742.35 [Step]3801 [Batch]3800 [Speed]4142.72ms/step [Loss]7.3723 [Metrics]{'val_loss:7.372260'}\n",
      "[Val Epoch]1/1 [Time]15753.08 [Step]4001 [Batch]4000 [Speed]3938.27ms/step [Loss]7.3708 [Metrics]{'val_loss:7.370761'}\n",
      "[Val Epoch]1/1 [Time]15763.80 [Step]4201 [Batch]4200 [Speed]3753.29ms/step [Loss]7.3715 [Metrics]{'val_loss:7.371481'}\n",
      "[Val Epoch]1/1 [Time]15774.52 [Step]4401 [Batch]4400 [Speed]3585.12ms/step [Loss]7.3693 [Metrics]{'val_loss:7.369298'}\n",
      "[Val Epoch]1/1 [Time]15785.23 [Step]4601 [Batch]4600 [Speed]3431.57ms/step [Loss]7.3704 [Metrics]{'val_loss:7.370442'}\n",
      "[Val Epoch]1/1 [Time]15795.94 [Step]4801 [Batch]4800 [Speed]3290.82ms/step [Loss]7.3713 [Metrics]{'val_loss:7.371311'}\n",
      "[Val Epoch]1/1 [Time]15806.66 [Step]5001 [Batch]5000 [Speed]3161.33ms/step [Loss]7.3720 [Metrics]{'val_loss:7.372029'}\n",
      "[Val Epoch]1/1 [Time]15817.38 [Step]5201 [Batch]5200 [Speed]3041.80ms/step [Loss]7.3736 [Metrics]{'val_loss:7.373629'}\n",
      "[Val Epoch]1/1 [Time]15828.13 [Step]5401 [Batch]5400 [Speed]2931.14ms/step [Loss]7.3745 [Metrics]{'val_loss:7.374526'}\n",
      "[Val Epoch]1/1 [Time]15840.08 [Step]5601 [Batch]5600 [Speed]2828.59ms/step [Loss]7.3731 [Metrics]{'val_loss:7.373068'}\n",
      "[Val Epoch]1/1 [Time]15852.14 [Step]5801 [Batch]5800 [Speed]2733.13ms/step [Loss]7.3722 [Metrics]{'val_loss:7.372221'}\n",
      "[Val Epoch]1/1 [Time]15864.21 [Step]6001 [Batch]6000 [Speed]2644.03ms/step [Loss]7.3717 [Metrics]{'val_loss:7.371718'}\n",
      "[Val Epoch]1/1 [Time]15876.43 [Step]6201 [Batch]6200 [Speed]2560.71ms/step [Loss]7.3705 [Metrics]{'val_loss:7.370469'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m val_batch_num, val_batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(val_dataloader):\n\u001b[1;32m    126\u001b[0m     inputs, target \u001b[39m=\u001b[39m val_batch_data\n\u001b[0;32m--> 127\u001b[0m     test_step(inputs, target\u001b[39m=\u001b[39;49mtarget, loss\u001b[39m=\u001b[39;49mval_loss)\n\u001b[1;32m    128\u001b[0m     val_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m val_batch_num \u001b[39m%\u001b[39m BERT4REC_CONFIG\u001b[39m.\u001b[39mbatch_num_printer_val \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '1_Model_v0.4.ipynb'\n",
    "\n",
    "class BERT4REC_CONFIG:\n",
    "    num_items = NUM_ITEMS\n",
    "    path_tfrecords = '../tfrecords/tfrecords_v0.4/'\n",
    "    restore_last_chekpoint = (False, 'model_bert4rec_complete_0.7/checkpoints/', 'ckpt-14')\n",
    "    model_name = 'model_bert4rec_complete_0.7.1'\n",
    "    checkpoint_filepath = f'../2_Models/'\n",
    "    num_records_dataset = 10_000_000\n",
    "    batch_size = 32\n",
    "    num_grad_accum_steps = 5\n",
    "    seq_len = 20\n",
    "    mask_prob = 0.4\n",
    "    reverse_prob = 0.25\n",
    "    emb_dim = 128\n",
    "    trf_dim = 128\n",
    "    num_heads = 4\n",
    "    num_layers = 1\n",
    "    ff_dim = trf_dim*4\n",
    "    drop_rate = 0.1\n",
    "    att_drop_rate = 0.1\n",
    "    epochs = 1\n",
    "    early_stopping = 5\n",
    "    batch_num_printer_train = 500\n",
    "    batch_num_printer_val = 200\n",
    "    clipnorm = 1\n",
    "    num_iters_save_checkpoint = 5_000 * num_grad_accum_steps\n",
    "    scheduler_scaler = 128 \n",
    "    warmup_steps = 10_000\n",
    "    log_wandb = True\n",
    "\n",
    "if BERT4REC_CONFIG.log_wandb:\n",
    "    time_suffix = datetime.now().__str__().split('.')[0]\n",
    "    dict_config = {k : v for k, v in zip(BERT4REC_CONFIG.__dict__.keys(), BERT4REC_CONFIG.__dict__.values()) if not k.startswith('__')}\n",
    "    init_wandb(wandb_project='otto-recsys', entity='enric1296', run_name=f'{BERT4REC_CONFIG.model_name}_{time_suffix}', dict_config=dict_config)\n",
    "    \n",
    "\n",
    "list_paths_train = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=train/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=train')]\n",
    "np.random.shuffle(list_paths_train)\n",
    "list_paths_val = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=val/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=val')]\n",
    "\n",
    "train_dataloader = Bert4RecDataLoader(list_paths_train, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len, \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=BERT4REC_CONFIG.mask_prob, \n",
    "                                     reverse_prob=BERT4REC_CONFIG.reverse_prob, \n",
    "                                     is_test=False,\n",
    "                                     is_val=False,\n",
    "                                     shuffle=True,\n",
    "                                     drop_remainder=True).get_generator()\n",
    "\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len,  \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     get_session=False,\n",
    "                                     is_val=True,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "# model = tf.keras.models.load_model(f'../2_Models/seq_len{BERT4REC_CONFIG.seq_len}_{BERT4REC_CONFIG.restore_last_chekpoint[1]}/', compile=False)\n",
    "optimizer = optimizers.Adam(learning_rate=CustomSchedule(BERT4REC_CONFIG.scheduler_scaler, \n",
    "                            warmup_steps=BERT4REC_CONFIG.warmup_steps),\n",
    "                            clipnorm=BERT4REC_CONFIG.clipnorm)\n",
    "# optimizer = AdamW(learning_rate=CustomSchedule(BERT4REC_CONFIG.scheduler_scaler, \n",
    "#                     warmup_steps=BERT4REC_CONFIG.warmup_steps),\n",
    "#                     clipnorm=BERT4REC_CONFIG.clipnorm,\n",
    "#                     weight_decay=1e-4)                            \n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)                           \n",
    "                            \n",
    "# Build utils\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "if BERT4REC_CONFIG.restore_last_chekpoint[0]:\n",
    "    checkpoint_path = os.path.join(BERT4REC_CONFIG.checkpoint_filepath, BERT4REC_CONFIG.restore_last_chekpoint[1])\n",
    "    ckpt.restore(os.path.join(checkpoint_path, BERT4REC_CONFIG.restore_last_chekpoint[2]))\n",
    "    print('Latest checkpoint restored!!')\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n",
    "else:\n",
    "    checkpoint_path = create_folder_with_version(BERT4REC_CONFIG.model_name, BERT4REC_CONFIG.checkpoint_filepath)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, os.path.join(BERT4REC_CONFIG.checkpoint_filepath, checkpoint_path, 'checkpoints'), \n",
    "                                            max_to_keep=10)\n",
    "\n",
    "# Loss function\n",
    "loss_function = custom_loss_bert4rec()\n",
    "recall_function = recall_top_k(top_k=20, seq_len=BERT4REC_CONFIG.seq_len)\n",
    "\n",
    "# Trackers\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "train_recall_k = tf.keras.metrics.Mean(name='train_recall_k')\n",
    "val_recall_k = tf.keras.metrics.Mean(name='val_recall_k')\n",
    "\n",
    "##############################################\n",
    "\n",
    "global_gradients = []\n",
    "total_step, val_step = 0, 0\n",
    "for epoch in range(BERT4REC_CONFIG.epochs):\n",
    "    start = time.time()\n",
    "    print('===='*20)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    metrics_reset_states(train_loss, val_loss, train_recall_k, val_recall_k)\n",
    "    \n",
    "    for batch_num, batch_data in enumerate(train_dataloader):\n",
    "        inputs, target = batch_data\n",
    "        step_gradients = train_step(inputs, target=target, loss=train_loss, num_accum_steps=BERT4REC_CONFIG.num_grad_accum_steps)\n",
    "        global_gradients = backward_optimization(BERT4REC_CONFIG.num_grad_accum_steps, global_gradients, step_gradients, total_step, model, optimizer)\n",
    "        if batch_num % BERT4REC_CONFIG.batch_num_printer_train == 0:\n",
    "            train_dict_metrics = {x.name : x.result() for x in [train_loss, train_recall_k]}\n",
    "            train_dict_metrics.update({'lr' : optimizer.lr(total_step//BERT4REC_CONFIG.num_grad_accum_steps).numpy().astype(np.float32)})\n",
    "            fancy_printer(train_loss, epoch, batch_num, start, step='Train', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=train_dict_metrics, num_step=total_step // BERT4REC_CONFIG.num_grad_accum_steps)\n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                train_dict_metrics.update({'step_grad' : total_step//BERT4REC_CONFIG.num_grad_accum_steps, 'step' : total_step})\n",
    "                log_wandb_metrics(step='train', num_step=total_step, gradients=global_gradients, dict_metrics=train_dict_metrics)     \n",
    "        total_step += 1  \n",
    "        if total_step % BERT4REC_CONFIG.num_iters_save_checkpoint==0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print(f'Saving checkpoint for epoch {epoch+1} at step {total_step} on path {checkpoint_path}')        \n",
    "            \n",
    "    for val_batch_num, val_batch_data in enumerate(val_dataloader):\n",
    "        inputs, target = val_batch_data\n",
    "        test_step(inputs, target=target, loss=val_loss)\n",
    "        val_step += 1\n",
    "        if val_batch_num % BERT4REC_CONFIG.batch_num_printer_val == 0:\n",
    "            val_dict_metrics = {x.name : x.result() for x in [val_loss, val_recall_k]}\n",
    "            fancy_printer(val_loss, epoch, val_batch_num, start, step='Val', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=val_dict_metrics, num_step=val_step)    \n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                log_wandb_metrics(step='val', num_step=val_step, dict_metrics=val_dict_metrics) \n",
    "                # if val_batch_num==0:\n",
    "                #     log_wandb_metrics(step=None, plot_image=True, \n",
    "                #                       model=model, inputs=inputs, epoch=epoch, target=target, stats=stats)\n",
    "    \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {checkpoint_path}')        \n",
    "    \n",
    "    epoch_dict_metrics = {x.name : x.result() for x in [train_loss, val_loss, train_recall_k, val_recall_k]}\n",
    "    printer = fancy_printer(None, epoch, epoch, start, step='epoch', num_step=epoch, dict_metrics=epoch_dict_metrics, \n",
    "                            train_loss=train_loss, val_loss=val_loss)\n",
    "    if BERT4REC_CONFIG.log_wandb:\n",
    "        log_wandb_metrics(step='epoch', num_step=total_step, dict_metrics=epoch_dict_metrics)\n",
    "\n",
    "if BERT4REC_CONFIG.log_wandb:\n",
    "    # wandb.save(checkpoint_path)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "361it [00:49,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.loss_scale.current_loss_scale\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.loss_scale.good_steps\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.embed_items.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.embed_type.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.mlp_proj_time_encoding.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.mlp_proj_time_encoding.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.mlp_proj_time_encoding.layer_with_weights-1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.mlp_proj_time_encoding.layer_with_weights-1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.mlp_proj_conts.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.mlp_proj_conts.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.mlp_proj_conts.layer_with_weights-1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.mlp_proj_conts.layer_with_weights-1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.layernorm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.layernorm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.layernorm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.layernorm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.att._query_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.att._query_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.att._key_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.att._key_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.att._value_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.att._value_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.att._output_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.att._output_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.ffn.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.ffn.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.ffn.layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).model.list_transformer_block.0.ffn.layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.embed_items.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.embed_type.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.mlp_proj_time_encoding.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.mlp_proj_time_encoding.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.mlp_proj_time_encoding.layer_with_weights-1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.mlp_proj_time_encoding.layer_with_weights-1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.mlp_proj_conts.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.mlp_proj_conts.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.mlp_proj_conts.layer_with_weights-1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.mlp_proj_conts.layer_with_weights-1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.layernorm1.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.layernorm1.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.layernorm2.gamma\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.layernorm2.beta\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.att._query_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.att._query_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.att._key_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.att._key_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.att._value_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.att._value_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.att._output_dense.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.att._output_dense.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.ffn.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.ffn.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.ffn.layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).model.list_transformer_block.0.ffn.layer_with_weights-1.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:20,  7.11it/s]\n",
      "100%|██████████| 192192/192192 [00:00<00:00, 232582.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.921920e+05</td>\n",
       "      <td>81269.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.373092e+06</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.720560e+06</td>\n",
       "      <td>0.428915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.129786e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.354874e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.572168e+06</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.289973e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            session         score\n",
       "count  1.921920e+05  81269.000000\n",
       "mean   6.373092e+06      0.262000\n",
       "std    3.720560e+06      0.428915\n",
       "min    1.600000e+01      0.000000\n",
       "25%    3.129786e+06      0.000000\n",
       "50%    6.354874e+06      0.000000\n",
       "75%    9.572168e+06      0.500000\n",
       "max    1.289973e+07      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'carts': 0.33366857321771026,\n",
       " 'clicks': 0.22240758913967942,\n",
       " 'orders': 0.49849532489193765}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Metric: 0.4214\n"
     ]
    }
   ],
   "source": [
    "def get_score_metric(y_true, y_pred, type_target, k=20):\n",
    "    score = 0\n",
    "    if len(y_true)==0:\n",
    "        return None\n",
    "    if type_target=='clicks':\n",
    "        num_targets = 1\n",
    "        hits = len([x for x in y_pred if x==y_true[0]])\n",
    "    else:\n",
    "        num_targets = min(k, len(y_true))\n",
    "        hits = len([x for x in y_pred if x in y_true])\n",
    "    score = hits / num_targets\n",
    "    return score\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.7/checkpoints'))\n",
    "# model = models.load_model('../2_Models/seq_len10_model_bert4rec_complete_v0.4_finetuned/', compile=False)\n",
    "list_paths_val = ['../tfrecords/tfrecords_v0.4/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=val')]\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20, \n",
    "                                     seq_len_target=20, \n",
    "                                     batch_size=64, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0, \n",
    "                                     is_val=True,\n",
    "                                     get_session=True, \n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "list_sessions, list_predictions, list_trues, list_types = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(val_dataloader)):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    target, type_target, idx_mask = targets\n",
    "    idxs = idx_mask.numpy() #tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[x for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        labels = [list(set([_target for _type, _target in zip(type_target.numpy()[i], target.numpy()[i]) if dict_map_type[type_]==_type and _target!=0])) for i in range(target.shape[0])]\n",
    "        ###\n",
    "        list_sessions.append(session.numpy())\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_trues = list_trues + labels\n",
    "    if num_batch==1_000:\n",
    "        break\n",
    "\n",
    "df_val = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'trues' : list_trues,\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_val['score'] = df_val.progress_apply(lambda x: get_score_metric(x['trues'], x['predictions'], x['type']), axis=1)\n",
    "\n",
    "display(df_val.describe())\n",
    "dict_scores = df_val.groupby('type')['score'].mean().to_dict()\n",
    "display(dict_scores)\n",
    "kaggle_metric = 0.1*dict_scores['clicks'] + 0.3*dict_scores['carts'] + 0.6*dict_scores['orders']\n",
    "print(f'Kaggle Metric: {kaggle_metric:.4f}')\n",
    "\n",
    "\n",
    "# (seq_len=20)model_bert4rec_complete_0.7 - ckpt14\n",
    "# {'carts': 0.3439392821182184,\n",
    "#  'clicks': 0.23017664376840039,\n",
    "#  'orders': 0.5087165589251029}\n",
    "# Kaggle Metric: 0.4314\n",
    "\n",
    "# (seq_len=20)model_bert4rec_complete_0.7 - ckpt27\n",
    "# {'carts': 0.3470019827927542,\n",
    "#  'clicks': 0.23410206084396468,\n",
    "#  'orders': 0.514586102958196}\n",
    "# Kaggle Metric: 0.4363\n",
    "\n",
    "# import wandb\n",
    "# api = wandb.Api()\n",
    "\n",
    "# run = api.run(\"<path to run>\")\n",
    "# run.summary[\"kaggle_metric\"] = metric\n",
    "# run.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f45ccb37ed492fa4febf17b0d7c976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666824068333123, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/enric/SSD1TB/KAGGLE/025_Kaggle-OTTO Recsys-2022/1_Scripts/wandb/run-20221120_190041-3nxxmndm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/enric1296/otto-recsys/runs/3nxxmndm\" target=\"_blank\">model_bert4rec_complete_0.7_finetune_fold_0</a></strong> to <a href=\"https://wandb.ai/enric1296/otto-recsys\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Fold: 0\n",
      "========================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enric/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 167903104 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4033/Unknown - 481s 119ms/step - loss: 7.8932 - recall_20: 0.3776"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 51\u001b[0m\n\u001b[1;32m     45\u001b[0m ckpt\u001b[39m.\u001b[39mrestore(tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mlatest_checkpoint(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../2_Models/model_bert4rec_complete_0.7/checkpoints\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     46\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m4e-5\u001b[39m, \n\u001b[1;32m     47\u001b[0m                                         clipnorm\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m),\n\u001b[1;32m     48\u001b[0m               loss\u001b[39m=\u001b[39mloss_function,\n\u001b[1;32m     49\u001b[0m               metrics\u001b[39m=\u001b[39m[recall_function])\n\u001b[0;32m---> 51\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataloader,\n\u001b[1;32m     52\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m     53\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     54\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[WandbCallback()],\n\u001b[1;32m     55\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     56\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     58\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../2_Models/model_bert4rec_complete_0.7_finetuned_fold_\u001b[39m\u001b[39m{\u001b[39;00mnum_fold\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)                   \n\u001b[1;32m     59\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "list_paths = ['../tfrecords/tfrecords_v0.4/na_split=test_aug/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=test_aug')]# + \\\n",
    "            #  ['../tfrecords/tfrecords_v0.4/na_split=val_aug/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=val_aug')] \n",
    "np.random.shuffle(list_paths)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for num_fold, (train_idxs, val_idxs) in enumerate(kfold.split(list_paths)):\n",
    "    train_paths = np.asarray(list_paths)[train_idxs]\n",
    "    val_paths = np.asarray(list_paths)[val_idxs]\n",
    "    if BERT4REC_CONFIG.log_wandb:\n",
    "        time_suffix = datetime.now().__str__().split('.')[0]\n",
    "        dict_config = {k : v for k, v in zip(BERT4REC_CONFIG.__dict__.keys(), BERT4REC_CONFIG.__dict__.values()) if not k.startswith('__')}\n",
    "        init_wandb(wandb_project='otto-recsys', entity='enric1296', run_name=f'{BERT4REC_CONFIG.model_name}_finetune_fold_{num_fold}', dict_config=dict_config)\n",
    "    print('===='*30)\n",
    "    print(f'Fold: {num_fold}')\n",
    "    print('===='*30)\n",
    "\n",
    "    train_dataloader = Bert4RecDataLoader(train_paths, \n",
    "                                         num_items=NUM_ITEMS, \n",
    "                                        seq_len=20,  \n",
    "                                        batch_size=32, \n",
    "                                        mask_prob=0.35, \n",
    "                                        reverse_prob=0.25,  \n",
    "                                        is_val=False,\n",
    "                                        is_test=False,\n",
    "                                        get_session=False,\n",
    "                                        shuffle=True).get_generator()\n",
    "\n",
    "    val_dataloader = Bert4RecDataLoader(val_paths, \n",
    "                                        num_items=NUM_ITEMS, \n",
    "                                        seq_len=20,  \n",
    "                                        batch_size=32, \n",
    "                                        mask_prob=0.35, \n",
    "                                        reverse_prob=0.25,  \n",
    "                                        is_val=True,\n",
    "                                        is_test=False,\n",
    "                                        get_session=False,\n",
    "                                        shuffle=False).get_generator()\n",
    "\n",
    "    loss_function = custom_loss_bert4rec()\n",
    "    recall_function = recall_top_k(top_k=20, seq_len=BERT4REC_CONFIG.seq_len)\n",
    "    model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.7/checkpoints'))\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=4e-5, \n",
    "                                            clipnorm=1.0, weight_decay=1e-4),\n",
    "                  loss=loss_function,\n",
    "                  metrics=[recall_function])\n",
    "\n",
    "    history = model.fit(train_dataloader,\n",
    "                        validation_data=val_dataloader,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[WandbCallback()],\n",
    "                        epochs=1,\n",
    "                        verbose=1)\n",
    "\n",
    "    model.save(f'../2_Models/model_bert4rec_complete_0.7_finetuned_fold_{num_fold}/')                   \n",
    "    wandb.finish()\n",
    "\n",
    "# 173/Unknown - 22s 113ms/step - loss: 7.9368 - recall_20: 0.3452\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 19:15:23.433225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "0it [00:00, ?it/s]2022-11-20 19:15:24.337587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "26122it [55:08,  7.90it/s]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.7/checkpoints'))\n",
    "\n",
    "\n",
    "list_paths_test = ['../tfrecords/tfrecords_v0.4/na_split=test/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=test')]\n",
    "test_dataloader = Bert4RecDataLoader(list_paths_test, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20,  \n",
    "                                     batch_size=64, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     is_val=False,\n",
    "                                     is_test=True,\n",
    "                                     get_session=True,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "list_predictions, list_sessions, list_types, list_scores = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(test_dataloader)):\n",
    "    features, idxs, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    idxs = idxs.numpy()\n",
    "    # idxs = tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    ###\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[dict_map[x] for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        topk_idxs = topk_idxs - 1\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_sessions.append(session.numpy())\n",
    "    # if num_batch==100:\n",
    "    #     break\n",
    "    \n",
    "\n",
    "# 26122it [54:28,  7.99it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_submission = f\"submission_{datetime.now().__str__().split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_')}\"\n",
    "\n",
    "df_inference = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_inference['session_type'] = df_inference['session'].astype(str) + '_' + df_inference['type']\n",
    "df_inference['labels'] = df_inference['predictions'].apply(lambda x : ' '.join([str(y) for y in x]))\n",
    "df_inference[['session_type', 'labels']].to_csv(f'../3_Submissions/{name_submission}.csv', index=False)\n",
    "\n",
    "print(df_inference.shape)\n",
    "display(\n",
    "    df_inference\n",
    ")\n",
    "\n",
    "import gzip\n",
    "with open(f'../3_Submissions/{name_submission}.csv', 'rb') as f_in, gzip.open(f'../3_Submissions/{name_submission}.csv.gz', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0432fa0070c5c9f7d9e158f590013ccc765eb84f02e6f69521746370c3bf6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
