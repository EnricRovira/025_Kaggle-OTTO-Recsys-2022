{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 23:56:54.639575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:56:54.695346: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-23 23:56:54.710541: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 23:56:54.967235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-11-23 23:56:54.967262: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-11-23 23:56:54.967264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 23:56:55.254487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 23:56:55.267921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 23:56:55.267995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Libraries #\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, models, metrics, optimizers, constraints\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import Sequence\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# tfrecords for kaggle\n",
    "\n",
    "# name_dataset = 'tfrecords_v0.4_kaggle'\n",
    "# path_out = f'../tfrecords/{name_dataset}/'\n",
    "\n",
    "# if not os.path.exists(path_out):\n",
    "#     os.mkdir(path_out)\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_train'):\n",
    "#     os.rename(path_out + 'na_split_train/' + file, \n",
    "#               path_out + 'na_split_train/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_val'):\n",
    "#     os.rename(path_out + 'na_split_val/' + file, \n",
    "#               path_out + 'na_split_val/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_test'):\n",
    "#     os.rename(path_out + 'na_split_test/' + file, \n",
    "#               path_out + 'na_split_test/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_val_aug'):\n",
    "#     os.rename(path_out + 'na_split_val_aug/' + file, \n",
    "#               path_out + 'na_split_val_aug/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_test_aug'):\n",
    "#     os.rename(path_out + 'na_split_test_aug/' + file, \n",
    "#               path_out + 'na_split_test_aug/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1855603/1855603 [00:00<00:00, 8056122.01it/s]\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Paths & Global Variables\n",
    "\n",
    "# Train: (datetime.datetime(2022, 7, 31, 22, 0, 0, 25000), datetime.datetime(2022, 8, 28, 21, 59, 59, 984000))\n",
    "# Test: (datetime.datetime(2022, 8, 28, 22, 0, 0, 278000), datetime.datetime(2022, 9, 4, 21, 59, 51, 563000))\n",
    "\n",
    "path_data_raw = '../0_Data/'\n",
    "\n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "df_mapping = pd.read_csv('../tfrecords/tfrecords_v0.4/df_mapping.csv')\n",
    "NUM_ITEMS = len(df_mapping['aid_map'].unique())\n",
    "print(NUM_ITEMS)\n",
    "\n",
    "dict_map = {}\n",
    "for x in tqdm(df_mapping.to_dict('records')):\n",
    "    dict_map[x['aid_map']] = x['aid']\n",
    "\n",
    "dict_map_type = {\n",
    "    'clicks' : 1,\n",
    "    'carts' : 2,\n",
    "    'orders' : 3\n",
    "  }\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert4RecDataLoader:\n",
    "    \"\"\"\n",
    "    Class that iterates over tfrecords in order to get the sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_paths, num_items, seq_len, batch_size, num_targets=-1, mask_prob=0.4, \n",
    "                 reverse_prob=0.2, get_session=False, get_only_first_on_val=False, seq_len_target=None,\n",
    "                 min_size_seq_to_mask=2, is_val=False, is_test=False, avoid_repeats=False, shuffle=False, drop_remainder=False):\n",
    "        self.list_paths = list_paths\n",
    "        self.num_items = num_items\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_targets = num_targets\n",
    "        self.mask_prob = mask_prob\n",
    "        self.reverse_prob = tf.constant(reverse_prob)\n",
    "        self.shuffle = shuffle\n",
    "        self.min_size_seq_to_mask = min_size_seq_to_mask\n",
    "        self.avoid_repeats = avoid_repeats\n",
    "        self.get_session = get_session\n",
    "        self.seq_len_target = seq_len if not seq_len_target else seq_len_target\n",
    "        self.get_only_first_on_val = get_only_first_on_val\n",
    "        self.is_val = is_val\n",
    "        self.is_test = is_test\n",
    "        self.drop_remainder = drop_remainder\n",
    "\n",
    "    def get_generator(self):\n",
    "        dataset = tf.data.TFRecordDataset(self.list_paths, num_parallel_reads=AUTO, compression_type='GZIP')\n",
    "        dataset = dataset.map(self.parse_tf_record, num_parallel_calls=AUTO)\n",
    "        if self.is_val:\n",
    "            dataset = dataset.map(self.make_transforms_val, num_parallel_calls=AUTO)\n",
    "        elif self.is_test:\n",
    "            dataset = dataset.map(self.make_transforms_test, num_parallel_calls=AUTO)\n",
    "        else:\n",
    "            dataset = dataset.map(self.make_transforms_train, num_parallel_calls=AUTO)\n",
    "        \n",
    "        dataset = dataset.map(self.set_shapes, num_parallel_calls=AUTO)\n",
    "        # dataset = dataset.map(self.normalize_features, num_parallel_calls=AUTO)\n",
    "        if self.shuffle:\n",
    "            dataset = dataset.shuffle(self.batch_size*50, reshuffle_each_iteration=True)\n",
    "\n",
    "        dataset = dataset.batch(self.batch_size, num_parallel_calls=AUTO, drop_remainder=self.drop_remainder).prefetch(AUTO)\n",
    "        return dataset\n",
    "\n",
    "    def parse_tf_record(self, data):\n",
    "        features_context = {\n",
    "             \"session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "             \"size_session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        if not self.is_val:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False),\n",
    "                \"seq_recency_aid\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        else:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_aid_target\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type_target\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False),\n",
    "                \"seq_recency_aid\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        data_context, data_sequence = tf.io.parse_single_sequence_example(data, context_features=features_context, sequence_features=features_seq)\n",
    "        return data_context, data_sequence\n",
    "\n",
    "    def pad_sequence(self, seq_to_pad, maxlen, return_pad_mask=False, dtype=tf.float32):\n",
    "        length, num_feats = tf.shape(seq_to_pad)[0], tf.shape(seq_to_pad)[-1]\n",
    "        ###\n",
    "        if length < maxlen:\n",
    "            pad = tf.zeros((maxlen - length, num_feats), dtype)\n",
    "            seq = tf.concat([seq_to_pad, pad], axis=0)\n",
    "            pad_mask = tf.concat([tf.ones(tf.shape(seq_to_pad), dtype=seq_to_pad.dtype), \n",
    "                                 pad], axis=0)\n",
    "        else:\n",
    "            seq = seq_to_pad[-maxlen:, :]\n",
    "            pad_mask = tf.ones((maxlen, tf.shape(seq_to_pad)[-1]), dtype=seq_to_pad.dtype)\n",
    "        if return_pad_mask:\n",
    "            return seq, pad_mask\n",
    "        return seq \n",
    "\n",
    "    def make_transforms_val(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        seq_items_target_raw, seq_type_target_raw =  dict_sequences['seq_aid_target'], dict_sequences['seq_type_target']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ###\n",
    "        # Build target\n",
    "        seq_items, seq_target = seq_items, seq_items_target_raw[:1] if not self.get_session else seq_items_target_raw[:self.seq_len_target]\n",
    "        seq_type, seq_type_target = seq_type, seq_type_target_raw[:1] if not self.get_session else seq_type_target_raw[:self.seq_len_target]\n",
    "        seq_items_target = tf.concat([seq_items, seq_target], axis=0)\n",
    "        seq_type_target = tf.concat([seq_type, seq_type_target], axis=0)\n",
    "        ###\n",
    "        #Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, seq_type_target[:1]], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        seq_recency = tf.concat([seq_recency, tf.zeros((1, tf.shape(seq_recency)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        idx_masked = tf.clip_by_value(tf.shape(seq_items)[0], 0, self.seq_len-1)\n",
    "        seq_items, _ = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_items_target = self.pad_sequence(seq_items_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "        seq_type_target = self.pad_sequence(seq_type_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)\n",
    "        \n",
    "        if self.get_session:\n",
    "            seq_items_target_all = self.pad_sequence(seq_items_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "            seq_type_target_all = self.pad_sequence(seq_type_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64) \n",
    "            return (seq_items, seq_type, seq_time_encoding, seq_recency), (seq_items_target_all[:, 0], seq_type_target_all[:, 0], idx_masked), session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), seq_items_target[:, 0]\n",
    "\n",
    "    def make_transforms_test(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ###\n",
    "        seq_items = seq_items[-self.seq_len:, :]\n",
    "        seq_type = seq_type[-self.seq_len:, :]\n",
    "        seq_time_encoding = seq_time_encoding[-self.seq_len:, :]\n",
    "        seq_recency = seq_recency[-self.seq_len:, :]\n",
    "        idx_masked = tf.clip_by_value(tf.shape(seq_items)[0], 0, self.seq_len-1)\n",
    "        # Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, tf.zeros((1, tf.shape(seq_type)[1]), tf.int64)], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        seq_recency = tf.concat([seq_recency, tf.zeros((1, tf.shape(seq_recency)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        seq_items, _ = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)   \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)   \n",
    "        if self.get_session:\n",
    "            return (seq_items, seq_type, seq_time_encoding, seq_recency), idx_masked, session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), idx_masked\n",
    "\n",
    "  \n",
    "    def make_transforms_train(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding, seq_recency =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding'], dict_sequences['seq_recency_aid']\n",
    "        qt_size_seq = dict_context['size_session']\n",
    "        seq_recency = self.normalize_features(seq_recency)\n",
    "        ### \n",
    "        # With prob reverse\n",
    "        if tf.random.uniform(shape=(1,1)) <= self.reverse_prob:\n",
    "            seq_items = tf.reverse(seq_items, axis=[0])\n",
    "            seq_type = tf.reverse(seq_type, axis=[0])\n",
    "            seq_time_encoding = tf.reverse(seq_time_encoding, axis=[0])\n",
    "            seq_recency = tf.reverse(seq_recency, axis=[0])\n",
    "            \n",
    "        # If our seq is longer than seq_len we can use it for data augmentation purpose \n",
    "        # and select a random idx to begin with.\n",
    "        if tf.shape(seq_items)[0] > self.seq_len:\n",
    "            idx_list = tf.range(tf.shape(seq_items)[0]-self.seq_len) \n",
    "            rand_idx = tf.random.shuffle(idx_list)[0]\n",
    "            seq_items = seq_items[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_type = seq_type[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_time_encoding = seq_time_encoding[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_recency = seq_recency[rand_idx:(rand_idx+self.seq_len), :]\n",
    "        \n",
    "        qt_size_seq = tf.shape(seq_items)[0]\n",
    "\n",
    "        ## Get idxs to mask for inputs and targets\n",
    "        probs = tf.random.uniform(shape=(qt_size_seq,), minval=0, maxval=1)\n",
    "        idxs_inputs = tf.cast(tf.where(probs >= (1-self.mask_prob)), tf.int64) # -> we mask to zero the inputs as we dont want to leak \n",
    "        idxs_target = tf.cast(tf.where(probs < (1-self.mask_prob)), tf.int64) # -> we mask to zero the targets as the loss will only be applied on non zero\n",
    "\n",
    "        # If all items are masked we leave an item unmasked\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.cast(qt_size_seq, tf.int64):\n",
    "            idxs_target = idxs_inputs[-1:]\n",
    "            idxs_inputs = idxs_inputs[:-1]\n",
    "            \n",
    "        # If no item has been masked we leave at least one item masked(be careful of size=1 seqs)\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.constant(0, dtype=tf.int64):\n",
    "            all_idxs = tf.cast(tf.random.shuffle(tf.range(0, qt_size_seq)), dtype=tf.int64)\n",
    "            idxs_inputs = all_idxs[:1][:, tf.newaxis]\n",
    "            idxs_target = all_idxs[1:][:, tf.newaxis]\n",
    "\n",
    "        # Mask inputs and targets\n",
    "        seq_items_raw = seq_items\n",
    "        updates_items = tf.zeros((len(idxs_inputs), seq_items.shape[-1]), tf.int64)\n",
    "        # updates_type = tf.zeros((len(idxs_inputs), seq_type.shape[-1]), tf.int64)\n",
    "        updates_time_encoding = tf.zeros((len(idxs_inputs), seq_time_encoding.shape[-1]), tf.float32)\n",
    "        updates_recency = tf.zeros((len(idxs_inputs), seq_recency.shape[-1]), tf.float32)\n",
    "        updates_target = tf.zeros((len(idxs_target), seq_items_raw.shape[-1]), tf.int64)\n",
    "        \n",
    "        seq_items = tf.tensor_scatter_nd_update(seq_items, idxs_inputs, updates_items)\n",
    "        # seq_type = tf.tensor_scatter_nd_update(seq_type, idxs_inputs, updates_type)\n",
    "        seq_time_encoding = tf.tensor_scatter_nd_update(seq_time_encoding, idxs_inputs, updates_time_encoding)\n",
    "        seq_recency = tf.tensor_scatter_nd_update(seq_recency, idxs_inputs, updates_recency)\n",
    "        seq_target = tf.tensor_scatter_nd_update(seq_items_raw, idxs_target, updates_target)\n",
    "        \n",
    "        # Padding\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32) \n",
    "        seq_recency = self.pad_sequence(seq_recency, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_target = self.pad_sequence(seq_target, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)  \n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding, seq_recency), seq_target[:, 0]\n",
    "  \n",
    "    def normalize_features(self, features):\n",
    "        return (features - tf.constant(5.45)/tf.constant(1.09))\n",
    "\n",
    "    # def normalize_features(self, features, targets=None, session=None):\n",
    "    #     seq_items, seq_type, seq_time_encoding, seq_recency = features\n",
    "    #     seq_recency = (seq_recency - tf.constant(5.45)/tf.constant(1.09))\n",
    "    #     features = (seq_items, seq_type, seq_time_encoding, seq_recency)\n",
    "    #     return features, targets, session\n",
    "\n",
    "    def set_shapes(self, features, targets=None, session=None):\n",
    "        features[0].set_shape((self.seq_len, 1))\n",
    "        features[1].set_shape((self.seq_len, 1))\n",
    "        features[2].set_shape((self.seq_len, 8))\n",
    "        features[3].set_shape((self.seq_len, 1))\n",
    "        if self.get_session:\n",
    "            return features, targets, session\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 23:56:57.120933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 23:56:57.121644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 23:56:57.121754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 23:56:57.121794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 23:56:57.385720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 23:56:57.385824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 23:56:57.385869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 23:56:57.385920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21908 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([32, 20, 1]), TensorShape([32, 20, 1]), TensorShape([32, 20, 8]), TensorShape([32, 20, 1])]\n",
      "[     0 400243      0 526989      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1212417       0 1159498       0  505110  171550       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_paths = ['../tfrecords/tfrecords_v0.4/na_split=train/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=train')]\n",
    "# 5,45, 1,09\n",
    "dataloader = Bert4RecDataLoader(list_paths, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20, \n",
    "                                     seq_len_target=None,\n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.4, \n",
    "                                     reverse_prob=0.25, \n",
    "                                     get_session=False,\n",
    "                                     is_val=False,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "# Train\n",
    "for batch in tqdm(dataloader):\n",
    "    features, target = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    break\n",
    "\n",
    "# # Test\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, target, session = batch\n",
    "#     seq_items, seq_type, seq_time, seq_recency = features\n",
    "#     idx_mask = target\n",
    "#     break\n",
    "\n",
    "# Val\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, targets, session = batch\n",
    "#     seq_items, seq_type, seq_time, seq_recency = features\n",
    "#     target, type_target, idx_mask = targets\n",
    "#     break\n",
    "\n",
    "print([x.shape for x in features])\n",
    "\n",
    "idx = 2\n",
    "print(seq_items[idx].numpy().flatten())\n",
    "print(seq_type[idx].numpy().flatten())\n",
    "print(target[idx].numpy().flatten())\n",
    "# print(idx_mask[idx].numpy().flatten())\n",
    "# print(type_target[idx].numpy().flatten())\n",
    "\n",
    "del features, target, seq_items, seq_type, seq_time, seq_recency\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransposed(tf.keras.layers.Layer):\n",
    "    def __init__(self, tied_to=None, activation=None, **kwargs):\n",
    "        super(EmbeddingTransposed, self).__init__(**kwargs)\n",
    "        self.tied_to = tied_to\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.custom_weights = self.tied_to.weights[0]\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.tied_to.weights[0].shape[0]\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        output = tf.keras.backend.dot(inputs, tf.keras.backend.transpose(self.custom_weights))\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'activation': tf.keras.activations.serialize(self.activation)}\n",
    "        base_config = super(EmbeddingTransposed, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class EncoderTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, attention_axes=None, drop_rate=0.1, att_drop_rate=0.1):\n",
    "        super(EncoderTransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes, dropout=att_drop_rate)\n",
    "        self.ffn = tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation='gelu'), \n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, query, key, training, attention_mask=None):\n",
    "        attn_output = self.att(query, key, attention_mask=attention_mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "      \n",
    "                 \n",
    "class ModelBert4Rec(tf.keras.models.Model):\n",
    "    def __init__(self, num_items, model_cfg):\n",
    "        super(ModelBert4Rec, self).__init__()\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        self.num_items = num_items\n",
    "        self.model_cfg = model_cfg\n",
    "        self.std_init = np.sqrt(1/(model_cfg.emb_dim*3)).round(4) #0.02 if model_cfg.trf_dim < 1024 else \n",
    "        self.embed_items = tf.keras.layers.Embedding(\n",
    "            num_items, model_cfg.emb_dim, \n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=self.std_init)\n",
    "        )\n",
    "        self.embed_type = tf.keras.layers.Embedding(\n",
    "            3+1, \n",
    "            model_cfg.emb_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=self.std_init)\n",
    "        )\n",
    "        self.mlp_proj_time_encoding = tf.keras.models.Sequential([\n",
    "           tf.keras.layers.Dropout(model_cfg.drop_rate), \n",
    "           tf.keras.layers.Dense(model_cfg.trf_dim, kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=self.std_init)),\n",
    "           tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        ])\n",
    "        # self.mlp_proj_conts = tf.keras.models.Sequential([\n",
    "        #    tf.keras.layers.Dropout(model_cfg.drop_rate), \n",
    "        #    tf.keras.layers.Dense(model_cfg.trf_dim, kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev=self.std_init)),\n",
    "        #    tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        # ])\n",
    "        self.list_transformer_block = [EncoderTransformerBlock(model_cfg.trf_dim, model_cfg.num_heads, \n",
    "                                                               model_cfg.ff_dim, attention_axes=None, \n",
    "                                                               drop_rate=model_cfg.drop_rate, \n",
    "                                                               att_drop_rate=model_cfg.att_drop_rate) \n",
    "                                       for _ in range(model_cfg.num_layers)]\n",
    "        # policy = mixed_precision.Policy('float32')\n",
    "        self.pred_layer = EmbeddingTransposed(tied_to=self.embed_items, activation='linear', dtype='float32')\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        x_seq_past, x_seq_type, x_seq_encoding, x_seq_recency = inputs\n",
    "        pad_mask = tf.cast(tf.where(tf.equal(x_seq_type, 0), 0, 1), tf.float32)\n",
    "        ###########\n",
    "        x_seq_past_items = self.embed_items(x_seq_past[:, :, 0])\n",
    "        x_seq_past_type = self.embed_type(x_seq_type[:, :, 0])\n",
    "        x_seq_time_encoding = self.mlp_proj_time_encoding(x_seq_encoding, training=training)\n",
    "        # x_seq_recency = self.mlp_proj_conts(x_seq_recency, training=training)\n",
    "        x_ones = tf.ones(tf.shape(x_seq_past_items))\n",
    "        ########### \n",
    "        x = x_seq_past_items * (x_ones + x_seq_past_type + x_seq_time_encoding)# + x_seq_recency)\n",
    "        for i in range(len(self.list_transformer_block)):\n",
    "            x = self.list_transformer_block[i](x, x, training=training, attention_mask=pad_mask)\n",
    "        probs = self.pred_layer(x)\n",
    "        return probs\n",
    "      \n",
    "\n",
    "def build_model_bert4Rec(num_items, model_cfg):\n",
    "    return ModelBert4Rec(num_items, model_cfg)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, weight_decay=None):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_decay_tensor = tf.cast(1. if not weight_decay else weight_decay, tf.float32)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          'd_model': self.d_model,\n",
    "          'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        if self.weight_decay:\n",
    "            return self.weight_decay_tensor * tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "        else:\n",
    "            return tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "    \n",
    "    \n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "def custom_loss_bert4rec(tensor_weights=None):\n",
    "    # @tf.function(jit_compile=True)\n",
    "    def loss(y_true, y_pred):\n",
    "        mask = tf.where(y_true >= 1, 1., 0.)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_true, y_pred)\n",
    "        if tensor_weights is not None:\n",
    "            weights = tf.gather(params=tensor_weights, indices=y_true)\n",
    "            return tf.reduce_sum(loss * weights * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "        else:\n",
    "            return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "    loss.__name__ = f'loss_bert4rec'\n",
    "    return loss\n",
    "\n",
    "def weighted_loss_bert4rec():\n",
    "    # @tf.function(jit_compile=True)\n",
    "    def loss(y_true, y_pred, y_type):\n",
    "        mask = tf.where(y_true >= 1, 1., 0.)\n",
    "        y_type = tf.squeeze(y_type, -1)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_true, y_pred)\n",
    "        w_clicks = tf.cast(y_type==1, tf.float32) * 0.1\n",
    "        w_cart = tf.cast(y_type==2, tf.float32) * 0.3\n",
    "        w_order = tf.cast(y_type==3, tf.float32) * 0.6\n",
    "        weights = tf.reduce_max(tf.stack([w_clicks, w_cart, w_order], axis=-1), -1)\n",
    "        return tf.reduce_sum(loss * mask * weights) / (tf.reduce_sum(mask * weights) + 1e-8)\n",
    "    loss.__name__ = f'weighted_loss_bert4rec'\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def custom_accuracy():\n",
    "    # @tf.function(jit_compile=True)\n",
    "    def masked_accuracy(y_true, y_pred):\n",
    "        y_pred = tf.argmax(y_pred, axis=2)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        match = y_true == y_pred\n",
    "        mask = y_true != 0\n",
    "        match = match & mask\n",
    "        match = tf.cast(match, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "    masked_accuracy.__name__ = f'seq_acc'\n",
    "    return masked_accuracy\n",
    "\n",
    "\n",
    "def mrr_topk_categorical(top_k):\n",
    "  \"\"\"\n",
    "  Mrr Topk Categorical metric\n",
    "  \"\"\"\n",
    "  def mrr(y_true, y_pred):                                      \n",
    "    n_samples = tf.shape(y_true)[0]\n",
    "    n_samples_mask = tf.where(tf.reduce_sum(y_true, -1) >= 1, 1., 0.)\n",
    "    _, top_index = tf.nn.top_k(y_pred, top_k)  \n",
    "    result = tf.constant(0.0)\n",
    "    top_index = tf.cast(top_index, tf.float32)\n",
    "    idxs_not_masked = tf.cast(tf.argmax(y_true, axis=-1), tf.int32)\n",
    "    for i in tf.range(n_samples):\n",
    "        ranked_indicies = tf.where(tf.equal(top_index[i, idxs_not_masked[i], :], y_true[i, :][:, tf.newaxis]))\n",
    "        if tf.shape(ranked_indicies)[0] > 0:\n",
    "            ranked_indicies = tf.cast(ranked_indicies[0], tf.int32)\n",
    "            #check that the prediction its not padding\n",
    "            if top_index[i, ranked_indicies[0], ranked_indicies[1]] != 0.0: \n",
    "                rr = tf.cast(1/(ranked_indicies[1]+1), tf.float32)\n",
    "            else:\n",
    "                rr = tf.constant(0.0)\n",
    "        else:\n",
    "            rr = tf.constant(0.0)\n",
    "        result+=rr\n",
    "    return result/(tf.reduce_sum(n_samples_mask) + 1e-8)\n",
    "  mrr.__name__ = f'mrr_{top_k}_categorical'\n",
    "  return mrr\n",
    "\n",
    "\n",
    "def recall_top_k(top_k=1, seq_len=10):\n",
    "    # @tf.function\n",
    "    def recall(y_true, y_pred):\n",
    "        n_samples = tf.shape(y_pred)[0]\n",
    "        y_true = tf.cast(y_true, tf.int64)\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), tf.int32)\n",
    "        _, top_index = tf.nn.top_k(y_pred, top_k) \n",
    "        top_index = tf.cast(top_index, tf.int64)\n",
    "        # cum_sum = tf.zeros(n_samples, tf.int32)\n",
    "        result = tf.constant(0, tf.int32)\n",
    "        for i in tf.range(seq_len):\n",
    "            indexes_i = top_index[:, i, :]\n",
    "            is_true = tf.reduce_sum(tf.reduce_max(tf.where(y_true[:, i:i+1]==indexes_i, 1, 0), -1) * mask[:, i])\n",
    "            result += is_true\n",
    "        return tf.cast(result, tf.float32) / (tf.cast(tf.reduce_sum(mask), tf.float32) + 1e-8)\n",
    "    recall.__name__ = f'recall_{top_k}'\n",
    "    return recall\n",
    "\n",
    "\n",
    "def create_folder_with_version(base_name, checkpoint_path):\n",
    "    if os.path.exists(os.path.join(checkpoint_path, base_name)):\n",
    "        version_ = base_name.split('_v')\n",
    "        if not version_ or len(version_)==1:\n",
    "            base_name_no_version = base_name\n",
    "            version_ = '_v1'\n",
    "        else:\n",
    "            base_name_no_version = '_'.join(base_name.split('_v')[:-1])\n",
    "            version_ = f'_v{int(version_[-1])+1}'\n",
    "        base_name = base_name_no_version + version_\n",
    "        return create_folder_with_version(base_name, checkpoint_path)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_path, base_name)\n",
    "        os.mkdir(checkpoint_path)\n",
    "        return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_lr = CustomSchedule(128, 10_000, weight_decay=None)\n",
    "finetune_lr = CustomSchedule(512, 5_000, weight_decay=None)\n",
    "plt.plot(tmp_lr(tf.range(10_000_000 // (32* 5), dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show();\n",
    "\n",
    "plt.plot(finetune_lr(tf.range(2_300_000 // (32), dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "\n",
    "def flat_gradients(grads_or_idx_slices: tf.Tensor) -> tf.Tensor:\n",
    "    '''Convert gradients if it's tf.IndexedSlices.\n",
    "    When computing gradients for operation concerning `tf.gather`, the type of gradients \n",
    "    '''\n",
    "    if type(grads_or_idx_slices) == tf.IndexedSlices:\n",
    "        return tf.scatter_nd(\n",
    "            tf.expand_dims(grads_or_idx_slices.indices, 1),\n",
    "            grads_or_idx_slices.values,\n",
    "            tf.cast(grads_or_idx_slices.dense_shape, tf.int64)\n",
    "        )\n",
    "    return grads_or_idx_slices\n",
    "\n",
    "def backward_optimization(num_grad_steps, global_gradients, step_gradients, step, model, optimizer):\n",
    "    if not global_gradients:\n",
    "        global_gradients = step_gradients\n",
    "    else:\n",
    "        for i, g in enumerate(step_gradients):\n",
    "            global_gradients[i] += flat_gradients(g)\n",
    "    if (step + 1) % num_grad_steps == 0:\n",
    "        optimizer.apply_gradients(zip(global_gradients, model.trainable_variables))\n",
    "        global_gradients = []\n",
    "    return global_gradients\n",
    "\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def train_step(*inputs, target, model, optimizer, num_accum_steps, **kwargs):\n",
    "    l_loss, l_acc = kwargs['loss'], kwargs['acc']\n",
    "    seq_type = kwargs['seq_type']\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(*inputs, training=True)\n",
    "        loss = loss_function(target, predictions, seq_type)\n",
    "        acc = acc_function(target, predictions)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss) / num_accum_steps\n",
    "\n",
    "    gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(gradients)\n",
    "    # optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    l_loss(loss)\n",
    "    l_acc(acc)\n",
    "    return gradients\n",
    "  \n",
    "@tf.function\n",
    "def test_step(*inputs, target, **kwargs):\n",
    "    l_loss, l_acc = kwargs['loss'], kwargs['acc']\n",
    "    seq_type = kwargs['seq_type']\n",
    "    predictions = model(*inputs, training=False)\n",
    "    loss = loss_function(target, predictions, seq_type)\n",
    "    l_loss(loss)\n",
    "    l_acc(acc)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def metrics_reset_states(*metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "\n",
    "def fancy_printer(loss_tracker, epoch, batch_num, start, step='train', dict_metrics={}, num_epochs=1, **kwargs):\n",
    "    num_step = kwargs['num_step']\n",
    "    dict_print_metrics = {' '.join(f\"{key}:{value:.6f}\" for key, value in dict_metrics.items())}\n",
    "    if step!='epoch':\n",
    "        printer = f'[{step} Epoch]{epoch + 1}/{num_epochs} [Time]{time.time() - start:.2f} [Step]{num_step} [Batch]{batch_num} [Speed]{((time.time() - start)/max(1, batch_num))*1000:.2f}ms/step '\n",
    "        printer += f'[Loss]{loss_tracker.result():.4f} ' + '[Metrics]' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "    else:\n",
    "        train_loss, val_loss = kwargs['train_loss'], kwargs['val_loss']\n",
    "        print(f'\\nTime taken for epoch {epoch+1}/{num_epochs}: {time.time() - start:.2f} secs')\n",
    "        printer = f'[Epoch]{epoch + 1}/{num_epochs} - [Train Loss]{train_loss.result():.4f} '\n",
    "        printer += f'- [Val Loss]{val_loss.result():.4f} ' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "\n",
    "\n",
    "def log_wandb_metrics(step='train', num_step=0, dict_metrics=None, gradients=None, plot_image=False, **kwargs):\n",
    "    # Scalar metrics\n",
    "    if step=='train' or step=='val':\n",
    "        wandb.log({name : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "    if step=='epoch':\n",
    "        wandb.log({f'epoch_{name}' : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "\n",
    "    # Gradients\n",
    "    if gradients:\n",
    "        wandb.log({\n",
    "            'mean_norm_gradients' : np.mean([tf.norm(x) for x in gradients]), \n",
    "            'max_norm_gradients': np.max([tf.norm(x) for x in gradients])\n",
    "        })\n",
    "\n",
    "def init_wandb(wandb_project='<your_project>', entity='', run_name='', dict_config=None):\n",
    "    wandb.init(project=wandb_project, entity=entity, name=run_name, settings=wandb.Settings(code_dir=\".\"),\n",
    "               config=dict_config)\n",
    "    wandb.run.log_code(\".\")\n",
    "\n",
    "\n",
    "def grad_accum_scheduler(num_samples, list_scheduler, max_grad_accum):\n",
    "    if num_samples >= len(list_scheduler):\n",
    "        return max_grad_accum\n",
    "    return list_scheduler[num_samples]\n",
    "\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menric1296\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/enric/SSD1TB/KAGGLE/025_Kaggle-OTTO Recsys-2022/1_Scripts/wandb/run-20221123_182419-2zmg82q5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/enric1296/otto-recsys/runs/2zmg82q5\" target=\"_blank\">model_bert4rec_complete_0.8.4_2022-11-23 18:24:18</a></strong> to <a href=\"https://wandb.ai/enric1296/otto-recsys\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n",
      "================================================================================\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 18:24:22.340066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/home/enric/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 335806208 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-11-23 18:24:23.019445: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x203f79a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-23 18:24:23.019461: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2022-11-23 18:24:23.034849: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. model_bert4_rec/encoder_transformer_block/dropout_1/dropout/random_uniform/RandomUniform\n",
      "2022-11-23 18:24:23.037652: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-23 18:24:24.146013: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-11-23 18:24:24.592102: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train Epoch]1/2 [Time]2.36 [Step]0 [Batch]0 [Speed]2362.95ms/step [Loss]14.1351 [Metrics]{'train_loss:14.135131 train_acc:0.000000 lr:0.000000 grad_accum:1.000000 total_samples:0.000000'}\n",
      "[Train Epoch]1/2 [Time]55.03 [Step]500 [Batch]500 [Speed]110.05ms/step [Loss]14.0531 [Metrics]{'train_loss:14.053089 train_acc:0.000000 lr:0.000044 grad_accum:1.000000 total_samples:16000.000000'}\n",
      "[Train Epoch]1/2 [Time]107.47 [Step]1000 [Batch]1000 [Speed]107.47ms/step [Loss]13.6867 [Metrics]{'train_loss:13.686704 train_acc:0.000026 lr:0.000088 grad_accum:1.000000 total_samples:32000.000000'}\n",
      "[Train Epoch]1/2 [Time]160.42 [Step]1500 [Batch]1500 [Speed]106.94ms/step [Loss]13.4400 [Metrics]{'train_loss:13.440045 train_acc:0.000124 lr:0.000133 grad_accum:1.000000 total_samples:48000.000000'}\n",
      "[Train Epoch]1/2 [Time]213.41 [Step]2000 [Batch]2000 [Speed]106.70ms/step [Loss]13.2751 [Metrics]{'train_loss:13.275105 train_acc:0.000235 lr:0.000177 grad_accum:1.000000 total_samples:64000.000000'}\n",
      "[Train Epoch]1/2 [Time]265.94 [Step]2500 [Batch]2500 [Speed]106.38ms/step [Loss]13.1319 [Metrics]{'train_loss:13.131946 train_acc:0.000677 lr:0.000221 grad_accum:1.000000 total_samples:80000.000000'}\n",
      "[Train Epoch]1/2 [Time]318.21 [Step]3000 [Batch]3000 [Speed]106.07ms/step [Loss]12.9903 [Metrics]{'train_loss:12.990346 train_acc:0.003265 lr:0.000265 grad_accum:1.000000 total_samples:96000.000000'}\n",
      "[Train Epoch]1/2 [Time]370.71 [Step]3500 [Batch]3500 [Speed]105.92ms/step [Loss]12.8675 [Metrics]{'train_loss:12.867508 train_acc:0.006443 lr:0.000309 grad_accum:1.000000 total_samples:112000.000000'}\n",
      "[Train Epoch]1/2 [Time]423.61 [Step]4000 [Batch]4000 [Speed]105.90ms/step [Loss]12.7549 [Metrics]{'train_loss:12.754928 train_acc:0.010151 lr:0.000354 grad_accum:1.000000 total_samples:128000.000000'}\n",
      "[Train Epoch]1/2 [Time]476.14 [Step]4500 [Batch]4500 [Speed]105.81ms/step [Loss]12.6502 [Metrics]{'train_loss:12.650196 train_acc:0.013900 lr:0.000398 grad_accum:1.000000 total_samples:144000.000000'}\n",
      "[Train Epoch]1/2 [Time]528.78 [Step]5000 [Batch]5000 [Speed]105.76ms/step [Loss]12.5557 [Metrics]{'train_loss:12.555749 train_acc:0.017055 lr:0.000442 grad_accum:1.000000 total_samples:160000.000000'}\n",
      "[Train Epoch]1/2 [Time]581.72 [Step]5500 [Batch]5500 [Speed]105.77ms/step [Loss]12.4613 [Metrics]{'train_loss:12.461306 train_acc:0.020288 lr:0.000486 grad_accum:1.000000 total_samples:176000.000000'}\n",
      "[Train Epoch]1/2 [Time]634.45 [Step]6000 [Batch]6000 [Speed]105.74ms/step [Loss]12.3782 [Metrics]{'train_loss:12.378181 train_acc:0.022974 lr:0.000530 grad_accum:1.000000 total_samples:192000.000000'}\n",
      "[Train Epoch]1/2 [Time]684.34 [Step]6500 [Batch]6500 [Speed]105.28ms/step [Loss]12.2969 [Metrics]{'train_loss:12.296866 train_acc:0.025366 lr:0.000575 grad_accum:1.000000 total_samples:208000.000000'}\n",
      "[Train Epoch]1/2 [Time]732.69 [Step]7000 [Batch]7000 [Speed]104.67ms/step [Loss]12.2181 [Metrics]{'train_loss:12.218120 train_acc:0.027451 lr:0.000619 grad_accum:1.000000 total_samples:224000.000000'}\n",
      "[Train Epoch]1/2 [Time]781.04 [Step]7500 [Batch]7500 [Speed]104.14ms/step [Loss]12.1426 [Metrics]{'train_loss:12.142629 train_acc:0.029494 lr:0.000663 grad_accum:1.000000 total_samples:240000.000000'}\n",
      "[Train Epoch]1/2 [Time]830.96 [Step]4000 [Batch]8000 [Speed]103.87ms/step [Loss]12.0767 [Metrics]{'train_loss:12.076651 train_acc:0.031170 lr:0.000354 grad_accum:2.000000 total_samples:256032.000000'}\n",
      "[Train Epoch]1/2 [Time]876.56 [Step]4250 [Batch]8500 [Speed]103.12ms/step [Loss]12.0140 [Metrics]{'train_loss:12.014021 train_acc:0.032813 lr:0.000376 grad_accum:2.000000 total_samples:272032.000000'}\n",
      "[Train Epoch]1/2 [Time]921.97 [Step]4500 [Batch]9000 [Speed]102.44ms/step [Loss]11.9549 [Metrics]{'train_loss:11.954939 train_acc:0.034241 lr:0.000398 grad_accum:2.000000 total_samples:288032.000000'}\n",
      "[Train Epoch]1/2 [Time]967.79 [Step]4750 [Batch]9500 [Speed]101.87ms/step [Loss]11.8921 [Metrics]{'train_loss:11.892149 train_acc:0.035542 lr:0.000420 grad_accum:2.000000 total_samples:304032.000000'}\n",
      "[Train Epoch]1/2 [Time]1013.44 [Step]5000 [Batch]10000 [Speed]101.34ms/step [Loss]11.8330 [Metrics]{'train_loss:11.833034 train_acc:0.036783 lr:0.000442 grad_accum:2.000000 total_samples:320032.000000'}\n",
      "[Train Epoch]1/2 [Time]1058.50 [Step]5250 [Batch]10500 [Speed]100.81ms/step [Loss]11.7787 [Metrics]{'train_loss:11.778714 train_acc:0.037789 lr:0.000464 grad_accum:2.000000 total_samples:336032.000000'}\n",
      "[Train Epoch]1/2 [Time]1103.17 [Step]5500 [Batch]11000 [Speed]100.29ms/step [Loss]11.7246 [Metrics]{'train_loss:11.724573 train_acc:0.038721 lr:0.000486 grad_accum:2.000000 total_samples:352032.000000'}\n",
      "[Train Epoch]1/2 [Time]1148.24 [Step]5750 [Batch]11500 [Speed]99.85ms/step [Loss]11.6742 [Metrics]{'train_loss:11.674198 train_acc:0.039745 lr:0.000508 grad_accum:2.000000 total_samples:368032.000000'}\n",
      "[Train Epoch]1/2 [Time]1193.58 [Step]6000 [Batch]12000 [Speed]99.47ms/step [Loss]11.6265 [Metrics]{'train_loss:11.626476 train_acc:0.040562 lr:0.000530 grad_accum:2.000000 total_samples:384032.000000'}\n",
      "[Train Epoch]1/2 [Time]1238.90 [Step]6250 [Batch]12500 [Speed]99.11ms/step [Loss]11.5826 [Metrics]{'train_loss:11.582615 train_acc:0.041183 lr:0.000552 grad_accum:2.000000 total_samples:400032.000000'}\n",
      "[Train Epoch]1/2 [Time]1284.03 [Step]6500 [Batch]13000 [Speed]98.77ms/step [Loss]11.5389 [Metrics]{'train_loss:11.538918 train_acc:0.041895 lr:0.000575 grad_accum:2.000000 total_samples:416032.000000'}\n",
      "[Train Epoch]1/2 [Time]1329.38 [Step]6750 [Batch]13500 [Speed]98.47ms/step [Loss]11.4954 [Metrics]{'train_loss:11.495409 train_acc:0.042569 lr:0.000597 grad_accum:2.000000 total_samples:432032.000000'}\n",
      "[Train Epoch]1/2 [Time]1374.90 [Step]7000 [Batch]14000 [Speed]98.21ms/step [Loss]11.4570 [Metrics]{'train_loss:11.457025 train_acc:0.043174 lr:0.000619 grad_accum:2.000000 total_samples:448032.000000'}\n",
      "[Train Epoch]1/2 [Time]1420.02 [Step]7250 [Batch]14500 [Speed]97.93ms/step [Loss]11.4157 [Metrics]{'train_loss:11.415709 train_acc:0.043820 lr:0.000641 grad_accum:2.000000 total_samples:464032.000000'}\n",
      "[Train Epoch]1/2 [Time]1464.60 [Step]7500 [Batch]15000 [Speed]97.64ms/step [Loss]11.3761 [Metrics]{'train_loss:11.376102 train_acc:0.044385 lr:0.000663 grad_accum:2.000000 total_samples:480032.000000'}\n",
      "[Train Epoch]1/2 [Time]1509.28 [Step]7750 [Batch]15500 [Speed]97.37ms/step [Loss]11.3361 [Metrics]{'train_loss:11.336080 train_acc:0.044923 lr:0.000685 grad_accum:2.000000 total_samples:496032.000000'}\n",
      "[Train Epoch]1/2 [Time]1553.00 [Step]5333 [Batch]16000 [Speed]97.06ms/step [Loss]11.2963 [Metrics]{'train_loss:11.296268 train_acc:0.045423 lr:0.000471 grad_accum:3.000000 total_samples:512000.000000'}\n",
      "[Train Epoch]1/2 [Time]1596.39 [Step]5500 [Batch]16500 [Speed]96.75ms/step [Loss]11.2549 [Metrics]{'train_loss:11.254850 train_acc:0.046014 lr:0.000486 grad_accum:3.000000 total_samples:528032.000000'}\n",
      "[Train Epoch]1/2 [Time]1639.80 [Step]5666 [Batch]17000 [Speed]96.46ms/step [Loss]11.2177 [Metrics]{'train_loss:11.217656 train_acc:0.046496 lr:0.000501 grad_accum:3.000000 total_samples:543968.000000'}\n",
      "[Train Epoch]1/2 [Time]1682.83 [Step]5833 [Batch]17500 [Speed]96.16ms/step [Loss]11.1792 [Metrics]{'train_loss:11.179219 train_acc:0.047112 lr:0.000516 grad_accum:3.000000 total_samples:560000.000000'}\n",
      "[Train Epoch]1/2 [Time]1726.29 [Step]6000 [Batch]18000 [Speed]95.91ms/step [Loss]11.1437 [Metrics]{'train_loss:11.143713 train_acc:0.047653 lr:0.000530 grad_accum:3.000000 total_samples:576032.000000'}\n",
      "[Train Epoch]1/2 [Time]1769.32 [Step]6166 [Batch]18500 [Speed]95.64ms/step [Loss]11.1101 [Metrics]{'train_loss:11.110074 train_acc:0.048175 lr:0.000545 grad_accum:3.000000 total_samples:591968.000000'}\n",
      "[Train Epoch]1/2 [Time]1812.27 [Step]6333 [Batch]19000 [Speed]95.38ms/step [Loss]11.0759 [Metrics]{'train_loss:11.075932 train_acc:0.048696 lr:0.000560 grad_accum:3.000000 total_samples:608000.000000'}\n",
      "[Train Epoch]1/2 [Time]1855.51 [Step]6500 [Batch]19500 [Speed]95.15ms/step [Loss]11.0427 [Metrics]{'train_loss:11.042672 train_acc:0.049174 lr:0.000575 grad_accum:3.000000 total_samples:624032.000000'}\n",
      "[Train Epoch]1/2 [Time]1898.40 [Step]6666 [Batch]20000 [Speed]94.92ms/step [Loss]11.0092 [Metrics]{'train_loss:11.009222 train_acc:0.049611 lr:0.000589 grad_accum:3.000000 total_samples:639968.000000'}\n",
      "[Train Epoch]1/2 [Time]1941.30 [Step]6833 [Batch]20500 [Speed]94.70ms/step [Loss]10.9786 [Metrics]{'train_loss:10.978639 train_acc:0.050019 lr:0.000604 grad_accum:3.000000 total_samples:656000.000000'}\n",
      "[Train Epoch]1/2 [Time]1983.76 [Step]7000 [Batch]21000 [Speed]94.46ms/step [Loss]10.9485 [Metrics]{'train_loss:10.948533 train_acc:0.050447 lr:0.000619 grad_accum:3.000000 total_samples:672032.000000'}\n",
      "[Train Epoch]1/2 [Time]2023.38 [Step]7166 [Batch]21500 [Speed]94.11ms/step [Loss]10.9179 [Metrics]{'train_loss:10.917891 train_acc:0.050870 lr:0.000633 grad_accum:3.000000 total_samples:687968.000000'}\n",
      "[Train Epoch]1/2 [Time]2062.97 [Step]7333 [Batch]22000 [Speed]93.77ms/step [Loss]10.8887 [Metrics]{'train_loss:10.888700 train_acc:0.051265 lr:0.000648 grad_accum:3.000000 total_samples:704000.000000'}\n",
      "[Train Epoch]1/2 [Time]2102.58 [Step]7500 [Batch]22500 [Speed]93.45ms/step [Loss]10.8589 [Metrics]{'train_loss:10.858924 train_acc:0.051681 lr:0.000663 grad_accum:3.000000 total_samples:720032.000000'}\n",
      "[Train Epoch]1/2 [Time]2142.22 [Step]7666 [Batch]23000 [Speed]93.14ms/step [Loss]10.8294 [Metrics]{'train_loss:10.829444 train_acc:0.052090 lr:0.000678 grad_accum:3.000000 total_samples:735968.000000'}\n",
      "[Train Epoch]1/2 [Time]2181.76 [Step]5875 [Batch]23500 [Speed]92.84ms/step [Loss]10.8004 [Metrics]{'train_loss:10.800420 train_acc:0.052476 lr:0.000519 grad_accum:4.000000 total_samples:752128.000000'}\n",
      "[Train Epoch]1/2 [Time]2220.52 [Step]6000 [Batch]24000 [Speed]92.52ms/step [Loss]10.7714 [Metrics]{'train_loss:10.771378 train_acc:0.052859 lr:0.000530 grad_accum:4.000000 total_samples:768128.000000'}\n",
      "[Train Epoch]1/2 [Time]2259.27 [Step]6125 [Batch]24500 [Speed]92.22ms/step [Loss]10.7443 [Metrics]{'train_loss:10.744334 train_acc:0.053213 lr:0.000541 grad_accum:4.000000 total_samples:784128.000000'}\n",
      "Saving checkpoint for epoch 1 at step 25000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]2307.18 [Step]6250 [Batch]25000 [Speed]92.29ms/step [Loss]10.7172 [Metrics]{'train_loss:10.717216 train_acc:0.053613 lr:0.000552 grad_accum:4.000000 total_samples:800128.000000'}\n",
      "[Train Epoch]1/2 [Time]2345.86 [Step]6375 [Batch]25500 [Speed]91.99ms/step [Loss]10.6936 [Metrics]{'train_loss:10.693595 train_acc:0.053980 lr:0.000563 grad_accum:4.000000 total_samples:816128.000000'}\n",
      "[Train Epoch]1/2 [Time]2384.63 [Step]6500 [Batch]26000 [Speed]91.72ms/step [Loss]10.6709 [Metrics]{'train_loss:10.670852 train_acc:0.054302 lr:0.000575 grad_accum:4.000000 total_samples:832128.000000'}\n",
      "[Train Epoch]1/2 [Time]2423.39 [Step]6625 [Batch]26500 [Speed]91.45ms/step [Loss]10.6480 [Metrics]{'train_loss:10.647989 train_acc:0.054599 lr:0.000586 grad_accum:4.000000 total_samples:848128.000000'}\n",
      "[Train Epoch]1/2 [Time]2462.11 [Step]6750 [Batch]27000 [Speed]91.19ms/step [Loss]10.6258 [Metrics]{'train_loss:10.625760 train_acc:0.054905 lr:0.000597 grad_accum:4.000000 total_samples:864128.000000'}\n",
      "[Train Epoch]1/2 [Time]2500.84 [Step]6875 [Batch]27500 [Speed]90.94ms/step [Loss]10.6025 [Metrics]{'train_loss:10.602508 train_acc:0.055236 lr:0.000608 grad_accum:4.000000 total_samples:880128.000000'}\n",
      "[Train Epoch]1/2 [Time]2539.57 [Step]7000 [Batch]28000 [Speed]90.70ms/step [Loss]10.5794 [Metrics]{'train_loss:10.579405 train_acc:0.055521 lr:0.000619 grad_accum:4.000000 total_samples:896128.000000'}\n",
      "[Train Epoch]1/2 [Time]2578.30 [Step]7125 [Batch]28500 [Speed]90.47ms/step [Loss]10.5572 [Metrics]{'train_loss:10.557211 train_acc:0.055777 lr:0.000630 grad_accum:4.000000 total_samples:912128.000000'}\n",
      "[Train Epoch]1/2 [Time]2617.08 [Step]7250 [Batch]29000 [Speed]90.24ms/step [Loss]10.5347 [Metrics]{'train_loss:10.534738 train_acc:0.056078 lr:0.000641 grad_accum:4.000000 total_samples:928128.000000'}\n",
      "[Train Epoch]1/2 [Time]2655.80 [Step]7375 [Batch]29500 [Speed]90.03ms/step [Loss]10.5123 [Metrics]{'train_loss:10.512280 train_acc:0.056353 lr:0.000652 grad_accum:4.000000 total_samples:944128.000000'}\n",
      "[Train Epoch]1/2 [Time]2694.59 [Step]7500 [Batch]30000 [Speed]89.82ms/step [Loss]10.4914 [Metrics]{'train_loss:10.491442 train_acc:0.056609 lr:0.000663 grad_accum:4.000000 total_samples:960128.000000'}\n",
      "[Train Epoch]1/2 [Time]2733.33 [Step]7625 [Batch]30500 [Speed]89.62ms/step [Loss]10.4704 [Metrics]{'train_loss:10.470446 train_acc:0.056933 lr:0.000674 grad_accum:4.000000 total_samples:976128.000000'}\n",
      "[Train Epoch]1/2 [Time]2772.04 [Step]7750 [Batch]31000 [Speed]89.42ms/step [Loss]10.4496 [Metrics]{'train_loss:10.449563 train_acc:0.057181 lr:0.000685 grad_accum:4.000000 total_samples:992128.000000'}\n",
      "[Train Epoch]1/2 [Time]2810.56 [Step]6300 [Batch]31500 [Speed]89.22ms/step [Loss]10.4299 [Metrics]{'train_loss:10.429903 train_acc:0.057434 lr:0.000557 grad_accum:5.000000 total_samples:1008224.000000'}\n",
      "[Train Epoch]1/2 [Time]2848.76 [Step]6400 [Batch]32000 [Speed]89.02ms/step [Loss]10.4106 [Metrics]{'train_loss:10.410645 train_acc:0.057653 lr:0.000566 grad_accum:5.000000 total_samples:1024224.000000'}\n",
      "[Train Epoch]1/2 [Time]2887.00 [Step]6500 [Batch]32500 [Speed]88.83ms/step [Loss]10.3918 [Metrics]{'train_loss:10.391803 train_acc:0.057871 lr:0.000575 grad_accum:5.000000 total_samples:1040224.000000'}\n",
      "[Train Epoch]1/2 [Time]2925.29 [Step]6600 [Batch]33000 [Speed]88.65ms/step [Loss]10.3721 [Metrics]{'train_loss:10.372085 train_acc:0.058130 lr:0.000583 grad_accum:5.000000 total_samples:1056224.000000'}\n",
      "[Train Epoch]1/2 [Time]2966.90 [Step]6700 [Batch]33500 [Speed]88.56ms/step [Loss]10.3531 [Metrics]{'train_loss:10.353142 train_acc:0.058352 lr:0.000592 grad_accum:5.000000 total_samples:1072224.000000'}\n",
      "[Train Epoch]1/2 [Time]3008.63 [Step]6800 [Batch]34000 [Speed]88.49ms/step [Loss]10.3355 [Metrics]{'train_loss:10.335547 train_acc:0.058552 lr:0.000601 grad_accum:5.000000 total_samples:1088224.000000'}\n",
      "[Train Epoch]1/2 [Time]3050.19 [Step]6900 [Batch]34500 [Speed]88.41ms/step [Loss]10.3172 [Metrics]{'train_loss:10.317213 train_acc:0.058763 lr:0.000610 grad_accum:5.000000 total_samples:1104224.000000'}\n",
      "[Train Epoch]1/2 [Time]3091.80 [Step]7000 [Batch]35000 [Speed]88.34ms/step [Loss]10.2997 [Metrics]{'train_loss:10.299723 train_acc:0.058968 lr:0.000619 grad_accum:5.000000 total_samples:1120224.000000'}\n",
      "[Train Epoch]1/2 [Time]3133.30 [Step]7100 [Batch]35500 [Speed]88.26ms/step [Loss]10.2826 [Metrics]{'train_loss:10.282610 train_acc:0.059184 lr:0.000628 grad_accum:5.000000 total_samples:1136224.000000'}\n",
      "[Train Epoch]1/2 [Time]3175.17 [Step]7200 [Batch]36000 [Speed]88.20ms/step [Loss]10.2657 [Metrics]{'train_loss:10.265676 train_acc:0.059351 lr:0.000636 grad_accum:5.000000 total_samples:1152224.000000'}\n",
      "[Train Epoch]1/2 [Time]3216.97 [Step]7300 [Batch]36500 [Speed]88.14ms/step [Loss]10.2490 [Metrics]{'train_loss:10.249026 train_acc:0.059522 lr:0.000645 grad_accum:5.000000 total_samples:1168224.000000'}\n",
      "[Train Epoch]1/2 [Time]3258.88 [Step]7400 [Batch]37000 [Speed]88.08ms/step [Loss]10.2325 [Metrics]{'train_loss:10.232482 train_acc:0.059716 lr:0.000654 grad_accum:5.000000 total_samples:1184224.000000'}\n",
      "[Train Epoch]1/2 [Time]3300.79 [Step]7500 [Batch]37500 [Speed]88.02ms/step [Loss]10.2168 [Metrics]{'train_loss:10.216835 train_acc:0.059915 lr:0.000663 grad_accum:5.000000 total_samples:1200224.000000'}\n",
      "[Train Epoch]1/2 [Time]3342.50 [Step]7600 [Batch]38000 [Speed]87.96ms/step [Loss]10.2011 [Metrics]{'train_loss:10.201053 train_acc:0.060107 lr:0.000672 grad_accum:5.000000 total_samples:1216224.000000'}\n",
      "[Train Epoch]1/2 [Time]3384.17 [Step]7700 [Batch]38500 [Speed]87.90ms/step [Loss]10.1856 [Metrics]{'train_loss:10.185566 train_acc:0.060266 lr:0.000681 grad_accum:5.000000 total_samples:1232224.000000'}\n",
      "[Train Epoch]1/2 [Time]3425.67 [Step]7800 [Batch]39000 [Speed]87.84ms/step [Loss]10.1690 [Metrics]{'train_loss:10.169007 train_acc:0.060467 lr:0.000689 grad_accum:5.000000 total_samples:1248224.000000'}\n",
      "[Train Epoch]1/2 [Time]3467.42 [Step]7900 [Batch]39500 [Speed]87.78ms/step [Loss]10.1537 [Metrics]{'train_loss:10.153690 train_acc:0.060661 lr:0.000698 grad_accum:5.000000 total_samples:1264224.000000'}\n",
      "[Train Epoch]1/2 [Time]3509.25 [Step]8000 [Batch]40000 [Speed]87.73ms/step [Loss]10.1390 [Metrics]{'train_loss:10.139002 train_acc:0.060838 lr:0.000707 grad_accum:5.000000 total_samples:1280224.000000'}\n",
      "[Train Epoch]1/2 [Time]3550.60 [Step]8100 [Batch]40500 [Speed]87.67ms/step [Loss]10.1244 [Metrics]{'train_loss:10.124393 train_acc:0.061014 lr:0.000716 grad_accum:5.000000 total_samples:1296224.000000'}\n",
      "[Train Epoch]1/2 [Time]3592.06 [Step]8200 [Batch]41000 [Speed]87.61ms/step [Loss]10.1101 [Metrics]{'train_loss:10.110077 train_acc:0.061197 lr:0.000725 grad_accum:5.000000 total_samples:1312224.000000'}\n",
      "[Train Epoch]1/2 [Time]3633.99 [Step]8300 [Batch]41500 [Speed]87.57ms/step [Loss]10.0968 [Metrics]{'train_loss:10.096838 train_acc:0.061357 lr:0.000734 grad_accum:5.000000 total_samples:1328224.000000'}\n",
      "[Train Epoch]1/2 [Time]3675.89 [Step]8400 [Batch]42000 [Speed]87.52ms/step [Loss]10.0826 [Metrics]{'train_loss:10.082579 train_acc:0.061545 lr:0.000742 grad_accum:5.000000 total_samples:1344224.000000'}\n",
      "[Train Epoch]1/2 [Time]3717.60 [Step]8500 [Batch]42500 [Speed]87.47ms/step [Loss]10.0686 [Metrics]{'train_loss:10.068589 train_acc:0.061722 lr:0.000751 grad_accum:5.000000 total_samples:1360224.000000'}\n",
      "[Train Epoch]1/2 [Time]3758.98 [Step]8600 [Batch]43000 [Speed]87.42ms/step [Loss]10.0549 [Metrics]{'train_loss:10.054905 train_acc:0.061862 lr:0.000760 grad_accum:5.000000 total_samples:1376224.000000'}\n",
      "[Train Epoch]1/2 [Time]3800.65 [Step]8700 [Batch]43500 [Speed]87.37ms/step [Loss]10.0420 [Metrics]{'train_loss:10.041995 train_acc:0.062021 lr:0.000769 grad_accum:5.000000 total_samples:1392224.000000'}\n",
      "[Train Epoch]1/2 [Time]3842.47 [Step]8800 [Batch]44000 [Speed]87.33ms/step [Loss]10.0289 [Metrics]{'train_loss:10.028876 train_acc:0.062160 lr:0.000778 grad_accum:5.000000 total_samples:1408224.000000'}\n",
      "[Train Epoch]1/2 [Time]3884.28 [Step]8900 [Batch]44500 [Speed]87.29ms/step [Loss]10.0154 [Metrics]{'train_loss:10.015396 train_acc:0.062307 lr:0.000787 grad_accum:5.000000 total_samples:1424224.000000'}\n",
      "[Train Epoch]1/2 [Time]3926.17 [Step]9000 [Batch]45000 [Speed]87.25ms/step [Loss]10.0024 [Metrics]{'train_loss:10.002445 train_acc:0.062454 lr:0.000795 grad_accum:5.000000 total_samples:1440224.000000'}\n",
      "[Train Epoch]1/2 [Time]3967.87 [Step]9100 [Batch]45500 [Speed]87.21ms/step [Loss]9.9893 [Metrics]{'train_loss:9.989337 train_acc:0.062601 lr:0.000804 grad_accum:5.000000 total_samples:1456224.000000'}\n",
      "[Train Epoch]1/2 [Time]4009.32 [Step]9200 [Batch]46000 [Speed]87.16ms/step [Loss]9.9762 [Metrics]{'train_loss:9.976202 train_acc:0.062752 lr:0.000813 grad_accum:5.000000 total_samples:1472224.000000'}\n",
      "[Train Epoch]1/2 [Time]4051.18 [Step]9300 [Batch]46500 [Speed]87.12ms/step [Loss]9.9635 [Metrics]{'train_loss:9.963466 train_acc:0.062904 lr:0.000822 grad_accum:5.000000 total_samples:1488224.000000'}\n",
      "[Train Epoch]1/2 [Time]4092.89 [Step]9400 [Batch]47000 [Speed]87.08ms/step [Loss]9.9513 [Metrics]{'train_loss:9.951309 train_acc:0.063025 lr:0.000831 grad_accum:5.000000 total_samples:1504224.000000'}\n",
      "[Train Epoch]1/2 [Time]4134.39 [Step]9500 [Batch]47500 [Speed]87.04ms/step [Loss]9.9396 [Metrics]{'train_loss:9.939638 train_acc:0.063172 lr:0.000840 grad_accum:5.000000 total_samples:1520224.000000'}\n",
      "[Train Epoch]1/2 [Time]4176.00 [Step]9600 [Batch]48000 [Speed]87.00ms/step [Loss]9.9282 [Metrics]{'train_loss:9.928226 train_acc:0.063301 lr:0.000849 grad_accum:5.000000 total_samples:1536224.000000'}\n",
      "[Train Epoch]1/2 [Time]4217.71 [Step]9700 [Batch]48500 [Speed]86.96ms/step [Loss]9.9166 [Metrics]{'train_loss:9.916578 train_acc:0.063427 lr:0.000857 grad_accum:5.000000 total_samples:1552224.000000'}\n",
      "[Train Epoch]1/2 [Time]4259.57 [Step]9800 [Batch]49000 [Speed]86.93ms/step [Loss]9.9045 [Metrics]{'train_loss:9.904541 train_acc:0.063576 lr:0.000866 grad_accum:5.000000 total_samples:1568224.000000'}\n",
      "[Train Epoch]1/2 [Time]4301.49 [Step]9900 [Batch]49500 [Speed]86.90ms/step [Loss]9.8934 [Metrics]{'train_loss:9.893445 train_acc:0.063728 lr:0.000875 grad_accum:5.000000 total_samples:1584224.000000'}\n",
      "Saving checkpoint for epoch 1 at step 50000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]4356.27 [Step]10000 [Batch]50000 [Speed]87.13ms/step [Loss]9.8827 [Metrics]{'train_loss:9.882698 train_acc:0.063861 lr:0.000884 grad_accum:5.000000 total_samples:1600224.000000'}\n",
      "[Train Epoch]1/2 [Time]4397.76 [Step]10100 [Batch]50500 [Speed]87.08ms/step [Loss]9.8725 [Metrics]{'train_loss:9.872457 train_acc:0.063980 lr:0.000879 grad_accum:5.000000 total_samples:1616224.000000'}\n",
      "[Train Epoch]1/2 [Time]4439.00 [Step]10200 [Batch]51000 [Speed]87.04ms/step [Loss]9.8610 [Metrics]{'train_loss:9.861035 train_acc:0.064093 lr:0.000875 grad_accum:5.000000 total_samples:1632224.000000'}\n",
      "[Train Epoch]1/2 [Time]4480.67 [Step]10300 [Batch]51500 [Speed]87.00ms/step [Loss]9.8501 [Metrics]{'train_loss:9.850135 train_acc:0.064199 lr:0.000871 grad_accum:5.000000 total_samples:1648224.000000'}\n",
      "[Train Epoch]1/2 [Time]4522.49 [Step]10400 [Batch]52000 [Speed]86.97ms/step [Loss]9.8391 [Metrics]{'train_loss:9.839131 train_acc:0.064317 lr:0.000867 grad_accum:5.000000 total_samples:1664224.000000'}\n",
      "[Train Epoch]1/2 [Time]4564.66 [Step]10500 [Batch]52500 [Speed]86.95ms/step [Loss]9.8286 [Metrics]{'train_loss:9.828571 train_acc:0.064438 lr:0.000863 grad_accum:5.000000 total_samples:1680224.000000'}\n",
      "[Train Epoch]1/2 [Time]4606.12 [Step]10600 [Batch]53000 [Speed]86.91ms/step [Loss]9.8181 [Metrics]{'train_loss:9.818077 train_acc:0.064565 lr:0.000859 grad_accum:5.000000 total_samples:1696224.000000'}\n",
      "[Train Epoch]1/2 [Time]4648.10 [Step]10700 [Batch]53500 [Speed]86.88ms/step [Loss]9.8078 [Metrics]{'train_loss:9.807826 train_acc:0.064659 lr:0.000854 grad_accum:5.000000 total_samples:1712224.000000'}\n",
      "[Train Epoch]1/2 [Time]4690.05 [Step]10800 [Batch]54000 [Speed]86.85ms/step [Loss]9.7978 [Metrics]{'train_loss:9.797838 train_acc:0.064760 lr:0.000851 grad_accum:5.000000 total_samples:1728224.000000'}\n",
      "[Train Epoch]1/2 [Time]4731.84 [Step]10900 [Batch]54500 [Speed]86.82ms/step [Loss]9.7874 [Metrics]{'train_loss:9.787415 train_acc:0.064881 lr:0.000847 grad_accum:5.000000 total_samples:1744224.000000'}\n",
      "[Train Epoch]1/2 [Time]4773.45 [Step]11000 [Batch]55000 [Speed]86.79ms/step [Loss]9.7777 [Metrics]{'train_loss:9.777651 train_acc:0.064973 lr:0.000843 grad_accum:5.000000 total_samples:1760224.000000'}\n",
      "[Train Epoch]1/2 [Time]4814.67 [Step]11100 [Batch]55500 [Speed]86.75ms/step [Loss]9.7678 [Metrics]{'train_loss:9.767756 train_acc:0.065062 lr:0.000839 grad_accum:5.000000 total_samples:1776224.000000'}\n",
      "[Train Epoch]1/2 [Time]4856.10 [Step]11200 [Batch]56000 [Speed]86.72ms/step [Loss]9.7577 [Metrics]{'train_loss:9.757674 train_acc:0.065176 lr:0.000835 grad_accum:5.000000 total_samples:1792224.000000'}\n",
      "[Train Epoch]1/2 [Time]4897.88 [Step]11300 [Batch]56500 [Speed]86.69ms/step [Loss]9.7474 [Metrics]{'train_loss:9.747403 train_acc:0.065286 lr:0.000831 grad_accum:5.000000 total_samples:1808224.000000'}\n",
      "[Train Epoch]1/2 [Time]4939.79 [Step]11400 [Batch]57000 [Speed]86.66ms/step [Loss]9.7376 [Metrics]{'train_loss:9.737597 train_acc:0.065404 lr:0.000828 grad_accum:5.000000 total_samples:1824224.000000'}\n",
      "[Train Epoch]1/2 [Time]4981.67 [Step]11500 [Batch]57500 [Speed]86.64ms/step [Loss]9.7275 [Metrics]{'train_loss:9.727515 train_acc:0.065511 lr:0.000824 grad_accum:5.000000 total_samples:1840224.000000'}\n",
      "[Train Epoch]1/2 [Time]5023.18 [Step]11600 [Batch]58000 [Speed]86.61ms/step [Loss]9.7186 [Metrics]{'train_loss:9.718563 train_acc:0.065611 lr:0.000821 grad_accum:5.000000 total_samples:1856224.000000'}\n",
      "[Train Epoch]1/2 [Time]5064.97 [Step]11700 [Batch]58500 [Speed]86.58ms/step [Loss]9.7092 [Metrics]{'train_loss:9.709245 train_acc:0.065705 lr:0.000817 grad_accum:5.000000 total_samples:1872224.000000'}\n",
      "[Train Epoch]1/2 [Time]5106.61 [Step]11800 [Batch]59000 [Speed]86.55ms/step [Loss]9.7002 [Metrics]{'train_loss:9.700191 train_acc:0.065795 lr:0.000814 grad_accum:5.000000 total_samples:1888224.000000'}\n",
      "[Train Epoch]1/2 [Time]5148.16 [Step]11900 [Batch]59500 [Speed]86.52ms/step [Loss]9.6915 [Metrics]{'train_loss:9.691540 train_acc:0.065909 lr:0.000810 grad_accum:5.000000 total_samples:1904224.000000'}\n",
      "[Train Epoch]1/2 [Time]5189.88 [Step]12000 [Batch]60000 [Speed]86.50ms/step [Loss]9.6832 [Metrics]{'train_loss:9.683244 train_acc:0.066013 lr:0.000807 grad_accum:5.000000 total_samples:1920224.000000'}\n",
      "[Train Epoch]1/2 [Time]5231.34 [Step]12100 [Batch]60500 [Speed]86.47ms/step [Loss]9.6747 [Metrics]{'train_loss:9.674700 train_acc:0.066081 lr:0.000804 grad_accum:5.000000 total_samples:1936224.000000'}\n",
      "[Train Epoch]1/2 [Time]5272.77 [Step]12200 [Batch]61000 [Speed]86.44ms/step [Loss]9.6660 [Metrics]{'train_loss:9.666004 train_acc:0.066158 lr:0.000800 grad_accum:5.000000 total_samples:1952224.000000'}\n",
      "[Train Epoch]1/2 [Time]5314.67 [Step]12300 [Batch]61500 [Speed]86.42ms/step [Loss]9.6568 [Metrics]{'train_loss:9.656836 train_acc:0.066251 lr:0.000797 grad_accum:5.000000 total_samples:1968224.000000'}\n",
      "[Train Epoch]1/2 [Time]5356.65 [Step]12400 [Batch]62000 [Speed]86.40ms/step [Loss]9.6486 [Metrics]{'train_loss:9.648552 train_acc:0.066333 lr:0.000794 grad_accum:5.000000 total_samples:1984224.000000'}\n",
      "[Train Epoch]1/2 [Time]5398.54 [Step]12500 [Batch]62500 [Speed]86.38ms/step [Loss]9.6404 [Metrics]{'train_loss:9.640427 train_acc:0.066425 lr:0.000791 grad_accum:5.000000 total_samples:2000224.000000'}\n",
      "[Train Epoch]1/2 [Time]5439.92 [Step]12600 [Batch]63000 [Speed]86.35ms/step [Loss]9.6324 [Metrics]{'train_loss:9.632365 train_acc:0.066504 lr:0.000787 grad_accum:5.000000 total_samples:2016224.000000'}\n",
      "[Train Epoch]1/2 [Time]5481.41 [Step]12700 [Batch]63500 [Speed]86.32ms/step [Loss]9.6240 [Metrics]{'train_loss:9.623993 train_acc:0.066591 lr:0.000784 grad_accum:5.000000 total_samples:2032224.000000'}\n",
      "[Train Epoch]1/2 [Time]5523.11 [Step]12800 [Batch]64000 [Speed]86.30ms/step [Loss]9.6156 [Metrics]{'train_loss:9.615599 train_acc:0.066669 lr:0.000781 grad_accum:5.000000 total_samples:2048224.000000'}\n",
      "[Train Epoch]1/2 [Time]5565.04 [Step]12900 [Batch]64500 [Speed]86.28ms/step [Loss]9.6077 [Metrics]{'train_loss:9.607669 train_acc:0.066751 lr:0.000778 grad_accum:5.000000 total_samples:2064224.000000'}\n",
      "[Train Epoch]1/2 [Time]5607.13 [Step]13000 [Batch]65000 [Speed]86.26ms/step [Loss]9.5992 [Metrics]{'train_loss:9.599179 train_acc:0.066835 lr:0.000775 grad_accum:5.000000 total_samples:2080224.000000'}\n",
      "[Train Epoch]1/2 [Time]5649.24 [Step]13100 [Batch]65500 [Speed]86.25ms/step [Loss]9.5909 [Metrics]{'train_loss:9.590868 train_acc:0.066911 lr:0.000772 grad_accum:5.000000 total_samples:2096224.000000'}\n",
      "[Train Epoch]1/2 [Time]5690.87 [Step]13200 [Batch]66000 [Speed]86.23ms/step [Loss]9.5829 [Metrics]{'train_loss:9.582891 train_acc:0.066996 lr:0.000769 grad_accum:5.000000 total_samples:2112224.000000'}\n",
      "[Train Epoch]1/2 [Time]5732.74 [Step]13300 [Batch]66500 [Speed]86.21ms/step [Loss]9.5747 [Metrics]{'train_loss:9.574658 train_acc:0.067090 lr:0.000766 grad_accum:5.000000 total_samples:2128224.000000'}\n",
      "[Train Epoch]1/2 [Time]5774.47 [Step]13400 [Batch]67000 [Speed]86.19ms/step [Loss]9.5668 [Metrics]{'train_loss:9.566810 train_acc:0.067148 lr:0.000764 grad_accum:5.000000 total_samples:2144224.000000'}\n",
      "[Train Epoch]1/2 [Time]5815.83 [Step]13500 [Batch]67500 [Speed]86.16ms/step [Loss]9.5589 [Metrics]{'train_loss:9.558856 train_acc:0.067217 lr:0.000761 grad_accum:5.000000 total_samples:2160224.000000'}\n",
      "[Train Epoch]1/2 [Time]5857.15 [Step]13600 [Batch]68000 [Speed]86.13ms/step [Loss]9.5511 [Metrics]{'train_loss:9.551069 train_acc:0.067304 lr:0.000758 grad_accum:5.000000 total_samples:2176224.000000'}\n",
      "[Train Epoch]1/2 [Time]5899.01 [Step]13700 [Batch]68500 [Speed]86.12ms/step [Loss]9.5433 [Metrics]{'train_loss:9.543301 train_acc:0.067381 lr:0.000755 grad_accum:5.000000 total_samples:2192224.000000'}\n",
      "[Train Epoch]1/2 [Time]5940.88 [Step]13800 [Batch]69000 [Speed]86.10ms/step [Loss]9.5352 [Metrics]{'train_loss:9.535222 train_acc:0.067472 lr:0.000752 grad_accum:5.000000 total_samples:2208224.000000'}\n",
      "[Train Epoch]1/2 [Time]5982.76 [Step]13900 [Batch]69500 [Speed]86.08ms/step [Loss]9.5275 [Metrics]{'train_loss:9.527483 train_acc:0.067561 lr:0.000750 grad_accum:5.000000 total_samples:2224224.000000'}\n",
      "[Train Epoch]1/2 [Time]6024.64 [Step]14000 [Batch]70000 [Speed]86.07ms/step [Loss]9.5198 [Metrics]{'train_loss:9.519840 train_acc:0.067636 lr:0.000747 grad_accum:5.000000 total_samples:2240224.000000'}\n",
      "[Train Epoch]1/2 [Time]6066.17 [Step]14100 [Batch]70500 [Speed]86.04ms/step [Loss]9.5120 [Metrics]{'train_loss:9.511988 train_acc:0.067705 lr:0.000744 grad_accum:5.000000 total_samples:2256224.000000'}\n",
      "[Train Epoch]1/2 [Time]6107.55 [Step]14200 [Batch]71000 [Speed]86.02ms/step [Loss]9.5044 [Metrics]{'train_loss:9.504402 train_acc:0.067777 lr:0.000742 grad_accum:5.000000 total_samples:2272224.000000'}\n",
      "[Train Epoch]1/2 [Time]6149.11 [Step]14300 [Batch]71500 [Speed]86.00ms/step [Loss]9.4965 [Metrics]{'train_loss:9.496521 train_acc:0.067850 lr:0.000739 grad_accum:5.000000 total_samples:2288224.000000'}\n",
      "[Train Epoch]1/2 [Time]6190.65 [Step]14400 [Batch]72000 [Speed]85.98ms/step [Loss]9.4888 [Metrics]{'train_loss:9.488791 train_acc:0.067919 lr:0.000737 grad_accum:5.000000 total_samples:2304224.000000'}\n",
      "[Train Epoch]1/2 [Time]6232.50 [Step]14500 [Batch]72500 [Speed]85.97ms/step [Loss]9.4813 [Metrics]{'train_loss:9.481329 train_acc:0.067985 lr:0.000734 grad_accum:5.000000 total_samples:2320224.000000'}\n",
      "[Train Epoch]1/2 [Time]6273.99 [Step]14600 [Batch]73000 [Speed]85.95ms/step [Loss]9.4738 [Metrics]{'train_loss:9.473780 train_acc:0.068052 lr:0.000732 grad_accum:5.000000 total_samples:2336224.000000'}\n",
      "[Train Epoch]1/2 [Time]6315.77 [Step]14700 [Batch]73500 [Speed]85.93ms/step [Loss]9.4664 [Metrics]{'train_loss:9.466403 train_acc:0.068127 lr:0.000729 grad_accum:5.000000 total_samples:2352224.000000'}\n",
      "[Train Epoch]1/2 [Time]6357.71 [Step]14800 [Batch]74000 [Speed]85.91ms/step [Loss]9.4598 [Metrics]{'train_loss:9.459777 train_acc:0.068190 lr:0.000727 grad_accum:5.000000 total_samples:2368224.000000'}\n",
      "[Train Epoch]1/2 [Time]6399.70 [Step]14900 [Batch]74500 [Speed]85.90ms/step [Loss]9.4532 [Metrics]{'train_loss:9.453218 train_acc:0.068261 lr:0.000724 grad_accum:5.000000 total_samples:2384224.000000'}\n",
      "Saving checkpoint for epoch 1 at step 75000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]6449.93 [Step]15000 [Batch]75000 [Speed]86.00ms/step [Loss]9.4467 [Metrics]{'train_loss:9.446655 train_acc:0.068355 lr:0.000722 grad_accum:5.000000 total_samples:2400224.000000'}\n",
      "[Train Epoch]1/2 [Time]6491.38 [Step]15100 [Batch]75500 [Speed]85.98ms/step [Loss]9.4408 [Metrics]{'train_loss:9.440764 train_acc:0.068411 lr:0.000719 grad_accum:5.000000 total_samples:2416224.000000'}\n",
      "[Train Epoch]1/2 [Time]6532.72 [Step]15200 [Batch]76000 [Speed]85.96ms/step [Loss]9.4341 [Metrics]{'train_loss:9.434150 train_acc:0.068481 lr:0.000717 grad_accum:5.000000 total_samples:2432224.000000'}\n",
      "[Train Epoch]1/2 [Time]6574.33 [Step]15300 [Batch]76500 [Speed]85.94ms/step [Loss]9.4275 [Metrics]{'train_loss:9.427485 train_acc:0.068557 lr:0.000715 grad_accum:5.000000 total_samples:2448224.000000'}\n",
      "[Train Epoch]1/2 [Time]6616.05 [Step]15400 [Batch]77000 [Speed]85.92ms/step [Loss]9.4215 [Metrics]{'train_loss:9.421488 train_acc:0.068618 lr:0.000712 grad_accum:5.000000 total_samples:2464224.000000'}\n",
      "[Train Epoch]1/2 [Time]6657.91 [Step]15500 [Batch]77500 [Speed]85.91ms/step [Loss]9.4156 [Metrics]{'train_loss:9.415586 train_acc:0.068685 lr:0.000710 grad_accum:5.000000 total_samples:2480224.000000'}\n",
      "[Train Epoch]1/2 [Time]6699.52 [Step]15600 [Batch]78000 [Speed]85.89ms/step [Loss]9.4098 [Metrics]{'train_loss:9.409765 train_acc:0.068737 lr:0.000708 grad_accum:5.000000 total_samples:2496224.000000'}\n",
      "[Train Epoch]1/2 [Time]6741.31 [Step]15700 [Batch]78500 [Speed]85.88ms/step [Loss]9.4038 [Metrics]{'train_loss:9.403755 train_acc:0.068797 lr:0.000705 grad_accum:5.000000 total_samples:2512224.000000'}\n",
      "[Train Epoch]1/2 [Time]6783.25 [Step]15800 [Batch]79000 [Speed]85.86ms/step [Loss]9.3978 [Metrics]{'train_loss:9.397799 train_acc:0.068842 lr:0.000703 grad_accum:5.000000 total_samples:2528224.000000'}\n",
      "[Train Epoch]1/2 [Time]6825.20 [Step]15900 [Batch]79500 [Speed]85.85ms/step [Loss]9.3920 [Metrics]{'train_loss:9.391966 train_acc:0.068894 lr:0.000701 grad_accum:5.000000 total_samples:2544224.000000'}\n",
      "[Train Epoch]1/2 [Time]6867.04 [Step]16000 [Batch]80000 [Speed]85.84ms/step [Loss]9.3862 [Metrics]{'train_loss:9.386221 train_acc:0.068976 lr:0.000699 grad_accum:5.000000 total_samples:2560224.000000'}\n",
      "[Train Epoch]1/2 [Time]6908.38 [Step]16100 [Batch]80500 [Speed]85.82ms/step [Loss]9.3808 [Metrics]{'train_loss:9.380755 train_acc:0.069037 lr:0.000697 grad_accum:5.000000 total_samples:2576224.000000'}\n",
      "[Train Epoch]1/2 [Time]6949.81 [Step]16200 [Batch]81000 [Speed]85.80ms/step [Loss]9.3752 [Metrics]{'train_loss:9.375216 train_acc:0.069109 lr:0.000694 grad_accum:5.000000 total_samples:2592224.000000'}\n",
      "[Train Epoch]1/2 [Time]6991.55 [Step]16300 [Batch]81500 [Speed]85.79ms/step [Loss]9.3688 [Metrics]{'train_loss:9.368836 train_acc:0.069176 lr:0.000692 grad_accum:5.000000 total_samples:2608224.000000'}\n",
      "[Train Epoch]1/2 [Time]7033.48 [Step]16400 [Batch]82000 [Speed]85.77ms/step [Loss]9.3629 [Metrics]{'train_loss:9.362864 train_acc:0.069220 lr:0.000690 grad_accum:5.000000 total_samples:2624224.000000'}\n",
      "[Train Epoch]1/2 [Time]7075.31 [Step]16500 [Batch]82500 [Speed]85.76ms/step [Loss]9.3570 [Metrics]{'train_loss:9.357042 train_acc:0.069277 lr:0.000688 grad_accum:5.000000 total_samples:2640224.000000'}\n",
      "[Train Epoch]1/2 [Time]7116.99 [Step]16600 [Batch]83000 [Speed]85.75ms/step [Loss]9.3511 [Metrics]{'train_loss:9.351110 train_acc:0.069349 lr:0.000686 grad_accum:5.000000 total_samples:2656224.000000'}\n",
      "[Train Epoch]1/2 [Time]7159.03 [Step]16700 [Batch]83500 [Speed]85.74ms/step [Loss]9.3453 [Metrics]{'train_loss:9.345344 train_acc:0.069408 lr:0.000684 grad_accum:5.000000 total_samples:2672224.000000'}\n",
      "[Train Epoch]1/2 [Time]7200.99 [Step]16800 [Batch]84000 [Speed]85.73ms/step [Loss]9.3394 [Metrics]{'train_loss:9.339400 train_acc:0.069472 lr:0.000682 grad_accum:5.000000 total_samples:2688224.000000'}\n",
      "[Train Epoch]1/2 [Time]7242.93 [Step]16900 [Batch]84500 [Speed]85.72ms/step [Loss]9.3337 [Metrics]{'train_loss:9.333668 train_acc:0.069533 lr:0.000680 grad_accum:5.000000 total_samples:2704224.000000'}\n",
      "[Train Epoch]1/2 [Time]7284.40 [Step]17000 [Batch]85000 [Speed]85.70ms/step [Loss]9.3281 [Metrics]{'train_loss:9.328103 train_acc:0.069578 lr:0.000678 grad_accum:5.000000 total_samples:2720224.000000'}\n",
      "[Train Epoch]1/2 [Time]7325.75 [Step]17100 [Batch]85500 [Speed]85.68ms/step [Loss]9.3226 [Metrics]{'train_loss:9.322563 train_acc:0.069634 lr:0.000676 grad_accum:5.000000 total_samples:2736224.000000'}\n",
      "[Train Epoch]1/2 [Time]7367.07 [Step]17200 [Batch]86000 [Speed]85.66ms/step [Loss]9.3169 [Metrics]{'train_loss:9.316933 train_acc:0.069676 lr:0.000674 grad_accum:5.000000 total_samples:2752224.000000'}\n",
      "[Train Epoch]1/2 [Time]7408.93 [Step]17300 [Batch]86500 [Speed]85.65ms/step [Loss]9.3118 [Metrics]{'train_loss:9.311790 train_acc:0.069741 lr:0.000672 grad_accum:5.000000 total_samples:2768224.000000'}\n",
      "[Train Epoch]1/2 [Time]7450.89 [Step]17400 [Batch]87000 [Speed]85.64ms/step [Loss]9.3065 [Metrics]{'train_loss:9.306516 train_acc:0.069797 lr:0.000670 grad_accum:5.000000 total_samples:2784224.000000'}\n",
      "[Train Epoch]1/2 [Time]7492.71 [Step]17500 [Batch]87500 [Speed]85.63ms/step [Loss]9.3013 [Metrics]{'train_loss:9.301318 train_acc:0.069858 lr:0.000668 grad_accum:5.000000 total_samples:2800224.000000'}\n",
      "[Train Epoch]1/2 [Time]7534.30 [Step]17600 [Batch]88000 [Speed]85.62ms/step [Loss]9.2958 [Metrics]{'train_loss:9.295826 train_acc:0.069918 lr:0.000666 grad_accum:5.000000 total_samples:2816224.000000'}\n",
      "[Train Epoch]1/2 [Time]7576.19 [Step]17700 [Batch]88500 [Speed]85.61ms/step [Loss]9.2902 [Metrics]{'train_loss:9.290249 train_acc:0.069962 lr:0.000664 grad_accum:5.000000 total_samples:2832224.000000'}\n",
      "[Train Epoch]1/2 [Time]7617.87 [Step]17800 [Batch]89000 [Speed]85.59ms/step [Loss]9.2844 [Metrics]{'train_loss:9.284433 train_acc:0.070020 lr:0.000662 grad_accum:5.000000 total_samples:2848224.000000'}\n",
      "[Train Epoch]1/2 [Time]7659.43 [Step]17900 [Batch]89500 [Speed]85.58ms/step [Loss]9.2789 [Metrics]{'train_loss:9.278932 train_acc:0.070071 lr:0.000661 grad_accum:5.000000 total_samples:2864224.000000'}\n",
      "[Train Epoch]1/2 [Time]7700.75 [Step]18000 [Batch]90000 [Speed]85.56ms/step [Loss]9.2736 [Metrics]{'train_loss:9.273586 train_acc:0.070118 lr:0.000659 grad_accum:5.000000 total_samples:2880224.000000'}\n",
      "[Train Epoch]1/2 [Time]7742.02 [Step]18100 [Batch]90500 [Speed]85.55ms/step [Loss]9.2682 [Metrics]{'train_loss:9.268231 train_acc:0.070175 lr:0.000657 grad_accum:5.000000 total_samples:2896224.000000'}\n",
      "[Train Epoch]1/2 [Time]7783.63 [Step]18200 [Batch]91000 [Speed]85.53ms/step [Loss]9.2632 [Metrics]{'train_loss:9.263222 train_acc:0.070228 lr:0.000655 grad_accum:5.000000 total_samples:2912224.000000'}\n",
      "[Train Epoch]1/2 [Time]7825.67 [Step]18300 [Batch]91500 [Speed]85.53ms/step [Loss]9.2581 [Metrics]{'train_loss:9.258131 train_acc:0.070282 lr:0.000653 grad_accum:5.000000 total_samples:2928224.000000'}\n",
      "[Train Epoch]1/2 [Time]7867.64 [Step]18400 [Batch]92000 [Speed]85.52ms/step [Loss]9.2533 [Metrics]{'train_loss:9.253344 train_acc:0.070337 lr:0.000652 grad_accum:5.000000 total_samples:2944224.000000'}\n",
      "[Train Epoch]1/2 [Time]7909.49 [Step]18500 [Batch]92500 [Speed]85.51ms/step [Loss]9.2485 [Metrics]{'train_loss:9.248527 train_acc:0.070389 lr:0.000650 grad_accum:5.000000 total_samples:2960224.000000'}\n",
      "[Train Epoch]1/2 [Time]7951.09 [Step]18600 [Batch]93000 [Speed]85.50ms/step [Loss]9.2434 [Metrics]{'train_loss:9.243359 train_acc:0.070435 lr:0.000648 grad_accum:5.000000 total_samples:2976224.000000'}\n",
      "[Train Epoch]1/2 [Time]7992.67 [Step]18700 [Batch]93500 [Speed]85.48ms/step [Loss]9.2382 [Metrics]{'train_loss:9.238229 train_acc:0.070487 lr:0.000646 grad_accum:5.000000 total_samples:2992224.000000'}\n",
      "[Train Epoch]1/2 [Time]8034.16 [Step]18800 [Batch]94000 [Speed]85.47ms/step [Loss]9.2333 [Metrics]{'train_loss:9.233347 train_acc:0.070525 lr:0.000645 grad_accum:5.000000 total_samples:3008224.000000'}\n",
      "[Train Epoch]1/2 [Time]8075.82 [Step]18900 [Batch]94500 [Speed]85.46ms/step [Loss]9.2287 [Metrics]{'train_loss:9.228671 train_acc:0.070574 lr:0.000643 grad_accum:5.000000 total_samples:3024224.000000'}\n",
      "[Train Epoch]1/2 [Time]8117.64 [Step]19000 [Batch]95000 [Speed]85.45ms/step [Loss]9.2237 [Metrics]{'train_loss:9.223749 train_acc:0.070619 lr:0.000641 grad_accum:5.000000 total_samples:3040224.000000'}\n",
      "[Train Epoch]1/2 [Time]8158.96 [Step]19100 [Batch]95500 [Speed]85.43ms/step [Loss]9.2190 [Metrics]{'train_loss:9.219007 train_acc:0.070663 lr:0.000640 grad_accum:5.000000 total_samples:3056224.000000'}\n",
      "[Train Epoch]1/2 [Time]8200.81 [Step]19200 [Batch]96000 [Speed]85.43ms/step [Loss]9.2144 [Metrics]{'train_loss:9.214409 train_acc:0.070715 lr:0.000638 grad_accum:5.000000 total_samples:3072224.000000'}\n",
      "[Train Epoch]1/2 [Time]8242.76 [Step]19300 [Batch]96500 [Speed]85.42ms/step [Loss]9.2097 [Metrics]{'train_loss:9.209672 train_acc:0.070757 lr:0.000636 grad_accum:5.000000 total_samples:3088224.000000'}\n",
      "[Train Epoch]1/2 [Time]8284.68 [Step]19400 [Batch]97000 [Speed]85.41ms/step [Loss]9.2049 [Metrics]{'train_loss:9.204927 train_acc:0.070797 lr:0.000635 grad_accum:5.000000 total_samples:3104224.000000'}\n",
      "[Train Epoch]1/2 [Time]8326.33 [Step]19500 [Batch]97500 [Speed]85.40ms/step [Loss]9.2003 [Metrics]{'train_loss:9.200329 train_acc:0.070845 lr:0.000633 grad_accum:5.000000 total_samples:3120224.000000'}\n",
      "[Train Epoch]1/2 [Time]8367.79 [Step]19600 [Batch]98000 [Speed]85.39ms/step [Loss]9.1958 [Metrics]{'train_loss:9.195819 train_acc:0.070893 lr:0.000631 grad_accum:5.000000 total_samples:3136224.000000'}\n",
      "[Train Epoch]1/2 [Time]8409.69 [Step]19700 [Batch]98500 [Speed]85.38ms/step [Loss]9.1914 [Metrics]{'train_loss:9.191445 train_acc:0.070931 lr:0.000630 grad_accum:5.000000 total_samples:3152224.000000'}\n",
      "[Train Epoch]1/2 [Time]8451.22 [Step]19800 [Batch]99000 [Speed]85.37ms/step [Loss]9.1867 [Metrics]{'train_loss:9.186660 train_acc:0.070977 lr:0.000628 grad_accum:5.000000 total_samples:3168224.000000'}\n",
      "[Train Epoch]1/2 [Time]8489.55 [Step]19900 [Batch]99500 [Speed]85.32ms/step [Loss]9.1823 [Metrics]{'train_loss:9.182323 train_acc:0.071018 lr:0.000627 grad_accum:5.000000 total_samples:3184224.000000'}\n",
      "Saving checkpoint for epoch 1 at step 100000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]8535.99 [Step]20000 [Batch]100000 [Speed]85.36ms/step [Loss]9.1777 [Metrics]{'train_loss:9.177740 train_acc:0.071063 lr:0.000625 grad_accum:5.000000 total_samples:3200224.000000'}\n",
      "[Train Epoch]1/2 [Time]8574.20 [Step]20100 [Batch]100500 [Speed]85.32ms/step [Loss]9.1732 [Metrics]{'train_loss:9.173170 train_acc:0.071116 lr:0.000623 grad_accum:5.000000 total_samples:3216224.000000'}\n",
      "[Train Epoch]1/2 [Time]8612.41 [Step]20200 [Batch]101000 [Speed]85.27ms/step [Loss]9.1687 [Metrics]{'train_loss:9.168714 train_acc:0.071166 lr:0.000622 grad_accum:5.000000 total_samples:3232224.000000'}\n",
      "[Train Epoch]1/2 [Time]8650.74 [Step]20300 [Batch]101500 [Speed]85.23ms/step [Loss]9.1641 [Metrics]{'train_loss:9.164114 train_acc:0.071205 lr:0.000620 grad_accum:5.000000 total_samples:3248224.000000'}\n",
      "[Train Epoch]1/2 [Time]8689.06 [Step]20400 [Batch]102000 [Speed]85.19ms/step [Loss]9.1599 [Metrics]{'train_loss:9.159944 train_acc:0.071256 lr:0.000619 grad_accum:5.000000 total_samples:3264224.000000'}\n",
      "[Train Epoch]1/2 [Time]8727.39 [Step]20500 [Batch]102500 [Speed]85.15ms/step [Loss]9.1557 [Metrics]{'train_loss:9.155712 train_acc:0.071298 lr:0.000617 grad_accum:5.000000 total_samples:3280224.000000'}\n",
      "[Train Epoch]1/2 [Time]8765.63 [Step]20600 [Batch]103000 [Speed]85.10ms/step [Loss]9.1517 [Metrics]{'train_loss:9.151683 train_acc:0.071339 lr:0.000616 grad_accum:5.000000 total_samples:3296224.000000'}\n",
      "[Train Epoch]1/2 [Time]8803.90 [Step]20700 [Batch]103500 [Speed]85.06ms/step [Loss]9.1473 [Metrics]{'train_loss:9.147256 train_acc:0.071376 lr:0.000614 grad_accum:5.000000 total_samples:3312224.000000'}\n",
      "[Train Epoch]1/2 [Time]8842.15 [Step]20800 [Batch]104000 [Speed]85.02ms/step [Loss]9.1431 [Metrics]{'train_loss:9.143078 train_acc:0.071423 lr:0.000613 grad_accum:5.000000 total_samples:3328224.000000'}\n",
      "[Train Epoch]1/2 [Time]8880.46 [Step]20900 [Batch]104500 [Speed]84.98ms/step [Loss]9.1391 [Metrics]{'train_loss:9.139071 train_acc:0.071453 lr:0.000611 grad_accum:5.000000 total_samples:3344224.000000'}\n",
      "[Train Epoch]1/2 [Time]8918.71 [Step]21000 [Batch]105000 [Speed]84.94ms/step [Loss]9.1347 [Metrics]{'train_loss:9.134739 train_acc:0.071493 lr:0.000610 grad_accum:5.000000 total_samples:3360224.000000'}\n",
      "[Train Epoch]1/2 [Time]8957.02 [Step]21100 [Batch]105500 [Speed]84.90ms/step [Loss]9.1303 [Metrics]{'train_loss:9.130324 train_acc:0.071543 lr:0.000608 grad_accum:5.000000 total_samples:3376224.000000'}\n",
      "[Train Epoch]1/2 [Time]8995.34 [Step]21200 [Batch]106000 [Speed]84.86ms/step [Loss]9.1260 [Metrics]{'train_loss:9.125996 train_acc:0.071588 lr:0.000607 grad_accum:5.000000 total_samples:3392224.000000'}\n",
      "[Train Epoch]1/2 [Time]9033.63 [Step]21300 [Batch]106500 [Speed]84.82ms/step [Loss]9.1218 [Metrics]{'train_loss:9.121761 train_acc:0.071630 lr:0.000606 grad_accum:5.000000 total_samples:3408224.000000'}\n",
      "[Train Epoch]1/2 [Time]9071.92 [Step]21400 [Batch]107000 [Speed]84.78ms/step [Loss]9.1176 [Metrics]{'train_loss:9.117619 train_acc:0.071666 lr:0.000604 grad_accum:5.000000 total_samples:3424224.000000'}\n",
      "[Train Epoch]1/2 [Time]9110.24 [Step]21500 [Batch]107500 [Speed]84.75ms/step [Loss]9.1135 [Metrics]{'train_loss:9.113498 train_acc:0.071701 lr:0.000603 grad_accum:5.000000 total_samples:3440224.000000'}\n",
      "[Train Epoch]1/2 [Time]9148.54 [Step]21600 [Batch]108000 [Speed]84.71ms/step [Loss]9.1096 [Metrics]{'train_loss:9.109584 train_acc:0.071743 lr:0.000601 grad_accum:5.000000 total_samples:3456224.000000'}\n",
      "[Train Epoch]1/2 [Time]9186.79 [Step]21700 [Batch]108500 [Speed]84.67ms/step [Loss]9.1058 [Metrics]{'train_loss:9.105761 train_acc:0.071781 lr:0.000600 grad_accum:5.000000 total_samples:3472224.000000'}\n",
      "[Train Epoch]1/2 [Time]9225.10 [Step]21800 [Batch]109000 [Speed]84.63ms/step [Loss]9.1016 [Metrics]{'train_loss:9.101622 train_acc:0.071827 lr:0.000599 grad_accum:5.000000 total_samples:3488224.000000'}\n",
      "[Train Epoch]1/2 [Time]9263.37 [Step]21900 [Batch]109500 [Speed]84.60ms/step [Loss]9.0977 [Metrics]{'train_loss:9.097674 train_acc:0.071856 lr:0.000597 grad_accum:5.000000 total_samples:3504224.000000'}\n",
      "[Train Epoch]1/2 [Time]9301.68 [Step]22000 [Batch]110000 [Speed]84.56ms/step [Loss]9.0940 [Metrics]{'train_loss:9.093981 train_acc:0.071887 lr:0.000596 grad_accum:5.000000 total_samples:3520224.000000'}\n",
      "[Train Epoch]1/2 [Time]9340.00 [Step]22100 [Batch]110500 [Speed]84.52ms/step [Loss]9.0903 [Metrics]{'train_loss:9.090300 train_acc:0.071931 lr:0.000595 grad_accum:5.000000 total_samples:3536224.000000'}\n",
      "[Train Epoch]1/2 [Time]9378.21 [Step]22200 [Batch]111000 [Speed]84.49ms/step [Loss]9.0867 [Metrics]{'train_loss:9.086735 train_acc:0.071971 lr:0.000593 grad_accum:5.000000 total_samples:3552224.000000'}\n",
      "[Train Epoch]1/2 [Time]9416.45 [Step]22300 [Batch]111500 [Speed]84.45ms/step [Loss]9.0826 [Metrics]{'train_loss:9.082610 train_acc:0.072014 lr:0.000592 grad_accum:5.000000 total_samples:3568224.000000'}\n",
      "[Train Epoch]1/2 [Time]9454.72 [Step]22400 [Batch]112000 [Speed]84.42ms/step [Loss]9.0787 [Metrics]{'train_loss:9.078720 train_acc:0.072050 lr:0.000591 grad_accum:5.000000 total_samples:3584224.000000'}\n",
      "[Train Epoch]1/2 [Time]9492.99 [Step]22500 [Batch]112500 [Speed]84.38ms/step [Loss]9.0750 [Metrics]{'train_loss:9.075031 train_acc:0.072070 lr:0.000589 grad_accum:5.000000 total_samples:3600224.000000'}\n",
      "[Train Epoch]1/2 [Time]9531.26 [Step]22600 [Batch]113000 [Speed]84.35ms/step [Loss]9.0712 [Metrics]{'train_loss:9.071224 train_acc:0.072114 lr:0.000588 grad_accum:5.000000 total_samples:3616224.000000'}\n",
      "[Train Epoch]1/2 [Time]9569.53 [Step]22700 [Batch]113500 [Speed]84.31ms/step [Loss]9.0674 [Metrics]{'train_loss:9.067350 train_acc:0.072155 lr:0.000587 grad_accum:5.000000 total_samples:3632224.000000'}\n",
      "[Train Epoch]1/2 [Time]9607.85 [Step]22800 [Batch]114000 [Speed]84.28ms/step [Loss]9.0637 [Metrics]{'train_loss:9.063724 train_acc:0.072180 lr:0.000585 grad_accum:5.000000 total_samples:3648224.000000'}\n",
      "[Train Epoch]1/2 [Time]9646.13 [Step]22900 [Batch]114500 [Speed]84.25ms/step [Loss]9.0597 [Metrics]{'train_loss:9.059727 train_acc:0.072222 lr:0.000584 grad_accum:5.000000 total_samples:3664224.000000'}\n",
      "[Train Epoch]1/2 [Time]9684.38 [Step]23000 [Batch]115000 [Speed]84.21ms/step [Loss]9.0560 [Metrics]{'train_loss:9.055971 train_acc:0.072252 lr:0.000583 grad_accum:5.000000 total_samples:3680224.000000'}\n",
      "[Train Epoch]1/2 [Time]9722.63 [Step]23100 [Batch]115500 [Speed]84.18ms/step [Loss]9.0521 [Metrics]{'train_loss:9.052067 train_acc:0.072284 lr:0.000582 grad_accum:5.000000 total_samples:3696224.000000'}\n",
      "[Train Epoch]1/2 [Time]9760.88 [Step]23200 [Batch]116000 [Speed]84.15ms/step [Loss]9.0483 [Metrics]{'train_loss:9.048265 train_acc:0.072317 lr:0.000580 grad_accum:5.000000 total_samples:3712224.000000'}\n",
      "[Train Epoch]1/2 [Time]9799.09 [Step]23300 [Batch]116500 [Speed]84.11ms/step [Loss]9.0445 [Metrics]{'train_loss:9.044470 train_acc:0.072352 lr:0.000579 grad_accum:5.000000 total_samples:3728224.000000'}\n",
      "[Train Epoch]1/2 [Time]9837.31 [Step]23400 [Batch]117000 [Speed]84.08ms/step [Loss]9.0407 [Metrics]{'train_loss:9.040736 train_acc:0.072395 lr:0.000578 grad_accum:5.000000 total_samples:3744224.000000'}\n",
      "[Train Epoch]1/2 [Time]9876.69 [Step]23500 [Batch]117500 [Speed]84.06ms/step [Loss]9.0370 [Metrics]{'train_loss:9.037029 train_acc:0.072431 lr:0.000577 grad_accum:5.000000 total_samples:3760224.000000'}\n",
      "[Train Epoch]1/2 [Time]9917.94 [Step]23600 [Batch]118000 [Speed]84.05ms/step [Loss]9.0339 [Metrics]{'train_loss:9.033947 train_acc:0.072463 lr:0.000575 grad_accum:5.000000 total_samples:3776224.000000'}\n",
      "[Train Epoch]1/2 [Time]9956.26 [Step]23700 [Batch]118500 [Speed]84.02ms/step [Loss]9.0305 [Metrics]{'train_loss:9.030514 train_acc:0.072499 lr:0.000574 grad_accum:5.000000 total_samples:3792224.000000'}\n",
      "[Train Epoch]1/2 [Time]9994.50 [Step]23800 [Batch]119000 [Speed]83.99ms/step [Loss]9.0266 [Metrics]{'train_loss:9.026649 train_acc:0.072530 lr:0.000573 grad_accum:5.000000 total_samples:3808224.000000'}\n",
      "[Train Epoch]1/2 [Time]10032.82 [Step]23900 [Batch]119500 [Speed]83.96ms/step [Loss]9.0230 [Metrics]{'train_loss:9.023031 train_acc:0.072570 lr:0.000572 grad_accum:5.000000 total_samples:3824224.000000'}\n",
      "[Train Epoch]1/2 [Time]10071.05 [Step]24000 [Batch]120000 [Speed]83.93ms/step [Loss]9.0196 [Metrics]{'train_loss:9.019580 train_acc:0.072604 lr:0.000571 grad_accum:5.000000 total_samples:3840224.000000'}\n",
      "[Train Epoch]1/2 [Time]10109.34 [Step]24100 [Batch]120500 [Speed]83.89ms/step [Loss]9.0161 [Metrics]{'train_loss:9.016061 train_acc:0.072659 lr:0.000569 grad_accum:5.000000 total_samples:3856224.000000'}\n",
      "[Train Epoch]1/2 [Time]10147.61 [Step]24200 [Batch]121000 [Speed]83.86ms/step [Loss]9.0128 [Metrics]{'train_loss:9.012808 train_acc:0.072696 lr:0.000568 grad_accum:5.000000 total_samples:3872224.000000'}\n",
      "[Train Epoch]1/2 [Time]10185.91 [Step]24300 [Batch]121500 [Speed]83.83ms/step [Loss]9.0093 [Metrics]{'train_loss:9.009314 train_acc:0.072737 lr:0.000567 grad_accum:5.000000 total_samples:3888224.000000'}\n",
      "[Train Epoch]1/2 [Time]10224.16 [Step]24400 [Batch]122000 [Speed]83.80ms/step [Loss]9.0058 [Metrics]{'train_loss:9.005815 train_acc:0.072778 lr:0.000566 grad_accum:5.000000 total_samples:3904224.000000'}\n",
      "[Train Epoch]1/2 [Time]10262.39 [Step]24500 [Batch]122500 [Speed]83.77ms/step [Loss]9.0024 [Metrics]{'train_loss:9.002375 train_acc:0.072812 lr:0.000565 grad_accum:5.000000 total_samples:3920224.000000'}\n",
      "[Train Epoch]1/2 [Time]10300.65 [Step]24600 [Batch]123000 [Speed]83.75ms/step [Loss]8.9990 [Metrics]{'train_loss:8.999048 train_acc:0.072849 lr:0.000564 grad_accum:5.000000 total_samples:3936224.000000'}\n",
      "[Train Epoch]1/2 [Time]10338.93 [Step]24700 [Batch]123500 [Speed]83.72ms/step [Loss]8.9956 [Metrics]{'train_loss:8.995551 train_acc:0.072886 lr:0.000562 grad_accum:5.000000 total_samples:3952224.000000'}\n",
      "[Train Epoch]1/2 [Time]10377.21 [Step]24800 [Batch]124000 [Speed]83.69ms/step [Loss]8.9926 [Metrics]{'train_loss:8.992597 train_acc:0.072920 lr:0.000561 grad_accum:5.000000 total_samples:3968224.000000'}\n",
      "[Train Epoch]1/2 [Time]10415.47 [Step]24900 [Batch]124500 [Speed]83.66ms/step [Loss]8.9893 [Metrics]{'train_loss:8.989347 train_acc:0.072963 lr:0.000560 grad_accum:5.000000 total_samples:3984224.000000'}\n",
      "Saving checkpoint for epoch 1 at step 125000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]10461.87 [Step]25000 [Batch]125000 [Speed]83.69ms/step [Loss]8.9861 [Metrics]{'train_loss:8.986121 train_acc:0.072984 lr:0.000559 grad_accum:5.000000 total_samples:4000224.000000'}\n",
      "[Train Epoch]1/2 [Time]10500.20 [Step]25100 [Batch]125500 [Speed]83.67ms/step [Loss]8.9829 [Metrics]{'train_loss:8.982856 train_acc:0.073006 lr:0.000558 grad_accum:5.000000 total_samples:4016224.000000'}\n",
      "[Train Epoch]1/2 [Time]10538.47 [Step]25200 [Batch]126000 [Speed]83.64ms/step [Loss]8.9795 [Metrics]{'train_loss:8.979492 train_acc:0.073035 lr:0.000557 grad_accum:5.000000 total_samples:4032224.000000'}\n",
      "[Train Epoch]1/2 [Time]10576.73 [Step]25300 [Batch]126500 [Speed]83.61ms/step [Loss]8.9765 [Metrics]{'train_loss:8.976508 train_acc:0.073067 lr:0.000556 grad_accum:5.000000 total_samples:4048224.000000'}\n",
      "[Train Epoch]1/2 [Time]10614.98 [Step]25400 [Batch]127000 [Speed]83.58ms/step [Loss]8.9734 [Metrics]{'train_loss:8.973362 train_acc:0.073091 lr:0.000555 grad_accum:5.000000 total_samples:4064224.000000'}\n",
      "[Train Epoch]1/2 [Time]10653.22 [Step]25500 [Batch]127500 [Speed]83.55ms/step [Loss]8.9701 [Metrics]{'train_loss:8.970097 train_acc:0.073119 lr:0.000554 grad_accum:5.000000 total_samples:4080224.000000'}\n",
      "[Train Epoch]1/2 [Time]10691.49 [Step]25600 [Batch]128000 [Speed]83.53ms/step [Loss]8.9671 [Metrics]{'train_loss:8.967093 train_acc:0.073147 lr:0.000552 grad_accum:5.000000 total_samples:4096224.000000'}\n",
      "[Train Epoch]1/2 [Time]10729.77 [Step]25700 [Batch]128500 [Speed]83.50ms/step [Loss]8.9640 [Metrics]{'train_loss:8.964039 train_acc:0.073182 lr:0.000551 grad_accum:5.000000 total_samples:4112224.000000'}\n",
      "[Train Epoch]1/2 [Time]10768.08 [Step]25800 [Batch]129000 [Speed]83.47ms/step [Loss]8.9610 [Metrics]{'train_loss:8.961036 train_acc:0.073211 lr:0.000550 grad_accum:5.000000 total_samples:4128224.000000'}\n",
      "[Train Epoch]1/2 [Time]10806.37 [Step]25900 [Batch]129500 [Speed]83.45ms/step [Loss]8.9579 [Metrics]{'train_loss:8.957869 train_acc:0.073240 lr:0.000549 grad_accum:5.000000 total_samples:4144224.000000'}\n",
      "[Train Epoch]1/2 [Time]10844.65 [Step]26000 [Batch]130000 [Speed]83.42ms/step [Loss]8.9548 [Metrics]{'train_loss:8.954761 train_acc:0.073272 lr:0.000548 grad_accum:5.000000 total_samples:4160224.000000'}\n",
      "[Train Epoch]1/2 [Time]10882.91 [Step]26100 [Batch]130500 [Speed]83.39ms/step [Loss]8.9514 [Metrics]{'train_loss:8.951417 train_acc:0.073306 lr:0.000547 grad_accum:5.000000 total_samples:4176224.000000'}\n",
      "[Train Epoch]1/2 [Time]10921.17 [Step]26200 [Batch]131000 [Speed]83.37ms/step [Loss]8.9486 [Metrics]{'train_loss:8.948609 train_acc:0.073323 lr:0.000546 grad_accum:5.000000 total_samples:4192224.000000'}\n",
      "[Train Epoch]1/2 [Time]10959.43 [Step]26300 [Batch]131500 [Speed]83.34ms/step [Loss]8.9455 [Metrics]{'train_loss:8.945510 train_acc:0.073355 lr:0.000545 grad_accum:5.000000 total_samples:4208224.000000'}\n",
      "[Train Epoch]1/2 [Time]10997.73 [Step]26400 [Batch]132000 [Speed]83.32ms/step [Loss]8.9422 [Metrics]{'train_loss:8.942230 train_acc:0.073387 lr:0.000544 grad_accum:5.000000 total_samples:4224224.000000'}\n",
      "[Train Epoch]1/2 [Time]11035.96 [Step]26500 [Batch]132500 [Speed]83.29ms/step [Loss]8.9393 [Metrics]{'train_loss:8.939260 train_acc:0.073412 lr:0.000543 grad_accum:5.000000 total_samples:4240224.000000'}\n",
      "[Train Epoch]1/2 [Time]11074.26 [Step]26600 [Batch]133000 [Speed]83.27ms/step [Loss]8.9363 [Metrics]{'train_loss:8.936259 train_acc:0.073444 lr:0.000542 grad_accum:5.000000 total_samples:4256224.000000'}\n",
      "[Train Epoch]1/2 [Time]11112.49 [Step]26700 [Batch]133500 [Speed]83.24ms/step [Loss]8.9332 [Metrics]{'train_loss:8.933208 train_acc:0.073477 lr:0.000541 grad_accum:5.000000 total_samples:4272224.000000'}\n",
      "[Train Epoch]1/2 [Time]11150.73 [Step]26800 [Batch]134000 [Speed]83.21ms/step [Loss]8.9301 [Metrics]{'train_loss:8.930064 train_acc:0.073511 lr:0.000540 grad_accum:5.000000 total_samples:4288224.000000'}\n",
      "[Train Epoch]1/2 [Time]11189.01 [Step]26900 [Batch]134500 [Speed]83.19ms/step [Loss]8.9271 [Metrics]{'train_loss:8.927058 train_acc:0.073538 lr:0.000539 grad_accum:5.000000 total_samples:4304224.000000'}\n",
      "[Train Epoch]1/2 [Time]11227.35 [Step]27000 [Batch]135000 [Speed]83.17ms/step [Loss]8.9243 [Metrics]{'train_loss:8.924338 train_acc:0.073564 lr:0.000538 grad_accum:5.000000 total_samples:4320224.000000'}\n",
      "[Train Epoch]1/2 [Time]11265.56 [Step]27100 [Batch]135500 [Speed]83.14ms/step [Loss]8.9217 [Metrics]{'train_loss:8.921696 train_acc:0.073589 lr:0.000537 grad_accum:5.000000 total_samples:4336224.000000'}\n",
      "[Train Epoch]1/2 [Time]11303.92 [Step]27200 [Batch]136000 [Speed]83.12ms/step [Loss]8.9188 [Metrics]{'train_loss:8.918769 train_acc:0.073623 lr:0.000536 grad_accum:5.000000 total_samples:4352224.000000'}\n",
      "[Train Epoch]1/2 [Time]11342.20 [Step]27300 [Batch]136500 [Speed]83.09ms/step [Loss]8.9157 [Metrics]{'train_loss:8.915699 train_acc:0.073644 lr:0.000535 grad_accum:5.000000 total_samples:4368224.000000'}\n",
      "[Train Epoch]1/2 [Time]11380.49 [Step]27400 [Batch]137000 [Speed]83.07ms/step [Loss]8.9127 [Metrics]{'train_loss:8.912706 train_acc:0.073669 lr:0.000534 grad_accum:5.000000 total_samples:4384224.000000'}\n",
      "[Train Epoch]1/2 [Time]11418.76 [Step]27500 [Batch]137500 [Speed]83.05ms/step [Loss]8.9096 [Metrics]{'train_loss:8.909622 train_acc:0.073697 lr:0.000533 grad_accum:5.000000 total_samples:4400224.000000'}\n",
      "[Train Epoch]1/2 [Time]11457.09 [Step]27600 [Batch]138000 [Speed]83.02ms/step [Loss]8.9067 [Metrics]{'train_loss:8.906656 train_acc:0.073722 lr:0.000532 grad_accum:5.000000 total_samples:4416224.000000'}\n",
      "[Train Epoch]1/2 [Time]11495.31 [Step]27700 [Batch]138500 [Speed]83.00ms/step [Loss]8.9038 [Metrics]{'train_loss:8.903848 train_acc:0.073750 lr:0.000531 grad_accum:5.000000 total_samples:4432224.000000'}\n",
      "[Train Epoch]1/2 [Time]11533.58 [Step]27800 [Batch]139000 [Speed]82.98ms/step [Loss]8.9012 [Metrics]{'train_loss:8.901249 train_acc:0.073778 lr:0.000530 grad_accum:5.000000 total_samples:4448224.000000'}\n",
      "[Train Epoch]1/2 [Time]11571.87 [Step]27900 [Batch]139500 [Speed]82.95ms/step [Loss]8.8986 [Metrics]{'train_loss:8.898628 train_acc:0.073795 lr:0.000529 grad_accum:5.000000 total_samples:4464224.000000'}\n",
      "[Train Epoch]1/2 [Time]11610.16 [Step]28000 [Batch]140000 [Speed]82.93ms/step [Loss]8.8956 [Metrics]{'train_loss:8.895587 train_acc:0.073827 lr:0.000528 grad_accum:5.000000 total_samples:4480224.000000'}\n",
      "[Train Epoch]1/2 [Time]11648.44 [Step]28100 [Batch]140500 [Speed]82.91ms/step [Loss]8.8930 [Metrics]{'train_loss:8.892988 train_acc:0.073844 lr:0.000527 grad_accum:5.000000 total_samples:4496224.000000'}\n",
      "[Train Epoch]1/2 [Time]11686.66 [Step]28200 [Batch]141000 [Speed]82.88ms/step [Loss]8.8903 [Metrics]{'train_loss:8.890261 train_acc:0.073859 lr:0.000526 grad_accum:5.000000 total_samples:4512224.000000'}\n",
      "[Train Epoch]1/2 [Time]11724.96 [Step]28300 [Batch]141500 [Speed]82.86ms/step [Loss]8.8873 [Metrics]{'train_loss:8.887321 train_acc:0.073895 lr:0.000525 grad_accum:5.000000 total_samples:4528224.000000'}\n",
      "[Train Epoch]1/2 [Time]11763.23 [Step]28400 [Batch]142000 [Speed]82.84ms/step [Loss]8.8844 [Metrics]{'train_loss:8.884443 train_acc:0.073918 lr:0.000524 grad_accum:5.000000 total_samples:4544224.000000'}\n",
      "[Train Epoch]1/2 [Time]11801.50 [Step]28500 [Batch]142500 [Speed]82.82ms/step [Loss]8.8818 [Metrics]{'train_loss:8.881790 train_acc:0.073936 lr:0.000524 grad_accum:5.000000 total_samples:4560224.000000'}\n",
      "[Train Epoch]1/2 [Time]11839.78 [Step]28600 [Batch]143000 [Speed]82.80ms/step [Loss]8.8793 [Metrics]{'train_loss:8.879333 train_acc:0.073959 lr:0.000523 grad_accum:5.000000 total_samples:4576224.000000'}\n",
      "[Train Epoch]1/2 [Time]11877.97 [Step]28700 [Batch]143500 [Speed]82.77ms/step [Loss]8.8766 [Metrics]{'train_loss:8.876649 train_acc:0.073987 lr:0.000522 grad_accum:5.000000 total_samples:4592224.000000'}\n",
      "[Train Epoch]1/2 [Time]11916.18 [Step]28800 [Batch]144000 [Speed]82.75ms/step [Loss]8.8738 [Metrics]{'train_loss:8.873832 train_acc:0.074017 lr:0.000521 grad_accum:5.000000 total_samples:4608224.000000'}\n",
      "[Train Epoch]1/2 [Time]11954.40 [Step]28900 [Batch]144500 [Speed]82.73ms/step [Loss]8.8714 [Metrics]{'train_loss:8.871399 train_acc:0.074036 lr:0.000520 grad_accum:5.000000 total_samples:4624224.000000'}\n",
      "[Train Epoch]1/2 [Time]11992.67 [Step]29000 [Batch]145000 [Speed]82.71ms/step [Loss]8.8687 [Metrics]{'train_loss:8.868680 train_acc:0.074070 lr:0.000519 grad_accum:5.000000 total_samples:4640224.000000'}\n",
      "[Train Epoch]1/2 [Time]12030.90 [Step]29100 [Batch]145500 [Speed]82.69ms/step [Loss]8.8659 [Metrics]{'train_loss:8.865883 train_acc:0.074100 lr:0.000518 grad_accum:5.000000 total_samples:4656224.000000'}\n",
      "[Train Epoch]1/2 [Time]12069.14 [Step]29200 [Batch]146000 [Speed]82.67ms/step [Loss]8.8632 [Metrics]{'train_loss:8.863239 train_acc:0.074120 lr:0.000517 grad_accum:5.000000 total_samples:4672224.000000'}\n",
      "[Train Epoch]1/2 [Time]12107.33 [Step]29300 [Batch]146500 [Speed]82.64ms/step [Loss]8.8606 [Metrics]{'train_loss:8.860648 train_acc:0.074138 lr:0.000516 grad_accum:5.000000 total_samples:4688224.000000'}\n",
      "[Train Epoch]1/2 [Time]12145.60 [Step]29400 [Batch]147000 [Speed]82.62ms/step [Loss]8.8579 [Metrics]{'train_loss:8.857891 train_acc:0.074165 lr:0.000515 grad_accum:5.000000 total_samples:4704224.000000'}\n",
      "[Train Epoch]1/2 [Time]12183.87 [Step]29500 [Batch]147500 [Speed]82.60ms/step [Loss]8.8555 [Metrics]{'train_loss:8.855506 train_acc:0.074181 lr:0.000515 grad_accum:5.000000 total_samples:4720224.000000'}\n",
      "[Train Epoch]1/2 [Time]12222.15 [Step]29600 [Batch]148000 [Speed]82.58ms/step [Loss]8.8533 [Metrics]{'train_loss:8.853333 train_acc:0.074205 lr:0.000514 grad_accum:5.000000 total_samples:4736224.000000'}\n",
      "[Train Epoch]1/2 [Time]12260.39 [Step]29700 [Batch]148500 [Speed]82.56ms/step [Loss]8.8510 [Metrics]{'train_loss:8.851007 train_acc:0.074229 lr:0.000513 grad_accum:5.000000 total_samples:4752224.000000'}\n",
      "[Train Epoch]1/2 [Time]12298.61 [Step]29800 [Batch]149000 [Speed]82.54ms/step [Loss]8.8486 [Metrics]{'train_loss:8.848601 train_acc:0.074254 lr:0.000512 grad_accum:5.000000 total_samples:4768224.000000'}\n",
      "[Train Epoch]1/2 [Time]12336.83 [Step]29900 [Batch]149500 [Speed]82.52ms/step [Loss]8.8464 [Metrics]{'train_loss:8.846372 train_acc:0.074283 lr:0.000511 grad_accum:5.000000 total_samples:4784224.000000'}\n",
      "Saving checkpoint for epoch 1 at step 150000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]12383.24 [Step]30000 [Batch]150000 [Speed]82.55ms/step [Loss]8.8441 [Metrics]{'train_loss:8.844108 train_acc:0.074300 lr:0.000510 grad_accum:5.000000 total_samples:4800224.000000'}\n",
      "[Train Epoch]1/2 [Time]12426.10 [Step]30100 [Batch]150500 [Speed]82.57ms/step [Loss]8.8417 [Metrics]{'train_loss:8.841687 train_acc:0.074331 lr:0.000509 grad_accum:5.000000 total_samples:4816224.000000'}\n",
      "[Train Epoch]1/2 [Time]12464.34 [Step]30200 [Batch]151000 [Speed]82.55ms/step [Loss]8.8393 [Metrics]{'train_loss:8.839293 train_acc:0.074355 lr:0.000509 grad_accum:5.000000 total_samples:4832224.000000'}\n",
      "[Train Epoch]1/2 [Time]12502.59 [Step]30300 [Batch]151500 [Speed]82.53ms/step [Loss]8.8368 [Metrics]{'train_loss:8.836798 train_acc:0.074378 lr:0.000508 grad_accum:5.000000 total_samples:4848224.000000'}\n",
      "[Train Epoch]1/2 [Time]12540.85 [Step]30400 [Batch]152000 [Speed]82.51ms/step [Loss]8.8344 [Metrics]{'train_loss:8.834409 train_acc:0.074390 lr:0.000507 grad_accum:5.000000 total_samples:4864224.000000'}\n",
      "[Train Epoch]1/2 [Time]12579.11 [Step]30500 [Batch]152500 [Speed]82.49ms/step [Loss]8.8319 [Metrics]{'train_loss:8.831861 train_acc:0.074414 lr:0.000506 grad_accum:5.000000 total_samples:4880224.000000'}\n",
      "[Train Epoch]1/2 [Time]12617.40 [Step]30600 [Batch]153000 [Speed]82.47ms/step [Loss]8.8297 [Metrics]{'train_loss:8.829675 train_acc:0.074437 lr:0.000505 grad_accum:5.000000 total_samples:4896224.000000'}\n",
      "[Train Epoch]1/2 [Time]12655.68 [Step]30700 [Batch]153500 [Speed]82.45ms/step [Loss]8.8271 [Metrics]{'train_loss:8.827135 train_acc:0.074458 lr:0.000504 grad_accum:5.000000 total_samples:4912224.000000'}\n",
      "[Train Epoch]1/2 [Time]12693.97 [Step]30800 [Batch]154000 [Speed]82.43ms/step [Loss]8.8246 [Metrics]{'train_loss:8.824594 train_acc:0.074471 lr:0.000504 grad_accum:5.000000 total_samples:4928224.000000'}\n",
      "[Train Epoch]1/2 [Time]12732.19 [Step]30900 [Batch]154500 [Speed]82.41ms/step [Loss]8.8221 [Metrics]{'train_loss:8.822093 train_acc:0.074485 lr:0.000503 grad_accum:5.000000 total_samples:4944224.000000'}\n",
      "[Train Epoch]1/2 [Time]12770.42 [Step]31000 [Batch]155000 [Speed]82.39ms/step [Loss]8.8197 [Metrics]{'train_loss:8.819717 train_acc:0.074508 lr:0.000502 grad_accum:5.000000 total_samples:4960224.000000'}\n",
      "[Train Epoch]1/2 [Time]12808.68 [Step]31100 [Batch]155500 [Speed]82.37ms/step [Loss]8.8173 [Metrics]{'train_loss:8.817270 train_acc:0.074533 lr:0.000501 grad_accum:5.000000 total_samples:4976224.000000'}\n",
      "[Train Epoch]1/2 [Time]12846.93 [Step]31200 [Batch]156000 [Speed]82.35ms/step [Loss]8.8150 [Metrics]{'train_loss:8.815015 train_acc:0.074543 lr:0.000500 grad_accum:5.000000 total_samples:4992224.000000'}\n",
      "[Train Epoch]1/2 [Time]12885.21 [Step]31300 [Batch]156500 [Speed]82.33ms/step [Loss]8.8128 [Metrics]{'train_loss:8.812828 train_acc:0.074559 lr:0.000500 grad_accum:5.000000 total_samples:5008224.000000'}\n",
      "[Train Epoch]1/2 [Time]12923.48 [Step]31400 [Batch]157000 [Speed]82.32ms/step [Loss]8.8103 [Metrics]{'train_loss:8.810306 train_acc:0.074589 lr:0.000499 grad_accum:5.000000 total_samples:5024224.000000'}\n",
      "[Train Epoch]1/2 [Time]12961.72 [Step]31500 [Batch]157500 [Speed]82.30ms/step [Loss]8.8081 [Metrics]{'train_loss:8.808134 train_acc:0.074616 lr:0.000498 grad_accum:5.000000 total_samples:5040224.000000'}\n",
      "[Train Epoch]1/2 [Time]12999.99 [Step]31600 [Batch]158000 [Speed]82.28ms/step [Loss]8.8059 [Metrics]{'train_loss:8.805913 train_acc:0.074644 lr:0.000497 grad_accum:5.000000 total_samples:5056224.000000'}\n",
      "[Train Epoch]1/2 [Time]13038.25 [Step]31700 [Batch]158500 [Speed]82.26ms/step [Loss]8.8039 [Metrics]{'train_loss:8.803935 train_acc:0.074668 lr:0.000496 grad_accum:5.000000 total_samples:5072224.000000'}\n",
      "[Train Epoch]1/2 [Time]13076.50 [Step]31800 [Batch]159000 [Speed]82.24ms/step [Loss]8.8020 [Metrics]{'train_loss:8.802022 train_acc:0.074696 lr:0.000496 grad_accum:5.000000 total_samples:5088224.000000'}\n",
      "[Train Epoch]1/2 [Time]13114.73 [Step]31900 [Batch]159500 [Speed]82.22ms/step [Loss]8.8001 [Metrics]{'train_loss:8.800060 train_acc:0.074720 lr:0.000495 grad_accum:5.000000 total_samples:5104224.000000'}\n",
      "[Train Epoch]1/2 [Time]13152.97 [Step]32000 [Batch]160000 [Speed]82.21ms/step [Loss]8.7979 [Metrics]{'train_loss:8.797853 train_acc:0.074743 lr:0.000494 grad_accum:5.000000 total_samples:5120224.000000'}\n",
      "[Train Epoch]1/2 [Time]13191.20 [Step]32100 [Batch]160500 [Speed]82.19ms/step [Loss]8.7955 [Metrics]{'train_loss:8.795522 train_acc:0.074760 lr:0.000493 grad_accum:5.000000 total_samples:5136224.000000'}\n",
      "[Train Epoch]1/2 [Time]13229.46 [Step]32200 [Batch]161000 [Speed]82.17ms/step [Loss]8.7931 [Metrics]{'train_loss:8.793134 train_acc:0.074775 lr:0.000493 grad_accum:5.000000 total_samples:5152224.000000'}\n",
      "[Train Epoch]1/2 [Time]13267.73 [Step]32300 [Batch]161500 [Speed]82.15ms/step [Loss]8.7908 [Metrics]{'train_loss:8.790762 train_acc:0.074791 lr:0.000492 grad_accum:5.000000 total_samples:5168224.000000'}\n",
      "[Train Epoch]1/2 [Time]13305.99 [Step]32400 [Batch]162000 [Speed]82.14ms/step [Loss]8.7884 [Metrics]{'train_loss:8.788417 train_acc:0.074811 lr:0.000491 grad_accum:5.000000 total_samples:5184224.000000'}\n",
      "[Train Epoch]1/2 [Time]13344.22 [Step]32500 [Batch]162500 [Speed]82.12ms/step [Loss]8.7862 [Metrics]{'train_loss:8.786191 train_acc:0.074831 lr:0.000490 grad_accum:5.000000 total_samples:5200224.000000'}\n",
      "[Train Epoch]1/2 [Time]13382.47 [Step]32600 [Batch]163000 [Speed]82.10ms/step [Loss]8.7839 [Metrics]{'train_loss:8.783858 train_acc:0.074848 lr:0.000490 grad_accum:5.000000 total_samples:5216224.000000'}\n",
      "[Train Epoch]1/2 [Time]13420.74 [Step]32700 [Batch]163500 [Speed]82.08ms/step [Loss]8.7815 [Metrics]{'train_loss:8.781505 train_acc:0.074870 lr:0.000489 grad_accum:5.000000 total_samples:5232224.000000'}\n",
      "[Train Epoch]1/2 [Time]13458.98 [Step]32800 [Batch]164000 [Speed]82.07ms/step [Loss]8.7795 [Metrics]{'train_loss:8.779490 train_acc:0.074882 lr:0.000488 grad_accum:5.000000 total_samples:5248224.000000'}\n",
      "[Train Epoch]1/2 [Time]13497.25 [Step]32900 [Batch]164500 [Speed]82.05ms/step [Loss]8.7771 [Metrics]{'train_loss:8.777061 train_acc:0.074907 lr:0.000487 grad_accum:5.000000 total_samples:5264224.000000'}\n",
      "[Train Epoch]1/2 [Time]13535.50 [Step]33000 [Batch]165000 [Speed]82.03ms/step [Loss]8.7747 [Metrics]{'train_loss:8.774668 train_acc:0.074928 lr:0.000487 grad_accum:5.000000 total_samples:5280224.000000'}\n",
      "[Train Epoch]1/2 [Time]13573.69 [Step]33100 [Batch]165500 [Speed]82.02ms/step [Loss]8.7722 [Metrics]{'train_loss:8.772182 train_acc:0.074950 lr:0.000486 grad_accum:5.000000 total_samples:5296224.000000'}\n",
      "[Train Epoch]1/2 [Time]13611.91 [Step]33200 [Batch]166000 [Speed]82.00ms/step [Loss]8.7701 [Metrics]{'train_loss:8.770078 train_acc:0.074965 lr:0.000485 grad_accum:5.000000 total_samples:5312224.000000'}\n",
      "[Train Epoch]1/2 [Time]13650.16 [Step]33300 [Batch]166500 [Speed]81.98ms/step [Loss]8.7678 [Metrics]{'train_loss:8.767842 train_acc:0.074984 lr:0.000484 grad_accum:5.000000 total_samples:5328224.000000'}\n",
      "[Train Epoch]1/2 [Time]13688.43 [Step]33400 [Batch]167000 [Speed]81.97ms/step [Loss]8.7656 [Metrics]{'train_loss:8.765602 train_acc:0.075004 lr:0.000484 grad_accum:5.000000 total_samples:5344224.000000'}\n",
      "[Train Epoch]1/2 [Time]13726.70 [Step]33500 [Batch]167500 [Speed]81.95ms/step [Loss]8.7632 [Metrics]{'train_loss:8.763199 train_acc:0.075024 lr:0.000483 grad_accum:5.000000 total_samples:5360224.000000'}\n",
      "[Train Epoch]1/2 [Time]13764.94 [Step]33600 [Batch]168000 [Speed]81.93ms/step [Loss]8.7612 [Metrics]{'train_loss:8.761151 train_acc:0.075039 lr:0.000482 grad_accum:5.000000 total_samples:5376224.000000'}\n",
      "[Train Epoch]1/2 [Time]13803.21 [Step]33700 [Batch]168500 [Speed]81.92ms/step [Loss]8.7589 [Metrics]{'train_loss:8.758880 train_acc:0.075064 lr:0.000481 grad_accum:5.000000 total_samples:5392224.000000'}\n",
      "[Train Epoch]1/2 [Time]13841.46 [Step]33800 [Batch]169000 [Speed]81.90ms/step [Loss]8.7567 [Metrics]{'train_loss:8.756652 train_acc:0.075084 lr:0.000481 grad_accum:5.000000 total_samples:5408224.000000'}\n",
      "[Train Epoch]1/2 [Time]13879.71 [Step]33900 [Batch]169500 [Speed]81.89ms/step [Loss]8.7546 [Metrics]{'train_loss:8.754568 train_acc:0.075104 lr:0.000480 grad_accum:5.000000 total_samples:5424224.000000'}\n",
      "[Train Epoch]1/2 [Time]13917.97 [Step]34000 [Batch]170000 [Speed]81.87ms/step [Loss]8.7524 [Metrics]{'train_loss:8.752428 train_acc:0.075122 lr:0.000479 grad_accum:5.000000 total_samples:5440224.000000'}\n",
      "[Train Epoch]1/2 [Time]13956.23 [Step]34100 [Batch]170500 [Speed]81.85ms/step [Loss]8.7503 [Metrics]{'train_loss:8.750273 train_acc:0.075141 lr:0.000479 grad_accum:5.000000 total_samples:5456224.000000'}\n",
      "[Train Epoch]1/2 [Time]13994.40 [Step]34200 [Batch]171000 [Speed]81.84ms/step [Loss]8.7483 [Metrics]{'train_loss:8.748279 train_acc:0.075159 lr:0.000478 grad_accum:5.000000 total_samples:5472224.000000'}\n",
      "[Train Epoch]1/2 [Time]14032.64 [Step]34300 [Batch]171500 [Speed]81.82ms/step [Loss]8.7462 [Metrics]{'train_loss:8.746222 train_acc:0.075175 lr:0.000477 grad_accum:5.000000 total_samples:5488224.000000'}\n",
      "[Train Epoch]1/2 [Time]14070.92 [Step]34400 [Batch]172000 [Speed]81.81ms/step [Loss]8.7442 [Metrics]{'train_loss:8.744242 train_acc:0.075198 lr:0.000477 grad_accum:5.000000 total_samples:5504224.000000'}\n",
      "[Train Epoch]1/2 [Time]14109.16 [Step]34500 [Batch]172500 [Speed]81.79ms/step [Loss]8.7422 [Metrics]{'train_loss:8.742202 train_acc:0.075214 lr:0.000476 grad_accum:5.000000 total_samples:5520224.000000'}\n",
      "[Train Epoch]1/2 [Time]14147.42 [Step]34600 [Batch]173000 [Speed]81.78ms/step [Loss]8.7402 [Metrics]{'train_loss:8.740212 train_acc:0.075233 lr:0.000475 grad_accum:5.000000 total_samples:5536224.000000'}\n",
      "[Train Epoch]1/2 [Time]14185.64 [Step]34700 [Batch]173500 [Speed]81.76ms/step [Loss]8.7382 [Metrics]{'train_loss:8.738233 train_acc:0.075264 lr:0.000474 grad_accum:5.000000 total_samples:5552224.000000'}\n",
      "[Train Epoch]1/2 [Time]14223.92 [Step]34800 [Batch]174000 [Speed]81.75ms/step [Loss]8.7365 [Metrics]{'train_loss:8.736495 train_acc:0.075288 lr:0.000474 grad_accum:5.000000 total_samples:5568224.000000'}\n",
      "[Train Epoch]1/2 [Time]14262.17 [Step]34900 [Batch]174500 [Speed]81.73ms/step [Loss]8.7343 [Metrics]{'train_loss:8.734306 train_acc:0.075318 lr:0.000473 grad_accum:5.000000 total_samples:5584224.000000'}\n",
      "Saving checkpoint for epoch 1 at step 175000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]14308.61 [Step]35000 [Batch]175000 [Speed]81.76ms/step [Loss]8.7323 [Metrics]{'train_loss:8.732297 train_acc:0.075339 lr:0.000472 grad_accum:5.000000 total_samples:5600224.000000'}\n",
      "[Train Epoch]1/2 [Time]14346.82 [Step]35100 [Batch]175500 [Speed]81.75ms/step [Loss]8.7305 [Metrics]{'train_loss:8.730474 train_acc:0.075355 lr:0.000472 grad_accum:5.000000 total_samples:5616224.000000'}\n",
      "[Train Epoch]1/2 [Time]14385.04 [Step]35200 [Batch]176000 [Speed]81.73ms/step [Loss]8.7286 [Metrics]{'train_loss:8.728640 train_acc:0.075370 lr:0.000471 grad_accum:5.000000 total_samples:5632224.000000'}\n",
      "[Train Epoch]1/2 [Time]14423.32 [Step]35300 [Batch]176500 [Speed]81.72ms/step [Loss]8.7266 [Metrics]{'train_loss:8.726626 train_acc:0.075389 lr:0.000470 grad_accum:5.000000 total_samples:5648224.000000'}\n",
      "[Train Epoch]1/2 [Time]14461.55 [Step]35400 [Batch]177000 [Speed]81.70ms/step [Loss]8.7246 [Metrics]{'train_loss:8.724577 train_acc:0.075413 lr:0.000470 grad_accum:5.000000 total_samples:5664224.000000'}\n",
      "[Train Epoch]1/2 [Time]14499.85 [Step]35500 [Batch]177500 [Speed]81.69ms/step [Loss]8.7227 [Metrics]{'train_loss:8.722723 train_acc:0.075429 lr:0.000469 grad_accum:5.000000 total_samples:5680224.000000'}\n",
      "[Train Epoch]1/2 [Time]14538.13 [Step]35600 [Batch]178000 [Speed]81.67ms/step [Loss]8.7210 [Metrics]{'train_loss:8.720984 train_acc:0.075443 lr:0.000468 grad_accum:5.000000 total_samples:5696224.000000'}\n",
      "[Train Epoch]1/2 [Time]14576.39 [Step]35700 [Batch]178500 [Speed]81.66ms/step [Loss]8.7192 [Metrics]{'train_loss:8.719158 train_acc:0.075460 lr:0.000468 grad_accum:5.000000 total_samples:5712224.000000'}\n",
      "[Train Epoch]1/2 [Time]14614.62 [Step]35800 [Batch]179000 [Speed]81.65ms/step [Loss]8.7173 [Metrics]{'train_loss:8.717349 train_acc:0.075484 lr:0.000467 grad_accum:5.000000 total_samples:5728224.000000'}\n",
      "[Train Epoch]1/2 [Time]14652.91 [Step]35900 [Batch]179500 [Speed]81.63ms/step [Loss]8.7155 [Metrics]{'train_loss:8.715487 train_acc:0.075508 lr:0.000466 grad_accum:5.000000 total_samples:5744224.000000'}\n",
      "[Train Epoch]1/2 [Time]14691.17 [Step]36000 [Batch]180000 [Speed]81.62ms/step [Loss]8.7137 [Metrics]{'train_loss:8.713745 train_acc:0.075531 lr:0.000466 grad_accum:5.000000 total_samples:5760224.000000'}\n",
      "[Train Epoch]1/2 [Time]14729.40 [Step]36100 [Batch]180500 [Speed]81.60ms/step [Loss]8.7120 [Metrics]{'train_loss:8.711981 train_acc:0.075553 lr:0.000465 grad_accum:5.000000 total_samples:5776224.000000'}\n",
      "[Train Epoch]1/2 [Time]14767.68 [Step]36200 [Batch]181000 [Speed]81.59ms/step [Loss]8.7103 [Metrics]{'train_loss:8.710327 train_acc:0.075564 lr:0.000465 grad_accum:5.000000 total_samples:5792224.000000'}\n",
      "[Train Epoch]1/2 [Time]14805.86 [Step]36300 [Batch]181500 [Speed]81.58ms/step [Loss]8.7084 [Metrics]{'train_loss:8.708385 train_acc:0.075586 lr:0.000464 grad_accum:5.000000 total_samples:5808224.000000'}\n",
      "[Train Epoch]1/2 [Time]14844.11 [Step]36400 [Batch]182000 [Speed]81.56ms/step [Loss]8.7065 [Metrics]{'train_loss:8.706455 train_acc:0.075600 lr:0.000463 grad_accum:5.000000 total_samples:5824224.000000'}\n",
      "[Train Epoch]1/2 [Time]14882.38 [Step]36500 [Batch]182500 [Speed]81.55ms/step [Loss]8.7045 [Metrics]{'train_loss:8.704468 train_acc:0.075622 lr:0.000463 grad_accum:5.000000 total_samples:5840224.000000'}\n",
      "[Train Epoch]1/2 [Time]14920.66 [Step]36600 [Batch]183000 [Speed]81.53ms/step [Loss]8.7026 [Metrics]{'train_loss:8.702600 train_acc:0.075633 lr:0.000462 grad_accum:5.000000 total_samples:5856224.000000'}\n",
      "[Train Epoch]1/2 [Time]14958.94 [Step]36700 [Batch]183500 [Speed]81.52ms/step [Loss]8.7007 [Metrics]{'train_loss:8.700718 train_acc:0.075652 lr:0.000461 grad_accum:5.000000 total_samples:5872224.000000'}\n",
      "[Train Epoch]1/2 [Time]14997.19 [Step]36800 [Batch]184000 [Speed]81.51ms/step [Loss]8.6988 [Metrics]{'train_loss:8.698801 train_acc:0.075672 lr:0.000461 grad_accum:5.000000 total_samples:5888224.000000'}\n",
      "[Train Epoch]1/2 [Time]15035.45 [Step]36900 [Batch]184500 [Speed]81.49ms/step [Loss]8.6970 [Metrics]{'train_loss:8.696967 train_acc:0.075687 lr:0.000460 grad_accum:5.000000 total_samples:5904224.000000'}\n",
      "[Train Epoch]1/2 [Time]15073.74 [Step]37000 [Batch]185000 [Speed]81.48ms/step [Loss]8.6951 [Metrics]{'train_loss:8.695082 train_acc:0.075700 lr:0.000460 grad_accum:5.000000 total_samples:5920224.000000'}\n",
      "[Train Epoch]1/2 [Time]15112.00 [Step]37100 [Batch]185500 [Speed]81.47ms/step [Loss]8.6932 [Metrics]{'train_loss:8.693222 train_acc:0.075714 lr:0.000459 grad_accum:5.000000 total_samples:5936224.000000'}\n",
      "[Train Epoch]1/2 [Time]15150.25 [Step]37200 [Batch]186000 [Speed]81.45ms/step [Loss]8.6913 [Metrics]{'train_loss:8.691343 train_acc:0.075734 lr:0.000458 grad_accum:5.000000 total_samples:5952224.000000'}\n",
      "[Train Epoch]1/2 [Time]15188.50 [Step]37300 [Batch]186500 [Speed]81.44ms/step [Loss]8.6896 [Metrics]{'train_loss:8.689552 train_acc:0.075751 lr:0.000458 grad_accum:5.000000 total_samples:5968224.000000'}\n",
      "[Train Epoch]1/2 [Time]15226.71 [Step]37400 [Batch]187000 [Speed]81.43ms/step [Loss]8.6877 [Metrics]{'train_loss:8.687707 train_acc:0.075768 lr:0.000457 grad_accum:5.000000 total_samples:5984224.000000'}\n",
      "[Train Epoch]1/2 [Time]15264.91 [Step]37500 [Batch]187500 [Speed]81.41ms/step [Loss]8.6859 [Metrics]{'train_loss:8.685916 train_acc:0.075785 lr:0.000456 grad_accum:5.000000 total_samples:6000224.000000'}\n",
      "[Train Epoch]1/2 [Time]15303.22 [Step]37600 [Batch]188000 [Speed]81.40ms/step [Loss]8.6840 [Metrics]{'train_loss:8.684037 train_acc:0.075806 lr:0.000456 grad_accum:5.000000 total_samples:6016224.000000'}\n",
      "[Train Epoch]1/2 [Time]15341.47 [Step]37700 [Batch]188500 [Speed]81.39ms/step [Loss]8.6822 [Metrics]{'train_loss:8.682232 train_acc:0.075817 lr:0.000455 grad_accum:5.000000 total_samples:6032224.000000'}\n",
      "[Train Epoch]1/2 [Time]15379.74 [Step]37800 [Batch]189000 [Speed]81.37ms/step [Loss]8.6805 [Metrics]{'train_loss:8.680528 train_acc:0.075830 lr:0.000455 grad_accum:5.000000 total_samples:6048224.000000'}\n",
      "[Train Epoch]1/2 [Time]15418.01 [Step]37900 [Batch]189500 [Speed]81.36ms/step [Loss]8.6788 [Metrics]{'train_loss:8.678827 train_acc:0.075845 lr:0.000454 grad_accum:5.000000 total_samples:6064224.000000'}\n",
      "[Train Epoch]1/2 [Time]15456.28 [Step]38000 [Batch]190000 [Speed]81.35ms/step [Loss]8.6770 [Metrics]{'train_loss:8.676962 train_acc:0.075865 lr:0.000453 grad_accum:5.000000 total_samples:6080224.000000'}\n",
      "[Train Epoch]1/2 [Time]15494.55 [Step]38100 [Batch]190500 [Speed]81.34ms/step [Loss]8.6752 [Metrics]{'train_loss:8.675206 train_acc:0.075881 lr:0.000453 grad_accum:5.000000 total_samples:6096224.000000'}\n",
      "[Train Epoch]1/2 [Time]15532.81 [Step]38200 [Batch]191000 [Speed]81.32ms/step [Loss]8.6734 [Metrics]{'train_loss:8.673382 train_acc:0.075897 lr:0.000452 grad_accum:5.000000 total_samples:6112224.000000'}\n",
      "[Train Epoch]1/2 [Time]15571.04 [Step]38300 [Batch]191500 [Speed]81.31ms/step [Loss]8.6716 [Metrics]{'train_loss:8.671640 train_acc:0.075916 lr:0.000452 grad_accum:5.000000 total_samples:6128224.000000'}\n",
      "[Train Epoch]1/2 [Time]15609.30 [Step]38400 [Batch]192000 [Speed]81.30ms/step [Loss]8.6698 [Metrics]{'train_loss:8.669750 train_acc:0.075933 lr:0.000451 grad_accum:5.000000 total_samples:6144224.000000'}\n",
      "[Train Epoch]1/2 [Time]15647.54 [Step]38500 [Batch]192500 [Speed]81.29ms/step [Loss]8.6681 [Metrics]{'train_loss:8.668051 train_acc:0.075949 lr:0.000450 grad_accum:5.000000 total_samples:6160224.000000'}\n",
      "[Train Epoch]1/2 [Time]15685.75 [Step]38600 [Batch]193000 [Speed]81.27ms/step [Loss]8.6663 [Metrics]{'train_loss:8.666252 train_acc:0.075969 lr:0.000450 grad_accum:5.000000 total_samples:6176224.000000'}\n",
      "[Train Epoch]1/2 [Time]15724.00 [Step]38700 [Batch]193500 [Speed]81.26ms/step [Loss]8.6644 [Metrics]{'train_loss:8.664398 train_acc:0.075989 lr:0.000449 grad_accum:5.000000 total_samples:6192224.000000'}\n",
      "[Train Epoch]1/2 [Time]15762.27 [Step]38800 [Batch]194000 [Speed]81.25ms/step [Loss]8.6626 [Metrics]{'train_loss:8.662621 train_acc:0.076006 lr:0.000449 grad_accum:5.000000 total_samples:6208224.000000'}\n",
      "[Train Epoch]1/2 [Time]15800.53 [Step]38900 [Batch]194500 [Speed]81.24ms/step [Loss]8.6608 [Metrics]{'train_loss:8.660842 train_acc:0.076019 lr:0.000448 grad_accum:5.000000 total_samples:6224224.000000'}\n",
      "[Train Epoch]1/2 [Time]15838.76 [Step]39000 [Batch]195000 [Speed]81.22ms/step [Loss]8.6592 [Metrics]{'train_loss:8.659182 train_acc:0.076037 lr:0.000448 grad_accum:5.000000 total_samples:6240224.000000'}\n",
      "[Train Epoch]1/2 [Time]15877.01 [Step]39100 [Batch]195500 [Speed]81.21ms/step [Loss]8.6576 [Metrics]{'train_loss:8.657639 train_acc:0.076059 lr:0.000447 grad_accum:5.000000 total_samples:6256224.000000'}\n",
      "[Train Epoch]1/2 [Time]15915.26 [Step]39200 [Batch]196000 [Speed]81.20ms/step [Loss]8.6558 [Metrics]{'train_loss:8.655832 train_acc:0.076079 lr:0.000446 grad_accum:5.000000 total_samples:6272224.000000'}\n",
      "[Train Epoch]1/2 [Time]15953.50 [Step]39300 [Batch]196500 [Speed]81.19ms/step [Loss]8.6542 [Metrics]{'train_loss:8.654150 train_acc:0.076097 lr:0.000446 grad_accum:5.000000 total_samples:6288224.000000'}\n",
      "[Train Epoch]1/2 [Time]15991.75 [Step]39400 [Batch]197000 [Speed]81.18ms/step [Loss]8.6525 [Metrics]{'train_loss:8.652488 train_acc:0.076113 lr:0.000445 grad_accum:5.000000 total_samples:6304224.000000'}\n",
      "[Train Epoch]1/2 [Time]16030.00 [Step]39500 [Batch]197500 [Speed]81.16ms/step [Loss]8.6507 [Metrics]{'train_loss:8.650739 train_acc:0.076126 lr:0.000445 grad_accum:5.000000 total_samples:6320224.000000'}\n",
      "[Train Epoch]1/2 [Time]16068.23 [Step]39600 [Batch]198000 [Speed]81.15ms/step [Loss]8.6491 [Metrics]{'train_loss:8.649106 train_acc:0.076138 lr:0.000444 grad_accum:5.000000 total_samples:6336224.000000'}\n",
      "[Train Epoch]1/2 [Time]16106.44 [Step]39700 [Batch]198500 [Speed]81.14ms/step [Loss]8.6475 [Metrics]{'train_loss:8.647486 train_acc:0.076156 lr:0.000444 grad_accum:5.000000 total_samples:6352224.000000'}\n",
      "[Train Epoch]1/2 [Time]16144.72 [Step]39800 [Batch]199000 [Speed]81.13ms/step [Loss]8.6459 [Metrics]{'train_loss:8.645870 train_acc:0.076173 lr:0.000443 grad_accum:5.000000 total_samples:6368224.000000'}\n",
      "[Train Epoch]1/2 [Time]16182.98 [Step]39900 [Batch]199500 [Speed]81.12ms/step [Loss]8.6441 [Metrics]{'train_loss:8.644088 train_acc:0.076192 lr:0.000442 grad_accum:5.000000 total_samples:6384224.000000'}\n",
      "Saving checkpoint for epoch 1 at step 200000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]16229.52 [Step]40000 [Batch]200000 [Speed]81.15ms/step [Loss]8.6423 [Metrics]{'train_loss:8.642344 train_acc:0.076200 lr:0.000442 grad_accum:5.000000 total_samples:6400224.000000'}\n",
      "[Train Epoch]1/2 [Time]16267.65 [Step]40100 [Batch]200500 [Speed]81.14ms/step [Loss]8.6406 [Metrics]{'train_loss:8.640643 train_acc:0.076209 lr:0.000441 grad_accum:5.000000 total_samples:6416224.000000'}\n",
      "[Train Epoch]1/2 [Time]16305.89 [Step]40200 [Batch]201000 [Speed]81.12ms/step [Loss]8.6390 [Metrics]{'train_loss:8.639001 train_acc:0.076222 lr:0.000441 grad_accum:5.000000 total_samples:6432224.000000'}\n",
      "[Train Epoch]1/2 [Time]16344.15 [Step]40300 [Batch]201500 [Speed]81.11ms/step [Loss]8.6377 [Metrics]{'train_loss:8.637664 train_acc:0.076229 lr:0.000440 grad_accum:5.000000 total_samples:6448224.000000'}\n",
      "[Train Epoch]1/2 [Time]16382.40 [Step]40400 [Batch]202000 [Speed]81.10ms/step [Loss]8.6362 [Metrics]{'train_loss:8.636161 train_acc:0.076259 lr:0.000440 grad_accum:5.000000 total_samples:6464224.000000'}\n",
      "[Train Epoch]1/2 [Time]16420.66 [Step]40500 [Batch]202500 [Speed]81.09ms/step [Loss]8.6349 [Metrics]{'train_loss:8.634944 train_acc:0.076275 lr:0.000439 grad_accum:5.000000 total_samples:6480224.000000'}\n",
      "[Train Epoch]1/2 [Time]16458.91 [Step]40600 [Batch]203000 [Speed]81.08ms/step [Loss]8.6334 [Metrics]{'train_loss:8.633418 train_acc:0.076285 lr:0.000439 grad_accum:5.000000 total_samples:6496224.000000'}\n",
      "[Train Epoch]1/2 [Time]16497.11 [Step]40700 [Batch]203500 [Speed]81.07ms/step [Loss]8.6318 [Metrics]{'train_loss:8.631835 train_acc:0.076300 lr:0.000438 grad_accum:5.000000 total_samples:6512224.000000'}\n",
      "[Train Epoch]1/2 [Time]16535.35 [Step]40800 [Batch]204000 [Speed]81.06ms/step [Loss]8.6302 [Metrics]{'train_loss:8.630205 train_acc:0.076313 lr:0.000438 grad_accum:5.000000 total_samples:6528224.000000'}\n",
      "[Train Epoch]1/2 [Time]16573.61 [Step]40900 [Batch]204500 [Speed]81.04ms/step [Loss]8.6287 [Metrics]{'train_loss:8.628695 train_acc:0.076324 lr:0.000437 grad_accum:5.000000 total_samples:6544224.000000'}\n",
      "[Train Epoch]1/2 [Time]16611.90 [Step]41000 [Batch]205000 [Speed]81.03ms/step [Loss]8.6270 [Metrics]{'train_loss:8.627016 train_acc:0.076339 lr:0.000437 grad_accum:5.000000 total_samples:6560224.000000'}\n",
      "[Train Epoch]1/2 [Time]16650.17 [Step]41100 [Batch]205500 [Speed]81.02ms/step [Loss]8.6256 [Metrics]{'train_loss:8.625637 train_acc:0.076348 lr:0.000436 grad_accum:5.000000 total_samples:6576224.000000'}\n",
      "[Train Epoch]1/2 [Time]16688.40 [Step]41200 [Batch]206000 [Speed]81.01ms/step [Loss]8.6240 [Metrics]{'train_loss:8.623956 train_acc:0.076365 lr:0.000435 grad_accum:5.000000 total_samples:6592224.000000'}\n",
      "[Train Epoch]1/2 [Time]16726.63 [Step]41300 [Batch]206500 [Speed]81.00ms/step [Loss]8.6223 [Metrics]{'train_loss:8.622320 train_acc:0.076378 lr:0.000435 grad_accum:5.000000 total_samples:6608224.000000'}\n",
      "[Train Epoch]1/2 [Time]16764.88 [Step]41400 [Batch]207000 [Speed]80.99ms/step [Loss]8.6206 [Metrics]{'train_loss:8.620608 train_acc:0.076396 lr:0.000434 grad_accum:5.000000 total_samples:6624224.000000'}\n",
      "[Train Epoch]1/2 [Time]16803.13 [Step]41500 [Batch]207500 [Speed]80.98ms/step [Loss]8.6190 [Metrics]{'train_loss:8.619005 train_acc:0.076409 lr:0.000434 grad_accum:5.000000 total_samples:6640224.000000'}\n",
      "[Train Epoch]1/2 [Time]16841.38 [Step]41600 [Batch]208000 [Speed]80.97ms/step [Loss]8.6174 [Metrics]{'train_loss:8.617419 train_acc:0.076421 lr:0.000433 grad_accum:5.000000 total_samples:6656224.000000'}\n",
      "[Train Epoch]1/2 [Time]16879.61 [Step]41700 [Batch]208500 [Speed]80.96ms/step [Loss]8.6158 [Metrics]{'train_loss:8.615837 train_acc:0.076432 lr:0.000433 grad_accum:5.000000 total_samples:6672224.000000'}\n",
      "[Train Epoch]1/2 [Time]16917.83 [Step]41800 [Batch]209000 [Speed]80.95ms/step [Loss]8.6144 [Metrics]{'train_loss:8.614380 train_acc:0.076445 lr:0.000432 grad_accum:5.000000 total_samples:6688224.000000'}\n",
      "[Train Epoch]1/2 [Time]16956.08 [Step]41900 [Batch]209500 [Speed]80.94ms/step [Loss]8.6128 [Metrics]{'train_loss:8.612792 train_acc:0.076457 lr:0.000432 grad_accum:5.000000 total_samples:6704224.000000'}\n",
      "[Train Epoch]1/2 [Time]16994.33 [Step]42000 [Batch]210000 [Speed]80.93ms/step [Loss]8.6111 [Metrics]{'train_loss:8.611099 train_acc:0.076473 lr:0.000431 grad_accum:5.000000 total_samples:6720224.000000'}\n",
      "[Train Epoch]1/2 [Time]17032.57 [Step]42100 [Batch]210500 [Speed]80.91ms/step [Loss]8.6094 [Metrics]{'train_loss:8.609436 train_acc:0.076479 lr:0.000431 grad_accum:5.000000 total_samples:6736224.000000'}\n",
      "[Train Epoch]1/2 [Time]17070.81 [Step]42200 [Batch]211000 [Speed]80.90ms/step [Loss]8.6080 [Metrics]{'train_loss:8.607950 train_acc:0.076490 lr:0.000430 grad_accum:5.000000 total_samples:6752224.000000'}\n",
      "[Train Epoch]1/2 [Time]17109.07 [Step]42300 [Batch]211500 [Speed]80.89ms/step [Loss]8.6063 [Metrics]{'train_loss:8.606254 train_acc:0.076505 lr:0.000430 grad_accum:5.000000 total_samples:6768224.000000'}\n",
      "[Train Epoch]1/2 [Time]17147.36 [Step]42400 [Batch]212000 [Speed]80.88ms/step [Loss]8.6045 [Metrics]{'train_loss:8.604513 train_acc:0.076527 lr:0.000429 grad_accum:5.000000 total_samples:6784224.000000'}\n",
      "[Train Epoch]1/2 [Time]17185.63 [Step]42500 [Batch]212500 [Speed]80.87ms/step [Loss]8.6029 [Metrics]{'train_loss:8.602921 train_acc:0.076540 lr:0.000429 grad_accum:5.000000 total_samples:6800224.000000'}\n",
      "[Train Epoch]1/2 [Time]17223.88 [Step]42600 [Batch]213000 [Speed]80.86ms/step [Loss]8.6014 [Metrics]{'train_loss:8.601359 train_acc:0.076552 lr:0.000428 grad_accum:5.000000 total_samples:6816224.000000'}\n",
      "[Train Epoch]1/2 [Time]17262.14 [Step]42700 [Batch]213500 [Speed]80.85ms/step [Loss]8.6000 [Metrics]{'train_loss:8.599956 train_acc:0.076563 lr:0.000428 grad_accum:5.000000 total_samples:6832224.000000'}\n",
      "[Train Epoch]1/2 [Time]17300.33 [Step]42800 [Batch]214000 [Speed]80.84ms/step [Loss]8.5984 [Metrics]{'train_loss:8.598420 train_acc:0.076574 lr:0.000427 grad_accum:5.000000 total_samples:6848224.000000'}\n",
      "[Train Epoch]1/2 [Time]17338.54 [Step]42900 [Batch]214500 [Speed]80.83ms/step [Loss]8.5969 [Metrics]{'train_loss:8.596869 train_acc:0.076591 lr:0.000427 grad_accum:5.000000 total_samples:6864224.000000'}\n",
      "[Train Epoch]1/2 [Time]17376.79 [Step]43000 [Batch]215000 [Speed]80.82ms/step [Loss]8.5953 [Metrics]{'train_loss:8.595349 train_acc:0.076604 lr:0.000426 grad_accum:5.000000 total_samples:6880224.000000'}\n",
      "[Train Epoch]1/2 [Time]17415.07 [Step]43100 [Batch]215500 [Speed]80.81ms/step [Loss]8.5941 [Metrics]{'train_loss:8.594061 train_acc:0.076623 lr:0.000426 grad_accum:5.000000 total_samples:6896224.000000'}\n",
      "[Train Epoch]1/2 [Time]17453.30 [Step]43200 [Batch]216000 [Speed]80.80ms/step [Loss]8.5927 [Metrics]{'train_loss:8.592700 train_acc:0.076641 lr:0.000425 grad_accum:5.000000 total_samples:6912224.000000'}\n",
      "[Train Epoch]1/2 [Time]17491.55 [Step]43300 [Batch]216500 [Speed]80.79ms/step [Loss]8.5914 [Metrics]{'train_loss:8.591367 train_acc:0.076661 lr:0.000425 grad_accum:5.000000 total_samples:6928224.000000'}\n",
      "[Train Epoch]1/2 [Time]17529.78 [Step]43400 [Batch]217000 [Speed]80.78ms/step [Loss]8.5900 [Metrics]{'train_loss:8.590023 train_acc:0.076673 lr:0.000424 grad_accum:5.000000 total_samples:6944224.000000'}\n",
      "[Train Epoch]1/2 [Time]17568.03 [Step]43500 [Batch]217500 [Speed]80.77ms/step [Loss]8.5887 [Metrics]{'train_loss:8.588657 train_acc:0.076693 lr:0.000424 grad_accum:5.000000 total_samples:6960224.000000'}\n",
      "[Train Epoch]1/2 [Time]17606.27 [Step]43600 [Batch]218000 [Speed]80.76ms/step [Loss]8.5874 [Metrics]{'train_loss:8.587412 train_acc:0.076706 lr:0.000423 grad_accum:5.000000 total_samples:6976224.000000'}\n",
      "[Train Epoch]1/2 [Time]17644.54 [Step]43700 [Batch]218500 [Speed]80.75ms/step [Loss]8.5861 [Metrics]{'train_loss:8.586072 train_acc:0.076716 lr:0.000423 grad_accum:5.000000 total_samples:6992224.000000'}\n",
      "[Train Epoch]1/2 [Time]17682.80 [Step]43800 [Batch]219000 [Speed]80.74ms/step [Loss]8.5847 [Metrics]{'train_loss:8.584655 train_acc:0.076733 lr:0.000422 grad_accum:5.000000 total_samples:7008224.000000'}\n",
      "[Train Epoch]1/2 [Time]17721.00 [Step]43900 [Batch]219500 [Speed]80.73ms/step [Loss]8.5831 [Metrics]{'train_loss:8.583141 train_acc:0.076747 lr:0.000422 grad_accum:5.000000 total_samples:7024224.000000'}\n",
      "[Train Epoch]1/2 [Time]17759.21 [Step]44000 [Batch]220000 [Speed]80.72ms/step [Loss]8.5815 [Metrics]{'train_loss:8.581538 train_acc:0.076758 lr:0.000421 grad_accum:5.000000 total_samples:7040224.000000'}\n",
      "[Train Epoch]1/2 [Time]17797.46 [Step]44100 [Batch]220500 [Speed]80.71ms/step [Loss]8.5801 [Metrics]{'train_loss:8.580088 train_acc:0.076771 lr:0.000421 grad_accum:5.000000 total_samples:7056224.000000'}\n",
      "[Train Epoch]1/2 [Time]17835.74 [Step]44200 [Batch]221000 [Speed]80.70ms/step [Loss]8.5786 [Metrics]{'train_loss:8.578589 train_acc:0.076786 lr:0.000420 grad_accum:5.000000 total_samples:7072224.000000'}\n",
      "[Train Epoch]1/2 [Time]17873.98 [Step]44300 [Batch]221500 [Speed]80.70ms/step [Loss]8.5771 [Metrics]{'train_loss:8.577136 train_acc:0.076794 lr:0.000420 grad_accum:5.000000 total_samples:7088224.000000'}\n",
      "[Train Epoch]1/2 [Time]17912.23 [Step]44400 [Batch]222000 [Speed]80.69ms/step [Loss]8.5758 [Metrics]{'train_loss:8.575789 train_acc:0.076804 lr:0.000419 grad_accum:5.000000 total_samples:7104224.000000'}\n",
      "[Train Epoch]1/2 [Time]17950.50 [Step]44500 [Batch]222500 [Speed]80.68ms/step [Loss]8.5744 [Metrics]{'train_loss:8.574367 train_acc:0.076816 lr:0.000419 grad_accum:5.000000 total_samples:7120224.000000'}\n",
      "[Train Epoch]1/2 [Time]17988.77 [Step]44600 [Batch]223000 [Speed]80.67ms/step [Loss]8.5730 [Metrics]{'train_loss:8.572988 train_acc:0.076830 lr:0.000419 grad_accum:5.000000 total_samples:7136224.000000'}\n",
      "[Train Epoch]1/2 [Time]18027.04 [Step]44700 [Batch]223500 [Speed]80.66ms/step [Loss]8.5718 [Metrics]{'train_loss:8.571760 train_acc:0.076842 lr:0.000418 grad_accum:5.000000 total_samples:7152224.000000'}\n",
      "[Train Epoch]1/2 [Time]18065.29 [Step]44800 [Batch]224000 [Speed]80.65ms/step [Loss]8.5706 [Metrics]{'train_loss:8.570572 train_acc:0.076859 lr:0.000418 grad_accum:5.000000 total_samples:7168224.000000'}\n",
      "[Train Epoch]1/2 [Time]18103.55 [Step]44900 [Batch]224500 [Speed]80.64ms/step [Loss]8.5691 [Metrics]{'train_loss:8.569139 train_acc:0.076872 lr:0.000417 grad_accum:5.000000 total_samples:7184224.000000'}\n",
      "Saving checkpoint for epoch 1 at step 225000 on path model_bert4rec_complete_0.8.4\n",
      "[Train Epoch]1/2 [Time]18149.89 [Step]45000 [Batch]225000 [Speed]80.67ms/step [Loss]8.5678 [Metrics]{'train_loss:8.567806 train_acc:0.076885 lr:0.000417 grad_accum:5.000000 total_samples:7200224.000000'}\n",
      "[Train Epoch]1/2 [Time]18188.01 [Step]45100 [Batch]225500 [Speed]80.66ms/step [Loss]8.5662 [Metrics]{'train_loss:8.566243 train_acc:0.076900 lr:0.000416 grad_accum:5.000000 total_samples:7216224.000000'}\n",
      "[Train Epoch]1/2 [Time]18226.25 [Step]45200 [Batch]226000 [Speed]80.65ms/step [Loss]8.5648 [Metrics]{'train_loss:8.564799 train_acc:0.076909 lr:0.000416 grad_accum:5.000000 total_samples:7232224.000000'}\n",
      "[Train Epoch]1/2 [Time]18264.51 [Step]45300 [Batch]226500 [Speed]80.64ms/step [Loss]8.5635 [Metrics]{'train_loss:8.563455 train_acc:0.076925 lr:0.000415 grad_accum:5.000000 total_samples:7248224.000000'}\n",
      "[Train Epoch]1/2 [Time]18302.76 [Step]45400 [Batch]227000 [Speed]80.63ms/step [Loss]8.5619 [Metrics]{'train_loss:8.561928 train_acc:0.076939 lr:0.000415 grad_accum:5.000000 total_samples:7264224.000000'}\n",
      "[Train Epoch]1/2 [Time]18341.02 [Step]45500 [Batch]227500 [Speed]80.62ms/step [Loss]8.5605 [Metrics]{'train_loss:8.560524 train_acc:0.076946 lr:0.000414 grad_accum:5.000000 total_samples:7280224.000000'}\n",
      "[Train Epoch]1/2 [Time]18379.27 [Step]45600 [Batch]228000 [Speed]80.61ms/step [Loss]8.5592 [Metrics]{'train_loss:8.559214 train_acc:0.076950 lr:0.000414 grad_accum:5.000000 total_samples:7296224.000000'}\n",
      "[Train Epoch]1/2 [Time]18417.50 [Step]45700 [Batch]228500 [Speed]80.60ms/step [Loss]8.5578 [Metrics]{'train_loss:8.557829 train_acc:0.076963 lr:0.000413 grad_accum:5.000000 total_samples:7312224.000000'}\n",
      "[Train Epoch]1/2 [Time]18455.78 [Step]45800 [Batch]229000 [Speed]80.59ms/step [Loss]8.5564 [Metrics]{'train_loss:8.556443 train_acc:0.076977 lr:0.000413 grad_accum:5.000000 total_samples:7328224.000000'}\n",
      "[Train Epoch]1/2 [Time]18494.02 [Step]45900 [Batch]229500 [Speed]80.58ms/step [Loss]8.5550 [Metrics]{'train_loss:8.555023 train_acc:0.076990 lr:0.000413 grad_accum:5.000000 total_samples:7344224.000000'}\n",
      "[Train Epoch]1/2 [Time]18532.26 [Step]46000 [Batch]230000 [Speed]80.58ms/step [Loss]8.5537 [Metrics]{'train_loss:8.553672 train_acc:0.076998 lr:0.000412 grad_accum:5.000000 total_samples:7360224.000000'}\n",
      "[Train Epoch]1/2 [Time]18570.45 [Step]46100 [Batch]230500 [Speed]80.57ms/step [Loss]8.5523 [Metrics]{'train_loss:8.552298 train_acc:0.077010 lr:0.000412 grad_accum:5.000000 total_samples:7376224.000000'}\n",
      "[Train Epoch]1/2 [Time]18608.67 [Step]46200 [Batch]231000 [Speed]80.56ms/step [Loss]8.5511 [Metrics]{'train_loss:8.551099 train_acc:0.077020 lr:0.000411 grad_accum:5.000000 total_samples:7392224.000000'}\n",
      "[Train Epoch]1/2 [Time]18646.91 [Step]46300 [Batch]231500 [Speed]80.55ms/step [Loss]8.5498 [Metrics]{'train_loss:8.549809 train_acc:0.077026 lr:0.000411 grad_accum:5.000000 total_samples:7408224.000000'}\n",
      "[Train Epoch]1/2 [Time]18685.14 [Step]46400 [Batch]232000 [Speed]80.54ms/step [Loss]8.5484 [Metrics]{'train_loss:8.548381 train_acc:0.077036 lr:0.000410 grad_accum:5.000000 total_samples:7424224.000000'}\n",
      "[Train Epoch]1/2 [Time]18723.41 [Step]46500 [Batch]232500 [Speed]80.53ms/step [Loss]8.5471 [Metrics]{'train_loss:8.547106 train_acc:0.077048 lr:0.000410 grad_accum:5.000000 total_samples:7440224.000000'}\n",
      "[Train Epoch]1/2 [Time]18761.62 [Step]46600 [Batch]233000 [Speed]80.52ms/step [Loss]8.5459 [Metrics]{'train_loss:8.545889 train_acc:0.077063 lr:0.000409 grad_accum:5.000000 total_samples:7456224.000000'}\n",
      "[Train Epoch]1/2 [Time]18799.88 [Step]46700 [Batch]233500 [Speed]80.51ms/step [Loss]8.5446 [Metrics]{'train_loss:8.544644 train_acc:0.077072 lr:0.000409 grad_accum:5.000000 total_samples:7472224.000000'}\n",
      "[Train Epoch]1/2 [Time]18838.12 [Step]46800 [Batch]234000 [Speed]80.50ms/step [Loss]8.5432 [Metrics]{'train_loss:8.543242 train_acc:0.077086 lr:0.000409 grad_accum:5.000000 total_samples:7488224.000000'}\n",
      "[Train Epoch]1/2 [Time]18876.35 [Step]46900 [Batch]234500 [Speed]80.50ms/step [Loss]8.5418 [Metrics]{'train_loss:8.541821 train_acc:0.077096 lr:0.000408 grad_accum:5.000000 total_samples:7504224.000000'}\n",
      "[Train Epoch]1/2 [Time]18914.60 [Step]47000 [Batch]235000 [Speed]80.49ms/step [Loss]8.5406 [Metrics]{'train_loss:8.540627 train_acc:0.077107 lr:0.000408 grad_accum:5.000000 total_samples:7520224.000000'}\n",
      "[Train Epoch]1/2 [Time]18952.78 [Step]47100 [Batch]235500 [Speed]80.48ms/step [Loss]8.5393 [Metrics]{'train_loss:8.539348 train_acc:0.077120 lr:0.000407 grad_accum:5.000000 total_samples:7536224.000000'}\n",
      "[Train Epoch]1/2 [Time]18991.00 [Step]47200 [Batch]236000 [Speed]80.47ms/step [Loss]8.5381 [Metrics]{'train_loss:8.538108 train_acc:0.077137 lr:0.000407 grad_accum:5.000000 total_samples:7552224.000000'}\n",
      "[Train Epoch]1/2 [Time]19029.22 [Step]47300 [Batch]236500 [Speed]80.46ms/step [Loss]8.5368 [Metrics]{'train_loss:8.536819 train_acc:0.077148 lr:0.000406 grad_accum:5.000000 total_samples:7568224.000000'}\n",
      "[Train Epoch]1/2 [Time]19067.50 [Step]47400 [Batch]237000 [Speed]80.45ms/step [Loss]8.5356 [Metrics]{'train_loss:8.535633 train_acc:0.077153 lr:0.000406 grad_accum:5.000000 total_samples:7584224.000000'}\n",
      "[Train Epoch]1/2 [Time]19105.75 [Step]47500 [Batch]237500 [Speed]80.45ms/step [Loss]8.5344 [Metrics]{'train_loss:8.534363 train_acc:0.077159 lr:0.000406 grad_accum:5.000000 total_samples:7600224.000000'}\n",
      "[Train Epoch]1/2 [Time]19144.01 [Step]47600 [Batch]238000 [Speed]80.44ms/step [Loss]8.5330 [Metrics]{'train_loss:8.533002 train_acc:0.077168 lr:0.000405 grad_accum:5.000000 total_samples:7616224.000000'}\n",
      "[Train Epoch]1/2 [Time]19182.24 [Step]47700 [Batch]238500 [Speed]80.43ms/step [Loss]8.5318 [Metrics]{'train_loss:8.531753 train_acc:0.077178 lr:0.000405 grad_accum:5.000000 total_samples:7632224.000000'}\n",
      "[Train Epoch]1/2 [Time]19220.51 [Step]47800 [Batch]239000 [Speed]80.42ms/step [Loss]8.5306 [Metrics]{'train_loss:8.530639 train_acc:0.077189 lr:0.000404 grad_accum:5.000000 total_samples:7648224.000000'}\n",
      "[Train Epoch]1/2 [Time]19258.72 [Step]47900 [Batch]239500 [Speed]80.41ms/step [Loss]8.5294 [Metrics]{'train_loss:8.529388 train_acc:0.077205 lr:0.000404 grad_accum:5.000000 total_samples:7664224.000000'}\n",
      "[Train Epoch]1/2 [Time]19296.97 [Step]48000 [Batch]240000 [Speed]80.40ms/step [Loss]8.5281 [Metrics]{'train_loss:8.528055 train_acc:0.077224 lr:0.000403 grad_accum:5.000000 total_samples:7680224.000000'}\n",
      "[Train Epoch]1/2 [Time]19335.24 [Step]48100 [Batch]240500 [Speed]80.40ms/step [Loss]8.5269 [Metrics]{'train_loss:8.526903 train_acc:0.077235 lr:0.000403 grad_accum:5.000000 total_samples:7696224.000000'}\n",
      "[Train Epoch]1/2 [Time]19373.46 [Step]48200 [Batch]241000 [Speed]80.39ms/step [Loss]8.5256 [Metrics]{'train_loss:8.525553 train_acc:0.077245 lr:0.000403 grad_accum:5.000000 total_samples:7712224.000000'}\n",
      "[Train Epoch]1/2 [Time]19411.67 [Step]48300 [Batch]241500 [Speed]80.38ms/step [Loss]8.5242 [Metrics]{'train_loss:8.524235 train_acc:0.077261 lr:0.000402 grad_accum:5.000000 total_samples:7728224.000000'}\n",
      "[Train Epoch]1/2 [Time]19449.91 [Step]48400 [Batch]242000 [Speed]80.37ms/step [Loss]8.5231 [Metrics]{'train_loss:8.523056 train_acc:0.077270 lr:0.000402 grad_accum:5.000000 total_samples:7744224.000000'}\n",
      "[Train Epoch]1/2 [Time]19489.76 [Step]48500 [Batch]242500 [Speed]80.37ms/step [Loss]8.5218 [Metrics]{'train_loss:8.521774 train_acc:0.077279 lr:0.000401 grad_accum:5.000000 total_samples:7760224.000000'}\n",
      "[Train Epoch]1/2 [Time]19531.86 [Step]48600 [Batch]243000 [Speed]80.38ms/step [Loss]8.5204 [Metrics]{'train_loss:8.520441 train_acc:0.077299 lr:0.000401 grad_accum:5.000000 total_samples:7776224.000000'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 116\u001b[0m\n\u001b[1;32m    112\u001b[0m inputs, target \u001b[39m=\u001b[39m batch_data\n\u001b[1;32m    113\u001b[0m grad_accum \u001b[39m=\u001b[39m grad_accum_scheduler(total_samples,\n\u001b[1;32m    114\u001b[0m                                   list_scheduler\u001b[39m=\u001b[39mlist_scheduler, \n\u001b[1;32m    115\u001b[0m                                   max_grad_accum\u001b[39m=\u001b[39mBERT4REC_CONFIG\u001b[39m.\u001b[39mtup_scheduler_grad_accum[\u001b[39m1\u001b[39m])                                                             \n\u001b[0;32m--> 116\u001b[0m step_gradients \u001b[39m=\u001b[39m train_step(inputs, target\u001b[39m=\u001b[39;49mtarget, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, num_accum_steps\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mconstant(grad_accum, tf\u001b[39m.\u001b[39;49mfloat32), loss\u001b[39m=\u001b[39;49mtrain_loss, acc\u001b[39m=\u001b[39;49mtrain_acc, seq_type\u001b[39m=\u001b[39;49minputs[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    117\u001b[0m global_gradients \u001b[39m=\u001b[39m backward_optimization(grad_accum, global_gradients, step_gradients, total_step, model, optimizer)\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m batch_num \u001b[39m%\u001b[39m BERT4REC_CONFIG\u001b[39m.\u001b[39mbatch_num_printer_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '1_Model_v0.4.ipynb'\n",
    "\n",
    "class BERT4REC_CONFIG:\n",
    "    num_items = NUM_ITEMS\n",
    "    path_tfrecords = '../tfrecords/tfrecords_v0.4/'\n",
    "    restore_last_chekpoint = (False, 'model_bert4rec_complete_0.8.3/checkpoints/', 'ckpt-12')\n",
    "    model_name = 'model_bert4rec_complete_0.8.4'\n",
    "    checkpoint_filepath = f'../2_Models/'\n",
    "    num_records_dataset = 10_000_000\n",
    "    batch_size = 32\n",
    "    tup_scheduler_grad_accum = (1, 5, 1_000_000) #(start_grad_accum, max_grad_accum, ramp_up_samples)\n",
    "    seq_len = 20\n",
    "    mask_prob = 0.3\n",
    "    reverse_prob = 0.5\n",
    "    emb_dim = 256\n",
    "    trf_dim = 256\n",
    "    num_heads = 4\n",
    "    num_layers = 1\n",
    "    ff_dim = trf_dim*4\n",
    "    drop_rate = 0.1\n",
    "    att_drop_rate = 0.1\n",
    "    epochs = 2\n",
    "    early_stopping = 5\n",
    "    batch_num_printer_train = 500\n",
    "    batch_num_printer_val = 250\n",
    "    clipnorm = 1.0\n",
    "    num_iters_save_checkpoint = 25_000\n",
    "    scheduler_scaler = 128 \n",
    "    warmup_steps = 10_000\n",
    "    weight_decay = 1e-1\n",
    "    log_wandb = True\n",
    "\n",
    "list_scheduler = np.linspace(BERT4REC_CONFIG.tup_scheduler_grad_accum[0], \n",
    "                             BERT4REC_CONFIG.tup_scheduler_grad_accum[1], \n",
    "                             BERT4REC_CONFIG.tup_scheduler_grad_accum[2]).astype(np.uint8).tolist()\n",
    "\n",
    "if BERT4REC_CONFIG.log_wandb:\n",
    "    time_suffix = datetime.now().__str__().split('.')[0]\n",
    "    dict_config = {k : v for k, v in zip(BERT4REC_CONFIG.__dict__.keys(), BERT4REC_CONFIG.__dict__.values()) if not k.startswith('__')}\n",
    "    init_wandb(wandb_project='otto-recsys', entity='enric1296', run_name=f'{BERT4REC_CONFIG.model_name}_{time_suffix}', dict_config=dict_config)\n",
    "    \n",
    "\n",
    "list_paths_train = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=train/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=train')]\n",
    "np.random.shuffle(list_paths_train)\n",
    "list_paths_val = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=val/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=val')]\n",
    "\n",
    "train_dataloader = Bert4RecDataLoader(list_paths_train, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len, \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=BERT4REC_CONFIG.mask_prob, \n",
    "                                     reverse_prob=BERT4REC_CONFIG.reverse_prob, \n",
    "                                     is_test=False,\n",
    "                                     is_val=False,\n",
    "                                     shuffle=True,\n",
    "                                     drop_remainder=True).get_generator()\n",
    "\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len,  \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     get_session=False,\n",
    "                                     is_val=True,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "optimizer = optimizers.Adam(learning_rate=CustomSchedule(BERT4REC_CONFIG.scheduler_scaler, warmup_steps=BERT4REC_CONFIG.warmup_steps),\n",
    "                            clipnorm=BERT4REC_CONFIG.clipnorm)\n",
    "                            # weight_decay=BERT4REC_CONFIG.weight_decay)                  \n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)                           \n",
    "                            \n",
    "# Build utils\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "if BERT4REC_CONFIG.restore_last_chekpoint[0]:\n",
    "    checkpoint_path = os.path.join(BERT4REC_CONFIG.checkpoint_filepath, BERT4REC_CONFIG.restore_last_chekpoint[1])\n",
    "    ckpt.restore(os.path.join(checkpoint_path, BERT4REC_CONFIG.restore_last_chekpoint[2]))\n",
    "    print('Latest checkpoint restored!!')\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)\n",
    "else:\n",
    "    checkpoint_path = create_folder_with_version(BERT4REC_CONFIG.model_name, BERT4REC_CONFIG.checkpoint_filepath)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, os.path.join(BERT4REC_CONFIG.checkpoint_filepath, checkpoint_path, 'checkpoints'), \n",
    "                                            max_to_keep=10)\n",
    "\n",
    "# Loss function\n",
    "loss_function = weighted_loss_bert4rec()#custom_loss_bert4rec()\n",
    "acc_function = custom_accuracy()\n",
    "\n",
    "# Trackers\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "train_acc = tf.keras.metrics.Mean(name='train_acc')\n",
    "val_acc = tf.keras.metrics.Mean(name='val_acc')\n",
    "\n",
    "##############################################\n",
    "\n",
    "global_gradients = []\n",
    "total_step, val_step, total_samples = 0, 0, 0\n",
    "for epoch in range(BERT4REC_CONFIG.epochs):\n",
    "    start = time.time()\n",
    "    print('===='*20)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    metrics_reset_states(train_loss, val_loss, train_acc, val_acc)\n",
    "    \n",
    "    for batch_num, batch_data in enumerate(train_dataloader):\n",
    "        inputs, target = batch_data\n",
    "        grad_accum = grad_accum_scheduler(total_samples,\n",
    "                                          list_scheduler=list_scheduler, \n",
    "                                          max_grad_accum=BERT4REC_CONFIG.tup_scheduler_grad_accum[1])                                                             \n",
    "        step_gradients = train_step(inputs, target=target, model=model, optimizer=optimizer, num_accum_steps=tf.constant(grad_accum, tf.float32), loss=train_loss, acc=train_acc, seq_type=inputs[1])\n",
    "        global_gradients = backward_optimization(grad_accum, global_gradients, step_gradients, total_step, model, optimizer)\n",
    "        if batch_num % BERT4REC_CONFIG.batch_num_printer_train == 0:\n",
    "            train_dict_metrics = {x.name : x.result() for x in [train_loss, train_acc]}\n",
    "            train_dict_metrics.update({'lr' : optimizer.lr(total_step//grad_accum).numpy().astype(np.float32), 'grad_accum' : grad_accum, 'total_samples' : total_samples})\n",
    "            fancy_printer(train_loss, epoch, batch_num, start, step='Train', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=train_dict_metrics, num_step=total_step // grad_accum)\n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                train_dict_metrics.update({'step_grad' : total_step//grad_accum, 'step' : total_step})\n",
    "                log_wandb_metrics(step='train', num_step=total_step, gradients=global_gradients, dict_metrics=train_dict_metrics)     \n",
    "        total_step += 1  \n",
    "        total_samples += BERT4REC_CONFIG.batch_size * grad_accum if total_step % grad_accum==0 else 0\n",
    "        if total_step % BERT4REC_CONFIG.num_iters_save_checkpoint==0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print(f'Saving checkpoint for epoch {epoch+1} at step {total_step} on path {checkpoint_path}')  \n",
    "     \n",
    "    for val_batch_num, val_batch_data in enumerate(val_dataloader):\n",
    "        inputs, target = val_batch_data\n",
    "        predictions = test_step(inputs, target=target, loss=val_loss, acc=val_acc, seq_type=inputs[1])\n",
    "        val_step += 1\n",
    "        if val_batch_num % BERT4REC_CONFIG.batch_num_printer_val == 0:\n",
    "            val_dict_metrics = {x.name : x.result() for x in [val_loss, val_acc]}\n",
    "            fancy_printer(val_loss, epoch, val_batch_num, start, step='Val', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=val_dict_metrics, num_step=val_step)    \n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                log_wandb_metrics(step='val', num_step=val_step, dict_metrics=val_dict_metrics) \n",
    "                # if val_batch_num==0:\n",
    "                #     log_wandb_metrics(step=None, plot_image=True, \n",
    "                #                       model=model, inputs=inputs, epoch=epoch, target=target, stats=stats)\n",
    "    \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {checkpoint_path}')        \n",
    "    \n",
    "    epoch_dict_metrics = {x.name : x.result() for x in [train_loss, val_loss, train_recall_k]}\n",
    "    printer = fancy_printer(None, epoch, epoch, start, step='epoch', num_step=epoch, dict_metrics=epoch_dict_metrics, \n",
    "                            train_loss=train_loss, val_loss=val_loss)\n",
    "    if BERT4REC_CONFIG.log_wandb:\n",
    "        log_wandb_metrics(step='epoch', num_step=total_step, dict_metrics=epoch_dict_metrics)\n",
    "\n",
    "if BERT4REC_CONFIG.log_wandb:\n",
    "    # wandb.save(checkpoint_path)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 23:57:16.073451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "0it [00:00, ?it/s]2022-11-23 23:57:17.448807: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "1000it [03:01,  5.50it/s]\n",
      "100%|██████████| 192192/192192 [00:00<00:00, 235308.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.921920e+05</td>\n",
       "      <td>81269.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.373092e+06</td>\n",
       "      <td>0.266961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.720560e+06</td>\n",
       "      <td>0.431234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.129786e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.354874e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.572168e+06</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.289973e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            session         score\n",
       "count  1.921920e+05  81269.000000\n",
       "mean   6.373092e+06      0.266961\n",
       "std    3.720560e+06      0.431234\n",
       "min    1.600000e+01      0.000000\n",
       "25%    3.129786e+06      0.000000\n",
       "50%    6.354874e+06      0.000000\n",
       "75%    9.572168e+06      0.666667\n",
       "max    1.289973e+07      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'carts': 0.3504520904714635,\n",
       " 'clicks': 0.22126267582597317,\n",
       " 'orders': 0.5380787421674682}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Metric: 0.4501\n"
     ]
    }
   ],
   "source": [
    "def get_score_metric(y_true, y_pred, type_target, k=20):\n",
    "    score = 0 \n",
    "    if len(y_true)==0:\n",
    "        return None\n",
    "    if type_target=='clicks':\n",
    "        num_targets = 1\n",
    "        hits = len([x for x in y_pred if x==y_true[0]])\n",
    "    else:\n",
    "        num_targets = min(k, len(y_true))\n",
    "        hits = len([x for x in y_pred if x in y_true])\n",
    "    score = hits / num_targets\n",
    "    return score\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.8.4/checkpoints'))\n",
    "list_paths_val = ['../tfrecords/tfrecords_v0.4/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=val')]\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20, \n",
    "                                     seq_len_target=20, \n",
    "                                     batch_size=64, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0, \n",
    "                                     is_val=True,\n",
    "                                     get_session=True, \n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "list_sessions, list_predictions, list_trues, list_types = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(val_dataloader)):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    target, type_target, idx_mask = targets\n",
    "    idxs = idx_mask.numpy() #tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[x for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        labels = [list(set([_target for _type, _target in zip(type_target.numpy()[i], target.numpy()[i]) if dict_map_type[type_]==_type and _target!=0])) for i in range(target.shape[0])]\n",
    "        ###\n",
    "        list_sessions.append(session.numpy())\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_trues = list_trues + labels\n",
    "    if num_batch==1_000:\n",
    "        break\n",
    "\n",
    "df_val = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'trues' : list_trues,\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_val['score'] = df_val.progress_apply(lambda x: get_score_metric(x['trues'], x['predictions'], x['type']), axis=1)\n",
    "\n",
    "display(df_val.describe())\n",
    "dict_scores = df_val.groupby('type')['score'].mean().to_dict()\n",
    "display(dict_scores)\n",
    "kaggle_metric = 0.1*dict_scores['clicks'] + 0.3*dict_scores['carts'] + 0.6*dict_scores['orders']\n",
    "print(f'Kaggle Metric: {kaggle_metric:.4f}')\n",
    "\n",
    "\n",
    "# (seq_len=20)model_bert4rec_complete_0.7 - ckpt27\n",
    "# {'carts': 0.3470019827927542,\n",
    "#  'clicks': 0.23410206084396468,\n",
    "#  'orders': 0.514586102958196}\n",
    "# Kaggle Metric: 0.4363\n",
    "\n",
    "# (seq_len=20)model_bert4rec_complete_0.8.4 - ckpt27\n",
    "# {'carts': 0.3504520904714635,\n",
    "#  'clicks': 0.22126267582597317,\n",
    "#  'orders': 0.5380787421674682}\n",
    "# Kaggle Metric: 0.4501\n",
    "\n",
    "# import wandb\n",
    "# api = wandb.Api()\n",
    "\n",
    "# run = api.run(\"<path to run>\")\n",
    "# run.summary[\"kaggle_metric\"] = kaggle_metric\n",
    "# run.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2azkkync) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aff4a8bf8940a29c3471a931b49af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.240576…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model_bert4rec_complete_0.8_finetune_fold_0</strong>: <a href=\"https://wandb.ai/enric1296/otto-recsys/runs/2azkkync\" target=\"_blank\">https://wandb.ai/enric1296/otto-recsys/runs/2azkkync</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221122_114558-2azkkync/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2azkkync). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e74e0d3acf4cf0b53a2d666f6c51de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668100016666663, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/enric/SSD1TB/KAGGLE/025_Kaggle-OTTO Recsys-2022/1_Scripts/wandb/run-20221122_114635-32p0b058</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/enric1296/otto-recsys/runs/32p0b058\" target=\"_blank\">model_bert4rec_complete_0.8_finetune_fold_0</a></strong> to <a href=\"https://wandb.ai/enric1296/otto-recsys\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Fold: 0\n",
      "========================================================================================================================\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 11:46:42.819523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n",
      "/home/enric/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:436: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 167903104 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-11-22 11:46:46.800646: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14149/Unknown - 4639s 328ms/step - loss: 8.2624 - seq_acc: 0.0945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m ckpt\u001b[39m.\u001b[39mrestore(tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mlatest_checkpoint(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../2_Models/model_bert4rec_complete_0.8/checkpoints\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     46\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m4e-5\u001b[39m, clipnorm\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m),\n\u001b[1;32m     47\u001b[0m               loss\u001b[39m=\u001b[39mloss_function,\n\u001b[1;32m     48\u001b[0m               metrics\u001b[39m=\u001b[39m[acc_function])\n\u001b[0;32m---> 50\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataloader,\n\u001b[1;32m     51\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m     52\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     53\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[WandbCallback()],\n\u001b[1;32m     54\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     55\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     57\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../2_Models/model_bert4rec_complete_0.7_finetuned_fold_\u001b[39m\u001b[39m{\u001b[39;00mnum_fold\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)                   \n\u001b[1;32m     58\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/engine/training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \n\u001b[1;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/utils/tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/keras/utils/tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/miniconda3/envs/py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "list_paths = ['../tfrecords/tfrecords_v0.4/na_split=test_aug/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=test_aug')]# + \\\n",
    "            #  ['../tfrecords/tfrecords_v0.4/na_split=val_aug/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=val_aug')] \n",
    "np.random.shuffle(list_paths)\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for num_fold, (train_idxs, val_idxs) in enumerate(kfold.split(list_paths)):\n",
    "    train_paths = np.asarray(list_paths)[train_idxs]\n",
    "    val_paths = np.asarray(list_paths)[val_idxs]\n",
    "    if BERT4REC_CONFIG.log_wandb:\n",
    "        time_suffix = datetime.now().__str__().split('.')[0]\n",
    "        dict_config = {k : v for k, v in zip(BERT4REC_CONFIG.__dict__.keys(), BERT4REC_CONFIG.__dict__.values()) if not k.startswith('__')}\n",
    "        init_wandb(wandb_project='otto-recsys', entity='enric1296', run_name=f'{BERT4REC_CONFIG.model_name}_finetune_fold_{num_fold}', dict_config=dict_config)\n",
    "    print('===='*30)\n",
    "    print(f'Fold: {num_fold}')\n",
    "    print('===='*30)\n",
    "\n",
    "    train_dataloader = Bert4RecDataLoader(train_paths, \n",
    "                                         num_items=NUM_ITEMS, \n",
    "                                        seq_len=20,  \n",
    "                                        batch_size=32, \n",
    "                                        mask_prob=0.4, \n",
    "                                        reverse_prob=0.25,  \n",
    "                                        is_val=False,\n",
    "                                        is_test=False,\n",
    "                                        get_session=False,\n",
    "                                        shuffle=True).get_generator()\n",
    "\n",
    "    val_dataloader = Bert4RecDataLoader(val_paths, \n",
    "                                        num_items=NUM_ITEMS, \n",
    "                                        seq_len=20,  \n",
    "                                        batch_size=32, \n",
    "                                        mask_prob=0.4, \n",
    "                                        reverse_prob=0.25,  \n",
    "                                        is_val=True,\n",
    "                                        is_test=False,\n",
    "                                        get_session=False,\n",
    "                                        shuffle=False).get_generator()\n",
    "\n",
    "    loss_function = custom_loss_bert4rec()\n",
    "    acc_function = custom_accuracy()\n",
    "    model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.8/checkpoints'))\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=4e-5, clipnorm=1.0),\n",
    "                  loss=loss_function,\n",
    "                  metrics=[acc_function])\n",
    "\n",
    "    history = model.fit(train_dataloader,\n",
    "                        validation_data=val_dataloader,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[WandbCallback()],\n",
    "                        epochs=1,\n",
    "                        verbose=1)\n",
    "\n",
    "    model.save(f'../2_Models/model_bert4rec_complete_0.7_finetuned_fold_{num_fold}/')                   \n",
    "    wandb.finish()\n",
    "\n",
    "# 173/Unknown - 22s 113ms/step - loss: 7.9368 - recall_20: 0.3452\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 19:15:23.433225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "0it [00:00, ?it/s]2022-11-20 19:15:24.337587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "26122it [55:08,  7.90it/s]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(tf.train.latest_checkpoint(f'../2_Models/model_bert4rec_complete_0.7/checkpoints'))\n",
    "\n",
    "\n",
    "list_paths_test = ['../tfrecords/tfrecords_v0.4/na_split=test/' + x for x in os.listdir('../tfrecords/tfrecords_v0.4/na_split=test')]\n",
    "test_dataloader = Bert4RecDataLoader(list_paths_test, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=20,  \n",
    "                                     batch_size=64, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     is_val=False,\n",
    "                                     is_test=True,\n",
    "                                     get_session=True,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "list_predictions, list_sessions, list_types, list_scores = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(test_dataloader)):\n",
    "    features, idxs, session = batch\n",
    "    seq_items, seq_type, seq_time, seq_recency = features\n",
    "    idxs = idxs.numpy()\n",
    "    # idxs = tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    ###\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time, seq_recency)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[dict_map[x] for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        topk_idxs = topk_idxs - 1\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_sessions.append(session.numpy())\n",
    "    # if num_batch==100:\n",
    "    #     break\n",
    "    \n",
    "\n",
    "# 26122it [54:28,  7.99it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_submission = f\"submission_{datetime.now().__str__().split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_')}\"\n",
    "\n",
    "df_inference = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_inference['session_type'] = df_inference['session'].astype(str) + '_' + df_inference['type']\n",
    "df_inference['labels'] = df_inference['predictions'].apply(lambda x : ' '.join([str(y) for y in x]))\n",
    "df_inference[['session_type', 'labels']].to_csv(f'../3_Submissions/{name_submission}.csv', index=False)\n",
    "\n",
    "print(df_inference.shape)\n",
    "display(\n",
    "    df_inference\n",
    ")\n",
    "\n",
    "import gzip\n",
    "with open(f'../3_Submissions/{name_submission}.csv', 'rb') as f_in, gzip.open(f'../3_Submissions/{name_submission}.csv.gz', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0432fa0070c5c9f7d9e158f590013ccc765eb84f02e6f69521746370c3bf6c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
