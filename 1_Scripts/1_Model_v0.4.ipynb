{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, models, metrics, optimizers, constraints\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# tfrecords for kaggle\n",
    "\n",
    "# name_dataset = 'tfrecords_v0.3_kaggle'\n",
    "# path_out = f'../tfrecords/{name_dataset}/'\n",
    "\n",
    "# if not os.path.exists(path_out):\n",
    "#     os.mkdir(path_out)\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_train'):\n",
    "#     os.rename(path_out + 'na_split_train/' + file, \n",
    "#               path_out + 'na_split_train/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_val'):\n",
    "#     os.rename(path_out + 'na_split_val/' + file, \n",
    "#               path_out + 'na_split_val/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "# for file in os.listdir(path_out + 'na_split_test'):\n",
    "#     os.rename(path_out + 'na_split_test/' + file, \n",
    "#               path_out + 'na_split_test/' + file.replace('-', '_').replace('gz', 'tfrec'))\n",
    "\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1855603/1855603 [00:00<00:00, 3289882.17it/s]\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# Paths & Global Variables\n",
    "\n",
    "# Train: (datetime.datetime(2022, 7, 31, 22, 0, 0, 25000), datetime.datetime(2022, 8, 28, 21, 59, 59, 984000))\n",
    "# Test: (datetime.datetime(2022, 8, 28, 22, 0, 0, 278000), datetime.datetime(2022, 9, 4, 21, 59, 51, 563000))\n",
    "\n",
    "path_data_raw = '../0_Data/'\n",
    "\n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "df_mapping = pd.read_csv('../tfrecords/tfrecords_v0.3/df_mapping.csv')\n",
    "NUM_ITEMS = len(df_mapping['aid_map'].unique())\n",
    "print(NUM_ITEMS)\n",
    "\n",
    "dict_map = {}\n",
    "for x in tqdm(df_mapping.to_dict('records')):\n",
    "    dict_map[x['aid_map']] = x['aid']\n",
    "\n",
    "dict_map_type = {\n",
    "    'clicks' : 1,\n",
    "    'carts' : 2,\n",
    "    'orders' : 3\n",
    "  }\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert4RecDataLoader:\n",
    "    \"\"\"\n",
    "    Class that iterates over tfrecords in order to get the sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_paths, num_items, seq_len, batch_size, num_targets=-1, mask_prob=0.4, \n",
    "                 reverse_prob=0.2, get_session=False, get_only_first_on_val=False, seq_len_target=None,\n",
    "                 min_size_seq_to_mask=2, is_val=False, is_test=False, avoid_repeats=False, shuffle=False, drop_remainder=False):\n",
    "        self.list_paths = list_paths\n",
    "        self.num_items = num_items\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_targets = num_targets\n",
    "        self.mask_prob = mask_prob\n",
    "        self.reverse_prob = tf.constant(reverse_prob)\n",
    "        self.shuffle = shuffle\n",
    "        self.min_size_seq_to_mask = min_size_seq_to_mask\n",
    "        self.avoid_repeats = avoid_repeats\n",
    "        self.get_session = get_session\n",
    "        self.seq_len_target = seq_len if not seq_len_target else seq_len_target\n",
    "        self.get_only_first_on_val = get_only_first_on_val\n",
    "        self.is_val = is_val\n",
    "        self.is_test = is_test\n",
    "        self.drop_remainder = drop_remainder\n",
    "\n",
    "    def get_generator(self):\n",
    "        dataset = tf.data.TFRecordDataset(self.list_paths, num_parallel_reads=AUTO, compression_type='GZIP')\n",
    "        dataset = dataset.map(self.parse_tf_record, num_parallel_calls=AUTO)\n",
    "        if self.is_val:\n",
    "            dataset = dataset.map(self.make_transforms_val, num_parallel_calls=AUTO)\n",
    "        elif self.is_test:\n",
    "            dataset = dataset.map(self.make_transforms_test, num_parallel_calls=AUTO)\n",
    "        else:\n",
    "            dataset = dataset.map(self.make_transforms_train, num_parallel_calls=AUTO)\n",
    "        dataset = dataset.map(self.set_shapes, num_parallel_calls=AUTO)\n",
    "        if self.shuffle:\n",
    "            dataset = dataset.shuffle(self.batch_size*50, reshuffle_each_iteration=True)\n",
    "\n",
    "        dataset = dataset.batch(self.batch_size, num_parallel_calls=AUTO, drop_remainder=self.drop_remainder).prefetch(AUTO)\n",
    "        return dataset\n",
    "\n",
    "    def parse_tf_record(self, data):\n",
    "        features_context = {\n",
    "             \"session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "             \"size_session\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        if not self.is_val:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        else:\n",
    "            features_seq = {\n",
    "                \"seq_aid\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_aid_target\" : tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_type_target\": tf.io.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=False),\n",
    "                \"seq_time_encoding\": tf.io.FixedLenSequenceFeature(shape=[8], dtype=tf.float32, allow_missing=False)\n",
    "            }\n",
    "        data_context, data_sequence = tf.io.parse_single_sequence_example(data, context_features=features_context, sequence_features=features_seq)\n",
    "        return data_context, data_sequence\n",
    "\n",
    "    def pad_sequence(self, seq_to_pad, maxlen, return_pad_mask=False, dtype=tf.float32):\n",
    "        length, num_feats = tf.shape(seq_to_pad)[0], tf.shape(seq_to_pad)[-1]\n",
    "        ###\n",
    "        if length < maxlen:\n",
    "            pad = tf.zeros((maxlen - length, num_feats), dtype)\n",
    "            seq = tf.concat([seq_to_pad, pad], axis=0)\n",
    "            pad_mask = tf.concat([tf.ones(tf.shape(seq_to_pad), dtype=seq_to_pad.dtype), \n",
    "                                 pad], axis=0)\n",
    "        else:\n",
    "            seq = seq_to_pad[-maxlen:, :]\n",
    "            pad_mask = tf.ones((maxlen, tf.shape(seq_to_pad)[-1]), dtype=seq_to_pad.dtype)\n",
    "        if return_pad_mask:\n",
    "            return seq, pad_mask\n",
    "        return seq \n",
    "\n",
    "    def make_transforms_val(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding']\n",
    "        seq_items_target_raw, seq_type_target_raw =  dict_sequences['seq_aid_target'], dict_sequences['seq_type_target']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        ###\n",
    "        # Build target\n",
    "        seq_items, seq_target = seq_items, seq_items_target_raw[:1] if not self.get_session else seq_items_target_raw[:self.seq_len_target]\n",
    "        seq_type, seq_type_target = seq_type, seq_type_target_raw[:1] if not self.get_session else seq_type_target_raw[:self.seq_len_target]\n",
    "        seq_time_encoding, seq_time_encoding_target = seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)\n",
    "        seq_items_target = tf.concat([seq_items, seq_target], axis=0)\n",
    "        seq_type_target = tf.concat([seq_type, seq_type_target], axis=0)\n",
    "        ###\n",
    "        #Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, seq_type_target[:1]], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_items_target = self.pad_sequence(seq_items_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "        seq_type_target = self.pad_sequence(seq_type_target, maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)\n",
    "        \n",
    "        if self.get_session:\n",
    "            seq_items_target_all = self.pad_sequence(seq_items_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64)  \n",
    "            seq_type_target_all = self.pad_sequence(seq_type_target_raw[:self.seq_len_target], maxlen=self.seq_len_target, return_pad_mask=False, dtype=tf.int64) \n",
    "            return (seq_items, seq_type, seq_time_encoding), (seq_items_target_all[:, 0], seq_type_target_all[:, 0]), session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding), seq_items_target[:, 0]\n",
    "\n",
    "    def make_transforms_test(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding']\n",
    "        session, qt_size_seq = dict_context['session'], dict_context['size_session']\n",
    "        ###\n",
    "        seq_items = seq_items[-self.seq_len:, :]\n",
    "        seq_type = seq_type[-self.seq_len:, :]\n",
    "        seq_time_encoding = seq_time_encoding[-self.seq_len:, :]\n",
    "        #Mask last position\n",
    "        seq_items = tf.concat([seq_items, tf.zeros((1, tf.shape(seq_items)[1]), tf.int64)], axis=0)\n",
    "        seq_type = tf.concat([seq_type, tf.zeros((1, tf.shape(seq_type)[1]), tf.int64)], axis=0)\n",
    "        seq_time_encoding = tf.concat([seq_time_encoding, tf.zeros((1, tf.shape(seq_time_encoding)[1]), tf.float32)], axis=0)\n",
    "        ###\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)   \n",
    "\n",
    "        if self.get_session:\n",
    "            return (seq_items, seq_type, seq_time_encoding), tf.zeros(tf.shape(seq_items)), session\n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding), tf.zeros(tf.shape(seq_items))\n",
    "\n",
    "  \n",
    "    def make_transforms_train(self, dict_context, dict_sequences):\n",
    "        seq_items, seq_type, seq_time_encoding =  dict_sequences['seq_aid'], dict_sequences['seq_type'], dict_sequences['seq_time_encoding']\n",
    "        qt_size_seq = dict_context['size_session']\n",
    "        ### \n",
    "        # With prob reverse\n",
    "        if tf.random.uniform(shape=(1,1)) <= self.reverse_prob:\n",
    "            seq_items = tf.reverse(seq_items, axis=[0])\n",
    "            seq_type = tf.reverse(seq_type, axis=[0])\n",
    "            seq_time_encoding = tf.reverse(seq_time_encoding, axis=[0])\n",
    "            \n",
    "        # If our seq is longer than seq_len we can use it for data augmentation purpose \n",
    "        # and select a random idx to begin with.\n",
    "        if tf.shape(seq_items)[0] > self.seq_len:\n",
    "            idx_list = tf.range(tf.shape(seq_items)[0]-self.seq_len) \n",
    "            rand_idx = tf.random.shuffle(idx_list)[0]\n",
    "            seq_items = seq_items[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_type = seq_type[rand_idx:(rand_idx+self.seq_len), :]\n",
    "            seq_time_encoding = seq_time_encoding[rand_idx:(rand_idx+self.seq_len), :]\n",
    "        \n",
    "        qt_size_seq = tf.shape(seq_items)[0]\n",
    "\n",
    "        ## Get idxs to mask for inputs and targets\n",
    "        probs = tf.random.uniform(shape=(qt_size_seq,), minval=0, maxval=1)\n",
    "        idxs_inputs = tf.cast(tf.where(probs >= (1-self.mask_prob)), tf.int64) # -> we mask to zero the inputs as we dont want to leak \n",
    "        idxs_target = tf.cast(tf.where(probs < (1-self.mask_prob)), tf.int64) # -> we mask to zero the targets as the loss will only be applied on non zero\n",
    "\n",
    "        # If all items are masked we leave an item unmasked\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.cast(qt_size_seq, tf.int64):\n",
    "            idxs_target = idxs_inputs[-1:]\n",
    "            idxs_inputs = idxs_inputs[:-1]\n",
    "            \n",
    "        # If no item has been masked we leave at least one item masked(be careful of size=1 seqs)\n",
    "        if tf.cast(tf.shape(idxs_inputs)[0], tf.int64) == tf.constant(0, dtype=tf.int64):\n",
    "            all_idxs = tf.cast(tf.random.shuffle(tf.range(0, qt_size_seq)), dtype=tf.int64)\n",
    "            idxs_inputs = all_idxs[:1][:, tf.newaxis]\n",
    "            idxs_target = all_idxs[1:][:, tf.newaxis]\n",
    "\n",
    "        # Mask inputs and targets\n",
    "        seq_items_raw = seq_items\n",
    "        updates_items = tf.zeros((len(idxs_inputs), seq_items.shape[-1]), tf.int64)\n",
    "        # updates_type = tf.zeros((len(idxs_inputs), seq_type.shape[-1]), tf.int64)\n",
    "        updates_time_encoding = tf.zeros((len(idxs_inputs), seq_time_encoding.shape[-1]), tf.float32)\n",
    "        updates_target = tf.zeros((len(idxs_target), seq_items_raw.shape[-1]), tf.int64)\n",
    "        \n",
    "        seq_items = tf.tensor_scatter_nd_update(seq_items, idxs_inputs, updates_items)\n",
    "        # seq_type = tf.tensor_scatter_nd_update(seq_type, idxs_inputs, updates_type)\n",
    "        seq_time_encoding = tf.tensor_scatter_nd_update(seq_time_encoding, idxs_inputs, updates_time_encoding)\n",
    "        seq_target = tf.tensor_scatter_nd_update(seq_items_raw, idxs_target, updates_target)\n",
    "        \n",
    "        # Padding\n",
    "        seq_items, pad_mask = self.pad_sequence(seq_items, maxlen=self.seq_len, return_pad_mask=True, dtype=tf.int64)\n",
    "        seq_type = self.pad_sequence(seq_type, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)\n",
    "        seq_time_encoding = self.pad_sequence(seq_time_encoding, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.float32)  \n",
    "        seq_target = self.pad_sequence(seq_target, maxlen=self.seq_len, return_pad_mask=False, dtype=tf.int64)  \n",
    "\n",
    "        return (seq_items, seq_type, seq_time_encoding), seq_target[:, 0]\n",
    "  \n",
    "  \n",
    "    def set_shapes(self, features, targets=None, session=None):\n",
    "        features[0].set_shape((self.seq_len, 1))\n",
    "        features[1].set_shape((self.seq_len, 1))\n",
    "        features[2].set_shape((self.seq_len, 8))\n",
    "        if self.get_session:\n",
    "            return features, targets, session\n",
    "        return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([32, 10, 1]), TensorShape([32, 10, 1]), TensorShape([32, 10, 8])]\n",
      "[ 863885 1166894  591718       0 1078084       0       0       0       0\n",
      "       0]\n",
      "[1 1 1 1 1 1 0 0 0 0]\n",
      "[     0      0      0 987942      0 432286      0      0      0      0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_paths = ['../tfrecords/tfrecords_v0.3/na_split=train/' + x for x in os.listdir('../tfrecords/tfrecords_v0.3/na_split=train')]\n",
    "\n",
    "dataloader = Bert4RecDataLoader(list_paths, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=10, \n",
    "                                     seq_len_target=None,\n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.4, \n",
    "                                     reverse_prob=0.2, \n",
    "                                     get_session=False,\n",
    "                                     is_val=False,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "# # Train\n",
    "for batch in tqdm(dataloader):\n",
    "    features, target = batch\n",
    "    seq_items, seq_type, seq_time = features\n",
    "    break\n",
    "\n",
    "# # # Test\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, target, session = batch\n",
    "#     seq_items, seq_type, seq_time = features\n",
    "#     break\n",
    "\n",
    "# Val\n",
    "# for batch in tqdm(dataloader):\n",
    "#     features, targets, session = batch\n",
    "#     seq_items, seq_type, seq_time = features\n",
    "#     target, type_target = targets\n",
    "#     break\n",
    "\n",
    "print([x.shape for x in features])\n",
    "\n",
    "idx = 15\n",
    "print(seq_items[idx].numpy().flatten())\n",
    "print(seq_type[idx].numpy().flatten())\n",
    "print(target[idx].numpy().flatten())\n",
    "# print(type_target[idx].numpy().flatten())\n",
    "\n",
    "del features, target, seq_items, seq_type, seq_time\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransposed(tf.keras.layers.Layer):\n",
    "    def __init__(self, tied_to=None, activation=None, **kwargs):\n",
    "        super(EmbeddingTransposed, self).__init__(**kwargs)\n",
    "        self.tied_to = tied_to\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.custom_weights = self.tied_to.weights[0]\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.tied_to.weights[0].shape[0]\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        output = tf.keras.backend.dot(inputs, tf.keras.backend.transpose(self.custom_weights))\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'activation': tf.keras.activations.serialize(self.activation)}\n",
    "        base_config = super(EmbeddingTransposed, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class EncoderTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, attention_axes=None, drop_rate=0.1, att_drop_rate=0.1):\n",
    "        super(EncoderTransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, attention_axes=attention_axes, dropout=att_drop_rate)\n",
    "        self.ffn = tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation='gelu'), \n",
    "             tf.keras.layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-7)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-7)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, query, key, training, attention_mask=None):\n",
    "        attn_output = self.att(query, key, attention_mask=attention_mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(query + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "      \n",
    "                 \n",
    "class ModelBert4Rec(tf.keras.models.Model):\n",
    "    def __init__(self, num_items, model_cfg):\n",
    "        super(ModelBert4Rec, self).__init__()\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        self.num_items = num_items\n",
    "        self.model_cfg = model_cfg\n",
    "        self.embed_items = tf.keras.layers.Embedding(\n",
    "            num_items, model_cfg.emb_dim, \n",
    "            # embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=0.02)\n",
    "        )\n",
    "        self.embed_type = tf.keras.layers.Embedding(3+1, model_cfg.emb_dim)\n",
    "        self.mlp_proj_encoding = tf.keras.models.Sequential([\n",
    "           tf.keras.layers.Dropout(model_cfg.drop_rate), \n",
    "           tf.keras.layers.Dense(model_cfg.trf_dim),\n",
    "           tf.keras.layers.LayerNormalization(epsilon=1e-7)\n",
    "        ])\n",
    "        self.list_transformer_block = [EncoderTransformerBlock(model_cfg.trf_dim, model_cfg.num_heads, \n",
    "                                                               model_cfg.ff_dim, attention_axes=None, \n",
    "                                                               drop_rate=model_cfg.drop_rate, \n",
    "                                                               att_drop_rate=model_cfg.att_drop_rate) \n",
    "                                       for _ in range(model_cfg.num_layers)]\n",
    "        # policy = mixed_precision.Policy('float32')\n",
    "        self.pred_layer = EmbeddingTransposed(tied_to=self.embed_items, activation='linear', dtype='float32')\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        x_seq_past, x_seq_type, x_seq_encoding = inputs\n",
    "        pad_mask = tf.cast(tf.where(tf.equal(x_seq_type, 0), 0, 1), tf.float32)\n",
    "        ###########\n",
    "        x_seq_past_items = self.embed_items(x_seq_past[:, :, 0])\n",
    "        x_seq_past_type = self.embed_type(x_seq_type[:, :, 0])\n",
    "        x_seq_time_encoding = self.mlp_proj_encoding(x_seq_encoding, training=training)\n",
    "        x_ones = tf.ones(tf.shape(x_seq_past_items))\n",
    "        ########### \n",
    "        x = x_seq_past_items * (x_ones + x_seq_time_encoding + x_seq_past_type)\n",
    "        for i in range(len(self.list_transformer_block)):\n",
    "            x = self.list_transformer_block[i](x, x, training=training, attention_mask=pad_mask)\n",
    "        probs = self.pred_layer(x)\n",
    "        return probs\n",
    "      \n",
    "\n",
    "def build_model_bert4Rec(num_items, model_cfg):\n",
    "    return ModelBert4Rec(num_items, model_cfg)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, weight_decay=None):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_decay_tensor = tf.cast(1. if not weight_decay else weight_decay, tf.float32)\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          'd_model': self.d_model,\n",
    "          'warmup_steps': self.warmup_steps,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        if self.weight_decay:\n",
    "            return self.weight_decay_tensor * tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "        else:\n",
    "            return tf.math.rsqrt(self.d_model) * tf.cast(tf.math.minimum(arg1, arg2), tf.float32)\n",
    "    \n",
    "    \n",
    "class ReturnBestEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
    "        elif self.restore_best_weights:\n",
    "            if self.verbose > 0:\n",
    "                print('Restoring model weights from the end of the best epoch.')\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "def custom_loss_bert4rec(tensor_weights=None):\n",
    "    def loss(y_true, y_pred):\n",
    "        mask = tf.where(y_true >= 1, 1., 0.)\n",
    "        ones = tf.ones(tf.shape(y_true))\n",
    "        y_pred = y_pred\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(y_true, y_pred)\n",
    "        if tensor_weights is not None:\n",
    "            weights = tf.gather(params=tensor_weights, indices=y_true)\n",
    "            return tf.reduce_sum(loss * weights * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "        else:\n",
    "            return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-8)\n",
    "    loss.__name__ = f'loss_bert4rec'\n",
    "    return loss\n",
    "\n",
    "\n",
    "def mrr_topk_categorical(top_k):\n",
    "  \"\"\"\n",
    "  Mrr Topk Categorical metric\n",
    "  \"\"\"\n",
    "  def mrr(y_true, y_pred):                                      \n",
    "    n_samples = tf.shape(y_true)[0]\n",
    "    n_samples_mask = tf.where(tf.reduce_sum(y_true, -1) >= 1, 1., 0.)\n",
    "    _, top_index = tf.nn.top_k(y_pred, top_k)  \n",
    "    result = tf.constant(0.0)\n",
    "    top_index = tf.cast(top_index, tf.float32)\n",
    "    idxs_not_masked = tf.cast(tf.argmax(y_true, axis=-1), tf.int32)\n",
    "    for i in tf.range(n_samples):\n",
    "        ranked_indicies = tf.where(tf.equal(top_index[i, idxs_not_masked[i], :], y_true[i, :][:, tf.newaxis]))\n",
    "        if tf.shape(ranked_indicies)[0] > 0:\n",
    "            ranked_indicies = tf.cast(ranked_indicies[0], tf.int32)\n",
    "            #check that the prediction its not padding\n",
    "            if top_index[i, ranked_indicies[0], ranked_indicies[1]] != 0.0: \n",
    "                rr = tf.cast(1/(ranked_indicies[1]+1), tf.float32)\n",
    "            else:\n",
    "                rr = tf.constant(0.0)\n",
    "        else:\n",
    "            rr = tf.constant(0.0)\n",
    "        result+=rr\n",
    "    return result/(tf.reduce_sum(n_samples_mask) + 1e-8)\n",
    "  mrr.__name__ = f'mrr_{top_k}_categorical'\n",
    "  return mrr\n",
    "\n",
    "def recall_top_k(top_k=1):\n",
    "    def recall(y_true, y_pred):\n",
    "        n_samples = tf.shape(y_true)[0]\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), tf.float32)\n",
    "        _, top_index = tf.nn.top_k(y_pred, top_k) \n",
    "        top_index = tf.cast(top_index, tf.float32)\n",
    "        cum_sum = tf.zeros(n_samples)\n",
    "        for i in tf.range(top_k):\n",
    "            indexes_i = top_index[:, :, i]\n",
    "            is_true = tf.reduce_sum(tf.cast(tf.equal(y_true, indexes_i), tf.float32), axis=-1)/tf.reduce_sum(mask, -1)\n",
    "            cum_sum += (is_true/tf.cast(i+1, tf.float32))\n",
    "        return tf.reduce_mean(cum_sum)\n",
    "    recall.__name__ = f'recall_{top_k}'\n",
    "    return recall\n",
    "\n",
    "def create_folder_with_version(base_name, checkpoint_path):\n",
    "    if os.path.exists(os.path.join(checkpoint_path, base_name)):\n",
    "        version_ = base_name.split('_v')\n",
    "        if not version_ or len(version_)==1:\n",
    "            base_name_no_version = base_name\n",
    "            version_ = '_v1'\n",
    "        else:\n",
    "            base_name_no_version = '_'.join(base_name.split('_v')[:-1])\n",
    "            version_ = f'_v{int(version_[-1])+1}'\n",
    "        base_name = base_name_no_version + version_\n",
    "        return create_folder_with_version(base_name, checkpoint_path)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_path, base_name)\n",
    "        os.mkdir(checkpoint_path)\n",
    "        return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_lr = CustomSchedule(128, 10_000, weight_decay=None)\n",
    "plt.plot(tmp_lr(tf.range(12_000_000 // (32 * 4), dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "\n",
    "def flat_gradients(grads_or_idx_slices: tf.Tensor) -> tf.Tensor:\n",
    "    '''Convert gradients if it's tf.IndexedSlices.\n",
    "    When computing gradients for operation concerning `tf.gather`, the type of gradients \n",
    "    '''\n",
    "    if type(grads_or_idx_slices) == tf.IndexedSlices:\n",
    "        return tf.scatter_nd(\n",
    "            tf.expand_dims(grads_or_idx_slices.indices, 1),\n",
    "            grads_or_idx_slices.values,\n",
    "            tf.cast(grads_or_idx_slices.dense_shape, tf.int64)\n",
    "        )\n",
    "    return grads_or_idx_slices\n",
    "\n",
    "def backward_optimization(num_grad_steps, global_gradients, step_gradients, step, model, optimizer):\n",
    "    # for i, g in enumerate(step_gradients):\n",
    "    #     step_gradients[i] += flat_gradients(g) / num_grad_steps\n",
    "    global_gradients = global_gradients + step_gradients\n",
    "    if (step + 1) % num_grad_steps == 0:\n",
    "        global_gradients = zip(global_gradients, model.trainable_variables)\n",
    "        optimizer.apply_gradients(global_gradients)\n",
    "        global_gradients = []\n",
    "    return global_gradients\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(*inputs, target, **kwargs):\n",
    "    l_loss = kwargs['loss']\n",
    "    num_accum_steps = kwargs['num_accum_steps']\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(*inputs, training=True)\n",
    "        loss = loss_function(target, predictions)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss / num_accum_steps)\n",
    "\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "    # gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    l_loss(loss)\n",
    "    return gradients\n",
    "  \n",
    "@tf.function\n",
    "def test_step(*inputs, target, **kwargs):\n",
    "    l_loss = kwargs['loss']\n",
    "    predictions = model(*inputs, training=False)\n",
    "    loss = loss_function(target, predictions)\n",
    "    l_loss(loss)\n",
    "\n",
    "\n",
    "def metrics_reset_states(*metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "\n",
    "def fancy_printer(loss_tracker, epoch, batch_num, start, step='train', dict_metrics={}, num_epochs=1, **kwargs):\n",
    "    dict_print_metrics = {' '.join(f\"{key}:{value:.4f}\" for key, value in dict_metrics.items())}\n",
    "    if step!='epoch':\n",
    "        printer = f'[{step} Epoch]{epoch + 1}/{num_epochs} [Time]{time.time() - start:.2f} [Batch]{batch_num} [Speed]{((time.time() - start)/max(1, batch_num))*1000:.2f}ms/step '\n",
    "        printer += f'[Loss]{loss_tracker.result():.4f} ' + '[Metrics]' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "    else:\n",
    "        train_loss, val_loss = kwargs['train_loss'], kwargs['val_loss']\n",
    "        print(f'\\nTime taken for epoch {epoch+1}/{num_epochs}: {time.time() - start:.2f} secs')\n",
    "        printer = f'[Epoch]{epoch + 1}/{num_epochs} - [Train Loss]{train_loss.result():.4f} '\n",
    "        printer += f'- [Val Loss]{val_loss.result():.4f} ' + str(dict_print_metrics)\n",
    "        print(printer)\n",
    "\n",
    "\n",
    "def log_wandb_metrics(step='train', num_step=0, dict_metrics=None, gradients=None, plot_image=False, **kwargs):\n",
    "    # Scalar metrics\n",
    "    if step=='train' or step=='val':\n",
    "        wandb.log({name : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "    if step=='epoch':\n",
    "        wandb.log({f'epoch_{name}' : value for name, value in dict_metrics.items()}, step=num_step)\n",
    "\n",
    "    # Gradients\n",
    "    if gradients:\n",
    "        wandb.log({\n",
    "            'mean_norm_gradients' : np.mean([tf.norm(x) for x in gradients]), \n",
    "            'max_norm_gradients': np.max([tf.norm(x) for x in gradients])\n",
    "        })\n",
    "\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2060, compute capability 7.5\n",
      "================================================================================\n",
      "Epoch 1\n",
      "[Train Epoch]1/5 [Time]2.24 [Batch]0 [Speed]2238.05ms/step [Loss]14.1112 [Metrics]{'train_loss:14.1112'}\n",
      "[Train Epoch]1/5 [Time]34.07 [Batch]200 [Speed]170.36ms/step [Loss]14.1000 [Metrics]{'train_loss:14.1000'}\n",
      "[Train Epoch]1/5 [Time]65.64 [Batch]400 [Speed]164.09ms/step [Loss]14.0980 [Metrics]{'train_loss:14.0980'}\n",
      "[Train Epoch]1/5 [Time]97.28 [Batch]600 [Speed]162.13ms/step [Loss]14.0946 [Metrics]{'train_loss:14.0946'}\n",
      "[Train Epoch]1/5 [Time]129.52 [Batch]800 [Speed]161.90ms/step [Loss]14.0890 [Metrics]{'train_loss:14.0890'}\n",
      "[Train Epoch]1/5 [Time]162.31 [Batch]1000 [Speed]162.31ms/step [Loss]14.0803 [Metrics]{'train_loss:14.0803'}\n",
      "[Train Epoch]1/5 [Time]194.32 [Batch]1200 [Speed]161.93ms/step [Loss]14.0679 [Metrics]{'train_loss:14.0679'}\n",
      "[Train Epoch]1/5 [Time]226.23 [Batch]1400 [Speed]161.60ms/step [Loss]14.0518 [Metrics]{'train_loss:14.0518'}\n",
      "[Train Epoch]1/5 [Time]258.16 [Batch]1600 [Speed]161.35ms/step [Loss]14.0310 [Metrics]{'train_loss:14.0310'}\n",
      "[Train Epoch]1/5 [Time]290.05 [Batch]1800 [Speed]161.14ms/step [Loss]14.0071 [Metrics]{'train_loss:14.0071'}\n",
      "[Train Epoch]1/5 [Time]321.97 [Batch]2000 [Speed]160.99ms/step [Loss]13.9778 [Metrics]{'train_loss:13.9778'}\n",
      "[Train Epoch]1/5 [Time]353.76 [Batch]2200 [Speed]160.80ms/step [Loss]13.9441 [Metrics]{'train_loss:13.9441'}\n",
      "[Train Epoch]1/5 [Time]385.40 [Batch]2400 [Speed]160.58ms/step [Loss]13.9067 [Metrics]{'train_loss:13.9067'}\n",
      "[Train Epoch]1/5 [Time]416.22 [Batch]2600 [Speed]160.08ms/step [Loss]13.8660 [Metrics]{'train_loss:13.8660'}\n",
      "[Train Epoch]1/5 [Time]446.82 [Batch]2800 [Speed]159.58ms/step [Loss]13.8237 [Metrics]{'train_loss:13.8237'}\n",
      "[Train Epoch]1/5 [Time]477.39 [Batch]3000 [Speed]159.13ms/step [Loss]13.7799 [Metrics]{'train_loss:13.7799'}\n",
      "[Train Epoch]1/5 [Time]507.85 [Batch]3200 [Speed]158.70ms/step [Loss]13.7385 [Metrics]{'train_loss:13.7385'}\n",
      "[Train Epoch]1/5 [Time]538.49 [Batch]3400 [Speed]158.38ms/step [Loss]13.6961 [Metrics]{'train_loss:13.6961'}\n",
      "[Train Epoch]1/5 [Time]569.07 [Batch]3600 [Speed]158.08ms/step [Loss]13.6567 [Metrics]{'train_loss:13.6567'}\n",
      "[Train Epoch]1/5 [Time]599.61 [Batch]3800 [Speed]157.79ms/step [Loss]13.6182 [Metrics]{'train_loss:13.6182'}\n",
      "[Train Epoch]1/5 [Time]630.13 [Batch]4000 [Speed]157.53ms/step [Loss]13.5852 [Metrics]{'train_loss:13.5852'}\n",
      "[Train Epoch]1/5 [Time]660.68 [Batch]4200 [Speed]157.30ms/step [Loss]13.5535 [Metrics]{'train_loss:13.5535'}\n",
      "[Train Epoch]1/5 [Time]691.23 [Batch]4400 [Speed]157.10ms/step [Loss]13.5240 [Metrics]{'train_loss:13.5240'}\n",
      "[Train Epoch]1/5 [Time]721.79 [Batch]4600 [Speed]156.91ms/step [Loss]13.4955 [Metrics]{'train_loss:13.4955'}\n",
      "[Train Epoch]1/5 [Time]752.37 [Batch]4800 [Speed]156.74ms/step [Loss]13.4681 [Metrics]{'train_loss:13.4681'}\n",
      "[Train Epoch]1/5 [Time]782.93 [Batch]5000 [Speed]156.59ms/step [Loss]13.4434 [Metrics]{'train_loss:13.4434'}\n",
      "[Train Epoch]1/5 [Time]813.46 [Batch]5200 [Speed]156.43ms/step [Loss]13.4203 [Metrics]{'train_loss:13.4203'}\n",
      "[Train Epoch]1/5 [Time]844.02 [Batch]5400 [Speed]156.30ms/step [Loss]13.3973 [Metrics]{'train_loss:13.3973'}\n",
      "[Train Epoch]1/5 [Time]874.56 [Batch]5600 [Speed]156.17ms/step [Loss]13.3759 [Metrics]{'train_loss:13.3759'}\n",
      "[Train Epoch]1/5 [Time]905.12 [Batch]5800 [Speed]156.05ms/step [Loss]13.3551 [Metrics]{'train_loss:13.3551'}\n",
      "[Train Epoch]1/5 [Time]937.85 [Batch]6000 [Speed]156.31ms/step [Loss]13.3348 [Metrics]{'train_loss:13.3348'}\n",
      "[Train Epoch]1/5 [Time]969.43 [Batch]6200 [Speed]156.36ms/step [Loss]13.3150 [Metrics]{'train_loss:13.3150'}\n",
      "[Train Epoch]1/5 [Time]1001.19 [Batch]6400 [Speed]156.44ms/step [Loss]13.2969 [Metrics]{'train_loss:13.2969'}\n",
      "[Train Epoch]1/5 [Time]1032.81 [Batch]6600 [Speed]156.49ms/step [Loss]13.2794 [Metrics]{'train_loss:13.2794'}\n",
      "[Train Epoch]1/5 [Time]1064.44 [Batch]6800 [Speed]156.53ms/step [Loss]13.2612 [Metrics]{'train_loss:13.2612'}\n",
      "[Train Epoch]1/5 [Time]1096.10 [Batch]7000 [Speed]156.59ms/step [Loss]13.2449 [Metrics]{'train_loss:13.2449'}\n",
      "[Train Epoch]1/5 [Time]1127.80 [Batch]7200 [Speed]156.64ms/step [Loss]13.2289 [Metrics]{'train_loss:13.2289'}\n",
      "[Train Epoch]1/5 [Time]1159.99 [Batch]7400 [Speed]156.76ms/step [Loss]13.2143 [Metrics]{'train_loss:13.2143'}\n",
      "[Train Epoch]1/5 [Time]1192.13 [Batch]7600 [Speed]156.86ms/step [Loss]13.1991 [Metrics]{'train_loss:13.1991'}\n",
      "[Train Epoch]1/5 [Time]1224.34 [Batch]7800 [Speed]156.97ms/step [Loss]13.1855 [Metrics]{'train_loss:13.1855'}\n",
      "[Train Epoch]1/5 [Time]1256.54 [Batch]8000 [Speed]157.07ms/step [Loss]13.1715 [Metrics]{'train_loss:13.1715'}\n",
      "[Train Epoch]1/5 [Time]1288.43 [Batch]8200 [Speed]157.13ms/step [Loss]13.1579 [Metrics]{'train_loss:13.1579'}\n",
      "[Train Epoch]1/5 [Time]1320.12 [Batch]8400 [Speed]157.16ms/step [Loss]13.1450 [Metrics]{'train_loss:13.1450'}\n",
      "[Train Epoch]1/5 [Time]1351.87 [Batch]8600 [Speed]157.19ms/step [Loss]13.1312 [Metrics]{'train_loss:13.1312'}\n",
      "[Train Epoch]1/5 [Time]1383.64 [Batch]8800 [Speed]157.23ms/step [Loss]13.1194 [Metrics]{'train_loss:13.1194'}\n",
      "[Train Epoch]1/5 [Time]1415.32 [Batch]9000 [Speed]157.26ms/step [Loss]13.1074 [Metrics]{'train_loss:13.1074'}\n",
      "[Train Epoch]1/5 [Time]1447.32 [Batch]9200 [Speed]157.32ms/step [Loss]13.0966 [Metrics]{'train_loss:13.0966'}\n",
      "[Train Epoch]1/5 [Time]1479.36 [Batch]9400 [Speed]157.38ms/step [Loss]13.0855 [Metrics]{'train_loss:13.0855'}\n",
      "[Train Epoch]1/5 [Time]1511.37 [Batch]9600 [Speed]157.43ms/step [Loss]13.0744 [Metrics]{'train_loss:13.0744'}\n",
      "[Train Epoch]1/5 [Time]1543.38 [Batch]9800 [Speed]157.49ms/step [Loss]13.0635 [Metrics]{'train_loss:13.0635'}\n",
      "Saving checkpoint for epoch 1 at step 10000 on path model_bert4rec_complete_0.4.1_v3\n",
      "[Train Epoch]1/5 [Time]1577.22 [Batch]10000 [Speed]157.72ms/step [Loss]13.0530 [Metrics]{'train_loss:13.0530'}\n",
      "[Train Epoch]1/5 [Time]1609.22 [Batch]10200 [Speed]157.77ms/step [Loss]13.0432 [Metrics]{'train_loss:13.0432'}\n",
      "[Train Epoch]1/5 [Time]1641.05 [Batch]10400 [Speed]157.79ms/step [Loss]13.0338 [Metrics]{'train_loss:13.0338'}\n",
      "[Train Epoch]1/5 [Time]1672.82 [Batch]10600 [Speed]157.81ms/step [Loss]13.0247 [Metrics]{'train_loss:13.0247'}\n",
      "[Train Epoch]1/5 [Time]1704.46 [Batch]10800 [Speed]157.82ms/step [Loss]13.0158 [Metrics]{'train_loss:13.0158'}\n",
      "[Train Epoch]1/5 [Time]1736.18 [Batch]11000 [Speed]157.83ms/step [Loss]13.0073 [Metrics]{'train_loss:13.0073'}\n",
      "[Train Epoch]1/5 [Time]1767.81 [Batch]11200 [Speed]157.84ms/step [Loss]12.9990 [Metrics]{'train_loss:12.9990'}\n",
      "[Train Epoch]1/5 [Time]1799.49 [Batch]11400 [Speed]157.85ms/step [Loss]12.9911 [Metrics]{'train_loss:12.9911'}\n",
      "[Train Epoch]1/5 [Time]1831.21 [Batch]11600 [Speed]157.86ms/step [Loss]12.9824 [Metrics]{'train_loss:12.9824'}\n",
      "[Train Epoch]1/5 [Time]1862.97 [Batch]11800 [Speed]157.88ms/step [Loss]12.9741 [Metrics]{'train_loss:12.9741'}\n",
      "[Train Epoch]1/5 [Time]1894.71 [Batch]12000 [Speed]157.89ms/step [Loss]12.9659 [Metrics]{'train_loss:12.9659'}\n",
      "[Train Epoch]1/5 [Time]1926.47 [Batch]12200 [Speed]157.91ms/step [Loss]12.9581 [Metrics]{'train_loss:12.9581'}\n",
      "[Train Epoch]1/5 [Time]1958.37 [Batch]12400 [Speed]157.93ms/step [Loss]12.9500 [Metrics]{'train_loss:12.9500'}\n",
      "[Train Epoch]1/5 [Time]1990.23 [Batch]12600 [Speed]157.95ms/step [Loss]12.9431 [Metrics]{'train_loss:12.9431'}\n",
      "[Train Epoch]1/5 [Time]2021.89 [Batch]12800 [Speed]157.96ms/step [Loss]12.9358 [Metrics]{'train_loss:12.9358'}\n",
      "[Train Epoch]1/5 [Time]2053.50 [Batch]13000 [Speed]157.96ms/step [Loss]12.9291 [Metrics]{'train_loss:12.9291'}\n",
      "[Train Epoch]1/5 [Time]2085.11 [Batch]13200 [Speed]157.96ms/step [Loss]12.9221 [Metrics]{'train_loss:12.9221'}\n",
      "[Train Epoch]1/5 [Time]2116.76 [Batch]13400 [Speed]157.97ms/step [Loss]12.9156 [Metrics]{'train_loss:12.9156'}\n",
      "[Train Epoch]1/5 [Time]2148.42 [Batch]13600 [Speed]157.97ms/step [Loss]12.9086 [Metrics]{'train_loss:12.9086'}\n",
      "[Train Epoch]1/5 [Time]2180.20 [Batch]13800 [Speed]157.99ms/step [Loss]12.9021 [Metrics]{'train_loss:12.9021'}\n",
      "[Train Epoch]1/5 [Time]2211.96 [Batch]14000 [Speed]158.00ms/step [Loss]12.8960 [Metrics]{'train_loss:12.8960'}\n",
      "[Train Epoch]1/5 [Time]2243.73 [Batch]14200 [Speed]158.01ms/step [Loss]12.8896 [Metrics]{'train_loss:12.8896'}\n",
      "[Train Epoch]1/5 [Time]2275.46 [Batch]14400 [Speed]158.02ms/step [Loss]12.8828 [Metrics]{'train_loss:12.8828'}\n",
      "[Train Epoch]1/5 [Time]2307.22 [Batch]14600 [Speed]158.03ms/step [Loss]12.8765 [Metrics]{'train_loss:12.8765'}\n",
      "[Train Epoch]1/5 [Time]2339.00 [Batch]14800 [Speed]158.04ms/step [Loss]12.8704 [Metrics]{'train_loss:12.8704'}\n",
      "[Train Epoch]1/5 [Time]2370.80 [Batch]15000 [Speed]158.05ms/step [Loss]12.8647 [Metrics]{'train_loss:12.8647'}\n",
      "[Train Epoch]1/5 [Time]2402.44 [Batch]15200 [Speed]158.06ms/step [Loss]12.8589 [Metrics]{'train_loss:12.8589'}\n",
      "[Train Epoch]1/5 [Time]2434.06 [Batch]15400 [Speed]158.06ms/step [Loss]12.8537 [Metrics]{'train_loss:12.8537'}\n",
      "[Train Epoch]1/5 [Time]2465.74 [Batch]15600 [Speed]158.06ms/step [Loss]12.8478 [Metrics]{'train_loss:12.8478'}\n",
      "[Train Epoch]1/5 [Time]2497.37 [Batch]15800 [Speed]158.06ms/step [Loss]12.8419 [Metrics]{'train_loss:12.8419'}\n",
      "[Train Epoch]1/5 [Time]2528.98 [Batch]16000 [Speed]158.06ms/step [Loss]12.8363 [Metrics]{'train_loss:12.8363'}\n",
      "[Train Epoch]1/5 [Time]2560.67 [Batch]16200 [Speed]158.07ms/step [Loss]12.8303 [Metrics]{'train_loss:12.8303'}\n",
      "[Train Epoch]1/5 [Time]2592.37 [Batch]16400 [Speed]158.07ms/step [Loss]12.8251 [Metrics]{'train_loss:12.8251'}\n",
      "[Train Epoch]1/5 [Time]2624.26 [Batch]16600 [Speed]158.09ms/step [Loss]12.8199 [Metrics]{'train_loss:12.8199'}\n",
      "[Train Epoch]1/5 [Time]2656.16 [Batch]16800 [Speed]158.10ms/step [Loss]12.8147 [Metrics]{'train_loss:12.8147'}\n",
      "[Train Epoch]1/5 [Time]2688.02 [Batch]17000 [Speed]158.12ms/step [Loss]12.8095 [Metrics]{'train_loss:12.8095'}\n",
      "[Train Epoch]1/5 [Time]2719.91 [Batch]17200 [Speed]158.13ms/step [Loss]12.8043 [Metrics]{'train_loss:12.8043'}\n",
      "[Train Epoch]1/5 [Time]2751.59 [Batch]17400 [Speed]158.14ms/step [Loss]12.7993 [Metrics]{'train_loss:12.7993'}\n",
      "[Train Epoch]1/5 [Time]2783.24 [Batch]17600 [Speed]158.14ms/step [Loss]12.7944 [Metrics]{'train_loss:12.7944'}\n",
      "[Train Epoch]1/5 [Time]2814.90 [Batch]17800 [Speed]158.14ms/step [Loss]12.7891 [Metrics]{'train_loss:12.7891'}\n",
      "[Train Epoch]1/5 [Time]2846.53 [Batch]18000 [Speed]158.14ms/step [Loss]12.7841 [Metrics]{'train_loss:12.7841'}\n",
      "[Train Epoch]1/5 [Time]2878.22 [Batch]18200 [Speed]158.14ms/step [Loss]12.7795 [Metrics]{'train_loss:12.7795'}\n",
      "[Train Epoch]1/5 [Time]2909.97 [Batch]18400 [Speed]158.15ms/step [Loss]12.7750 [Metrics]{'train_loss:12.7750'}\n",
      "[Train Epoch]1/5 [Time]2941.70 [Batch]18600 [Speed]158.16ms/step [Loss]12.7707 [Metrics]{'train_loss:12.7707'}\n",
      "[Train Epoch]1/5 [Time]2973.47 [Batch]18800 [Speed]158.16ms/step [Loss]12.7661 [Metrics]{'train_loss:12.7661'}\n",
      "[Train Epoch]1/5 [Time]3005.32 [Batch]19000 [Speed]158.17ms/step [Loss]12.7610 [Metrics]{'train_loss:12.7610'}\n",
      "[Train Epoch]1/5 [Time]3037.22 [Batch]19200 [Speed]158.19ms/step [Loss]12.7558 [Metrics]{'train_loss:12.7558'}\n",
      "[Train Epoch]1/5 [Time]3069.11 [Batch]19400 [Speed]158.20ms/step [Loss]12.7509 [Metrics]{'train_loss:12.7509'}\n",
      "[Train Epoch]1/5 [Time]3101.01 [Batch]19600 [Speed]158.21ms/step [Loss]12.7458 [Metrics]{'train_loss:12.7458'}\n",
      "[Train Epoch]1/5 [Time]3132.70 [Batch]19800 [Speed]158.22ms/step [Loss]12.7417 [Metrics]{'train_loss:12.7417'}\n",
      "Saving checkpoint for epoch 1 at step 20000 on path model_bert4rec_complete_0.4.1_v3\n",
      "[Train Epoch]1/5 [Time]3166.09 [Batch]20000 [Speed]158.30ms/step [Loss]12.7371 [Metrics]{'train_loss:12.7371'}\n",
      "[Train Epoch]1/5 [Time]3197.82 [Batch]20200 [Speed]158.31ms/step [Loss]12.7326 [Metrics]{'train_loss:12.7326'}\n",
      "[Train Epoch]1/5 [Time]3229.65 [Batch]20400 [Speed]158.32ms/step [Loss]12.7285 [Metrics]{'train_loss:12.7285'}\n",
      "[Train Epoch]1/5 [Time]3261.31 [Batch]20600 [Speed]158.32ms/step [Loss]12.7248 [Metrics]{'train_loss:12.7248'}\n",
      "[Train Epoch]1/5 [Time]3292.97 [Batch]20800 [Speed]158.32ms/step [Loss]12.7202 [Metrics]{'train_loss:12.7202'}\n",
      "[Train Epoch]1/5 [Time]3324.72 [Batch]21000 [Speed]158.32ms/step [Loss]12.7157 [Metrics]{'train_loss:12.7157'}\n",
      "[Train Epoch]1/5 [Time]3356.47 [Batch]21200 [Speed]158.32ms/step [Loss]12.7116 [Metrics]{'train_loss:12.7116'}\n",
      "[Train Epoch]1/5 [Time]3388.20 [Batch]21400 [Speed]158.33ms/step [Loss]12.7066 [Metrics]{'train_loss:12.7066'}\n",
      "[Train Epoch]1/5 [Time]3420.00 [Batch]21600 [Speed]158.33ms/step [Loss]12.7023 [Metrics]{'train_loss:12.7023'}\n",
      "[Train Epoch]1/5 [Time]3451.80 [Batch]21800 [Speed]158.34ms/step [Loss]12.6978 [Metrics]{'train_loss:12.6978'}\n",
      "[Train Epoch]1/5 [Time]3483.42 [Batch]22000 [Speed]158.34ms/step [Loss]12.6936 [Metrics]{'train_loss:12.6936'}\n",
      "[Train Epoch]1/5 [Time]3515.13 [Batch]22200 [Speed]158.34ms/step [Loss]12.6897 [Metrics]{'train_loss:12.6897'}\n",
      "[Train Epoch]1/5 [Time]3546.80 [Batch]22400 [Speed]158.34ms/step [Loss]12.6855 [Metrics]{'train_loss:12.6855'}\n",
      "[Train Epoch]1/5 [Time]3578.47 [Batch]22600 [Speed]158.34ms/step [Loss]12.6812 [Metrics]{'train_loss:12.6812'}\n",
      "[Train Epoch]1/5 [Time]3610.19 [Batch]22800 [Speed]158.34ms/step [Loss]12.6769 [Metrics]{'train_loss:12.6769'}\n",
      "[Train Epoch]1/5 [Time]3641.98 [Batch]23000 [Speed]158.35ms/step [Loss]12.6725 [Metrics]{'train_loss:12.6725'}\n",
      "[Train Epoch]1/5 [Time]3673.90 [Batch]23200 [Speed]158.36ms/step [Loss]12.6681 [Metrics]{'train_loss:12.6681'}\n",
      "[Train Epoch]1/5 [Time]3705.84 [Batch]23400 [Speed]158.37ms/step [Loss]12.6642 [Metrics]{'train_loss:12.6642'}\n",
      "[Train Epoch]1/5 [Time]3737.74 [Batch]23600 [Speed]158.38ms/step [Loss]12.6594 [Metrics]{'train_loss:12.6594'}\n",
      "[Train Epoch]1/5 [Time]3769.66 [Batch]23800 [Speed]158.39ms/step [Loss]12.6548 [Metrics]{'train_loss:12.6548'}\n",
      "[Train Epoch]1/5 [Time]3801.70 [Batch]24000 [Speed]158.40ms/step [Loss]12.6509 [Metrics]{'train_loss:12.6509'}\n",
      "[Train Epoch]1/5 [Time]3833.48 [Batch]24200 [Speed]158.41ms/step [Loss]12.6467 [Metrics]{'train_loss:12.6467'}\n",
      "[Train Epoch]1/5 [Time]3865.72 [Batch]24400 [Speed]158.43ms/step [Loss]12.6427 [Metrics]{'train_loss:12.6427'}\n",
      "[Train Epoch]1/5 [Time]3897.80 [Batch]24600 [Speed]158.45ms/step [Loss]12.6383 [Metrics]{'train_loss:12.6383'}\n",
      "[Train Epoch]1/5 [Time]3928.33 [Batch]24800 [Speed]158.40ms/step [Loss]12.6343 [Metrics]{'train_loss:12.6343'}\n",
      "[Train Epoch]1/5 [Time]3958.85 [Batch]25000 [Speed]158.35ms/step [Loss]12.6300 [Metrics]{'train_loss:12.6300'}\n",
      "[Train Epoch]1/5 [Time]3989.37 [Batch]25200 [Speed]158.31ms/step [Loss]12.6256 [Metrics]{'train_loss:12.6256'}\n",
      "[Train Epoch]1/5 [Time]4019.85 [Batch]25400 [Speed]158.26ms/step [Loss]12.6210 [Metrics]{'train_loss:12.6210'}\n",
      "[Train Epoch]1/5 [Time]4050.33 [Batch]25600 [Speed]158.22ms/step [Loss]12.6166 [Metrics]{'train_loss:12.6166'}\n",
      "[Train Epoch]1/5 [Time]4080.76 [Batch]25800 [Speed]158.17ms/step [Loss]12.6124 [Metrics]{'train_loss:12.6124'}\n",
      "[Train Epoch]1/5 [Time]4111.26 [Batch]26000 [Speed]158.13ms/step [Loss]12.6082 [Metrics]{'train_loss:12.6082'}\n",
      "[Train Epoch]1/5 [Time]4141.76 [Batch]26200 [Speed]158.08ms/step [Loss]12.6047 [Metrics]{'train_loss:12.6047'}\n",
      "[Train Epoch]1/5 [Time]4172.29 [Batch]26400 [Speed]158.04ms/step [Loss]12.6005 [Metrics]{'train_loss:12.6005'}\n",
      "[Train Epoch]1/5 [Time]4202.76 [Batch]26600 [Speed]158.00ms/step [Loss]12.5963 [Metrics]{'train_loss:12.5963'}\n",
      "[Train Epoch]1/5 [Time]4233.30 [Batch]26800 [Speed]157.96ms/step [Loss]12.5922 [Metrics]{'train_loss:12.5922'}\n",
      "[Train Epoch]1/5 [Time]4263.82 [Batch]27000 [Speed]157.92ms/step [Loss]12.5879 [Metrics]{'train_loss:12.5879'}\n",
      "[Train Epoch]1/5 [Time]4294.36 [Batch]27200 [Speed]157.88ms/step [Loss]12.5837 [Metrics]{'train_loss:12.5837'}\n",
      "[Train Epoch]1/5 [Time]4324.84 [Batch]27400 [Speed]157.84ms/step [Loss]12.5795 [Metrics]{'train_loss:12.5795'}\n",
      "[Train Epoch]1/5 [Time]4355.34 [Batch]27600 [Speed]157.80ms/step [Loss]12.5752 [Metrics]{'train_loss:12.5752'}\n",
      "[Train Epoch]1/5 [Time]4385.82 [Batch]27800 [Speed]157.76ms/step [Loss]12.5710 [Metrics]{'train_loss:12.5710'}\n",
      "[Train Epoch]1/5 [Time]4416.35 [Batch]28000 [Speed]157.73ms/step [Loss]12.5669 [Metrics]{'train_loss:12.5669'}\n",
      "[Train Epoch]1/5 [Time]4446.85 [Batch]28200 [Speed]157.69ms/step [Loss]12.5629 [Metrics]{'train_loss:12.5629'}\n",
      "[Train Epoch]1/5 [Time]4477.37 [Batch]28400 [Speed]157.65ms/step [Loss]12.5586 [Metrics]{'train_loss:12.5586'}\n",
      "[Train Epoch]1/5 [Time]4507.89 [Batch]28600 [Speed]157.62ms/step [Loss]12.5544 [Metrics]{'train_loss:12.5544'}\n",
      "[Train Epoch]1/5 [Time]4538.40 [Batch]28800 [Speed]157.58ms/step [Loss]12.5500 [Metrics]{'train_loss:12.5500'}\n",
      "[Train Epoch]1/5 [Time]4568.95 [Batch]29000 [Speed]157.55ms/step [Loss]12.5455 [Metrics]{'train_loss:12.5455'}\n",
      "[Train Epoch]1/5 [Time]4599.43 [Batch]29200 [Speed]157.51ms/step [Loss]12.5412 [Metrics]{'train_loss:12.5412'}\n",
      "[Train Epoch]1/5 [Time]4629.94 [Batch]29400 [Speed]157.48ms/step [Loss]12.5367 [Metrics]{'train_loss:12.5367'}\n",
      "[Train Epoch]1/5 [Time]4660.45 [Batch]29600 [Speed]157.45ms/step [Loss]12.5327 [Metrics]{'train_loss:12.5327'}\n",
      "[Train Epoch]1/5 [Time]4690.98 [Batch]29800 [Speed]157.42ms/step [Loss]12.5283 [Metrics]{'train_loss:12.5283'}\n",
      "Saving checkpoint for epoch 1 at step 30000 on path model_bert4rec_complete_0.4.1_v3\n",
      "[Train Epoch]1/5 [Time]4723.30 [Batch]30000 [Speed]157.44ms/step [Loss]12.5243 [Metrics]{'train_loss:12.5243'}\n",
      "[Train Epoch]1/5 [Time]4753.84 [Batch]30200 [Speed]157.41ms/step [Loss]12.5204 [Metrics]{'train_loss:12.5204'}\n",
      "[Train Epoch]1/5 [Time]4784.36 [Batch]30400 [Speed]157.38ms/step [Loss]12.5162 [Metrics]{'train_loss:12.5162'}\n",
      "[Train Epoch]1/5 [Time]4814.84 [Batch]30600 [Speed]157.35ms/step [Loss]12.5121 [Metrics]{'train_loss:12.5121'}\n",
      "[Train Epoch]1/5 [Time]4845.33 [Batch]30800 [Speed]157.32ms/step [Loss]12.5075 [Metrics]{'train_loss:12.5075'}\n",
      "[Train Epoch]1/5 [Time]4875.81 [Batch]31000 [Speed]157.28ms/step [Loss]12.5034 [Metrics]{'train_loss:12.5034'}\n",
      "[Train Epoch]1/5 [Time]4906.29 [Batch]31200 [Speed]157.25ms/step [Loss]12.4991 [Metrics]{'train_loss:12.4991'}\n",
      "[Train Epoch]1/5 [Time]4936.79 [Batch]31400 [Speed]157.22ms/step [Loss]12.4950 [Metrics]{'train_loss:12.4950'}\n",
      "[Train Epoch]1/5 [Time]4967.28 [Batch]31600 [Speed]157.19ms/step [Loss]12.4908 [Metrics]{'train_loss:12.4908'}\n",
      "[Train Epoch]1/5 [Time]4997.77 [Batch]31800 [Speed]157.16ms/step [Loss]12.4865 [Metrics]{'train_loss:12.4865'}\n",
      "[Train Epoch]1/5 [Time]5028.25 [Batch]32000 [Speed]157.13ms/step [Loss]12.4824 [Metrics]{'train_loss:12.4824'}\n",
      "[Train Epoch]1/5 [Time]5058.73 [Batch]32200 [Speed]157.10ms/step [Loss]12.4783 [Metrics]{'train_loss:12.4783'}\n",
      "[Train Epoch]1/5 [Time]5089.25 [Batch]32400 [Speed]157.08ms/step [Loss]12.4743 [Metrics]{'train_loss:12.4743'}\n",
      "[Train Epoch]1/5 [Time]5119.76 [Batch]32600 [Speed]157.05ms/step [Loss]12.4700 [Metrics]{'train_loss:12.4700'}\n",
      "[Train Epoch]1/5 [Time]5150.31 [Batch]32800 [Speed]157.02ms/step [Loss]12.4661 [Metrics]{'train_loss:12.4661'}\n",
      "[Train Epoch]1/5 [Time]5180.84 [Batch]33000 [Speed]157.00ms/step [Loss]12.4620 [Metrics]{'train_loss:12.4620'}\n",
      "[Train Epoch]1/5 [Time]5211.35 [Batch]33200 [Speed]156.97ms/step [Loss]12.4579 [Metrics]{'train_loss:12.4579'}\n",
      "[Train Epoch]1/5 [Time]5241.83 [Batch]33400 [Speed]156.94ms/step [Loss]12.4538 [Metrics]{'train_loss:12.4538'}\n",
      "[Train Epoch]1/5 [Time]5272.33 [Batch]33600 [Speed]156.91ms/step [Loss]12.4498 [Metrics]{'train_loss:12.4498'}\n",
      "[Train Epoch]1/5 [Time]5302.80 [Batch]33800 [Speed]156.89ms/step [Loss]12.4456 [Metrics]{'train_loss:12.4456'}\n",
      "[Train Epoch]1/5 [Time]5333.29 [Batch]34000 [Speed]156.86ms/step [Loss]12.4417 [Metrics]{'train_loss:12.4417'}\n",
      "[Train Epoch]1/5 [Time]5363.76 [Batch]34200 [Speed]156.84ms/step [Loss]12.4382 [Metrics]{'train_loss:12.4382'}\n",
      "[Train Epoch]1/5 [Time]5394.28 [Batch]34400 [Speed]156.81ms/step [Loss]12.4344 [Metrics]{'train_loss:12.4344'}\n",
      "[Train Epoch]1/5 [Time]5424.82 [Batch]34600 [Speed]156.79ms/step [Loss]12.4306 [Metrics]{'train_loss:12.4306'}\n",
      "[Train Epoch]1/5 [Time]5455.38 [Batch]34800 [Speed]156.76ms/step [Loss]12.4265 [Metrics]{'train_loss:12.4265'}\n",
      "[Train Epoch]1/5 [Time]5485.89 [Batch]35000 [Speed]156.74ms/step [Loss]12.4227 [Metrics]{'train_loss:12.4227'}\n",
      "[Train Epoch]1/5 [Time]5516.42 [Batch]35200 [Speed]156.72ms/step [Loss]12.4184 [Metrics]{'train_loss:12.4184'}\n",
      "[Train Epoch]1/5 [Time]5546.92 [Batch]35400 [Speed]156.69ms/step [Loss]12.4149 [Metrics]{'train_loss:12.4149'}\n",
      "[Train Epoch]1/5 [Time]5577.44 [Batch]35600 [Speed]156.67ms/step [Loss]12.4106 [Metrics]{'train_loss:12.4106'}\n",
      "[Train Epoch]1/5 [Time]5607.97 [Batch]35800 [Speed]156.65ms/step [Loss]12.4064 [Metrics]{'train_loss:12.4064'}\n",
      "[Train Epoch]1/5 [Time]5638.47 [Batch]36000 [Speed]156.62ms/step [Loss]12.4024 [Metrics]{'train_loss:12.4024'}\n",
      "[Train Epoch]1/5 [Time]5668.99 [Batch]36200 [Speed]156.60ms/step [Loss]12.3986 [Metrics]{'train_loss:12.3986'}\n",
      "[Train Epoch]1/5 [Time]5699.54 [Batch]36400 [Speed]156.58ms/step [Loss]12.3947 [Metrics]{'train_loss:12.3947'}\n",
      "[Train Epoch]1/5 [Time]5730.06 [Batch]36600 [Speed]156.56ms/step [Loss]12.3907 [Metrics]{'train_loss:12.3907'}\n",
      "[Train Epoch]1/5 [Time]5760.62 [Batch]36800 [Speed]156.54ms/step [Loss]12.3868 [Metrics]{'train_loss:12.3868'}\n",
      "[Train Epoch]1/5 [Time]5791.17 [Batch]37000 [Speed]156.52ms/step [Loss]12.3825 [Metrics]{'train_loss:12.3825'}\n",
      "[Train Epoch]1/5 [Time]5821.70 [Batch]37200 [Speed]156.50ms/step [Loss]12.3785 [Metrics]{'train_loss:12.3785'}\n",
      "[Train Epoch]1/5 [Time]5852.24 [Batch]37400 [Speed]156.48ms/step [Loss]12.3745 [Metrics]{'train_loss:12.3745'}\n",
      "[Train Epoch]1/5 [Time]5882.77 [Batch]37600 [Speed]156.46ms/step [Loss]12.3706 [Metrics]{'train_loss:12.3706'}\n",
      "[Train Epoch]1/5 [Time]5914.95 [Batch]37800 [Speed]156.48ms/step [Loss]12.3667 [Metrics]{'train_loss:12.3667'}\n",
      "[Train Epoch]1/5 [Time]5947.25 [Batch]38000 [Speed]156.51ms/step [Loss]12.3628 [Metrics]{'train_loss:12.3628'}\n",
      "[Train Epoch]1/5 [Time]5979.50 [Batch]38200 [Speed]156.53ms/step [Loss]12.3591 [Metrics]{'train_loss:12.3591'}\n",
      "[Train Epoch]1/5 [Time]6010.07 [Batch]38400 [Speed]156.51ms/step [Loss]12.3550 [Metrics]{'train_loss:12.3550'}\n",
      "[Train Epoch]1/5 [Time]6040.62 [Batch]38600 [Speed]156.49ms/step [Loss]12.3513 [Metrics]{'train_loss:12.3513'}\n",
      "[Train Epoch]1/5 [Time]6071.19 [Batch]38800 [Speed]156.47ms/step [Loss]12.3477 [Metrics]{'train_loss:12.3477'}\n",
      "[Train Epoch]1/5 [Time]6101.72 [Batch]39000 [Speed]156.45ms/step [Loss]12.3441 [Metrics]{'train_loss:12.3441'}\n",
      "[Train Epoch]1/5 [Time]6132.23 [Batch]39200 [Speed]156.43ms/step [Loss]12.3404 [Metrics]{'train_loss:12.3404'}\n",
      "[Train Epoch]1/5 [Time]6162.78 [Batch]39400 [Speed]156.42ms/step [Loss]12.3366 [Metrics]{'train_loss:12.3366'}\n",
      "[Train Epoch]1/5 [Time]6193.29 [Batch]39600 [Speed]156.40ms/step [Loss]12.3329 [Metrics]{'train_loss:12.3329'}\n",
      "[Train Epoch]1/5 [Time]6223.84 [Batch]39800 [Speed]156.38ms/step [Loss]12.3294 [Metrics]{'train_loss:12.3294'}\n",
      "Saving checkpoint for epoch 1 at step 40000 on path model_bert4rec_complete_0.4.1_v3\n",
      "[Train Epoch]1/5 [Time]6256.26 [Batch]40000 [Speed]156.41ms/step [Loss]12.3257 [Metrics]{'train_loss:12.3257'}\n",
      "[Train Epoch]1/5 [Time]6286.89 [Batch]40200 [Speed]156.39ms/step [Loss]12.3220 [Metrics]{'train_loss:12.3220'}\n",
      "[Train Epoch]1/5 [Time]6317.41 [Batch]40400 [Speed]156.37ms/step [Loss]12.3185 [Metrics]{'train_loss:12.3185'}\n",
      "[Train Epoch]1/5 [Time]6347.98 [Batch]40600 [Speed]156.35ms/step [Loss]12.3147 [Metrics]{'train_loss:12.3147'}\n",
      "[Train Epoch]1/5 [Time]6378.54 [Batch]40800 [Speed]156.34ms/step [Loss]12.3110 [Metrics]{'train_loss:12.3110'}\n",
      "[Train Epoch]1/5 [Time]6409.09 [Batch]41000 [Speed]156.32ms/step [Loss]12.3072 [Metrics]{'train_loss:12.3072'}\n",
      "[Train Epoch]1/5 [Time]6439.62 [Batch]41200 [Speed]156.30ms/step [Loss]12.3031 [Metrics]{'train_loss:12.3031'}\n",
      "[Train Epoch]1/5 [Time]6470.18 [Batch]41400 [Speed]156.28ms/step [Loss]12.2994 [Metrics]{'train_loss:12.2994'}\n",
      "[Train Epoch]1/5 [Time]6500.77 [Batch]41600 [Speed]156.27ms/step [Loss]12.2958 [Metrics]{'train_loss:12.2958'}\n",
      "[Train Epoch]1/5 [Time]6531.29 [Batch]41800 [Speed]156.25ms/step [Loss]12.2918 [Metrics]{'train_loss:12.2918'}\n",
      "[Train Epoch]1/5 [Time]6561.86 [Batch]42000 [Speed]156.23ms/step [Loss]12.2880 [Metrics]{'train_loss:12.2880'}\n",
      "[Train Epoch]1/5 [Time]6592.40 [Batch]42200 [Speed]156.22ms/step [Loss]12.2846 [Metrics]{'train_loss:12.2846'}\n",
      "[Train Epoch]1/5 [Time]6622.96 [Batch]42400 [Speed]156.20ms/step [Loss]12.2807 [Metrics]{'train_loss:12.2807'}\n",
      "[Train Epoch]1/5 [Time]6653.52 [Batch]42600 [Speed]156.19ms/step [Loss]12.2775 [Metrics]{'train_loss:12.2775'}\n",
      "[Train Epoch]1/5 [Time]6684.08 [Batch]42800 [Speed]156.17ms/step [Loss]12.2738 [Metrics]{'train_loss:12.2738'}\n",
      "[Train Epoch]1/5 [Time]6714.63 [Batch]43000 [Speed]156.15ms/step [Loss]12.2703 [Metrics]{'train_loss:12.2703'}\n",
      "[Train Epoch]1/5 [Time]6745.16 [Batch]43200 [Speed]156.14ms/step [Loss]12.2668 [Metrics]{'train_loss:12.2668'}\n",
      "[Train Epoch]1/5 [Time]6775.72 [Batch]43400 [Speed]156.12ms/step [Loss]12.2631 [Metrics]{'train_loss:12.2631'}\n",
      "[Train Epoch]1/5 [Time]6806.31 [Batch]43600 [Speed]156.11ms/step [Loss]12.2599 [Metrics]{'train_loss:12.2599'}\n",
      "[Train Epoch]1/5 [Time]6836.88 [Batch]43800 [Speed]156.09ms/step [Loss]12.2563 [Metrics]{'train_loss:12.2563'}\n",
      "[Train Epoch]1/5 [Time]6867.47 [Batch]44000 [Speed]156.08ms/step [Loss]12.2527 [Metrics]{'train_loss:12.2527'}\n",
      "[Train Epoch]1/5 [Time]6898.03 [Batch]44200 [Speed]156.06ms/step [Loss]12.2490 [Metrics]{'train_loss:12.2490'}\n",
      "[Train Epoch]1/5 [Time]6928.55 [Batch]44400 [Speed]156.05ms/step [Loss]12.2455 [Metrics]{'train_loss:12.2455'}\n",
      "[Train Epoch]1/5 [Time]6959.07 [Batch]44600 [Speed]156.03ms/step [Loss]12.2419 [Metrics]{'train_loss:12.2419'}\n",
      "[Train Epoch]1/5 [Time]6989.58 [Batch]44800 [Speed]156.02ms/step [Loss]12.2385 [Metrics]{'train_loss:12.2385'}\n",
      "[Train Epoch]1/5 [Time]7020.16 [Batch]45000 [Speed]156.00ms/step [Loss]12.2349 [Metrics]{'train_loss:12.2349'}\n",
      "[Train Epoch]1/5 [Time]7052.66 [Batch]45200 [Speed]156.03ms/step [Loss]12.2317 [Metrics]{'train_loss:12.2317'}\n",
      "[Train Epoch]1/5 [Time]7084.32 [Batch]45400 [Speed]156.04ms/step [Loss]12.2283 [Metrics]{'train_loss:12.2283'}\n",
      "[Train Epoch]1/5 [Time]7115.25 [Batch]45600 [Speed]156.04ms/step [Loss]12.2250 [Metrics]{'train_loss:12.2250'}\n",
      "[Train Epoch]1/5 [Time]7145.80 [Batch]45800 [Speed]156.02ms/step [Loss]12.2212 [Metrics]{'train_loss:12.2212'}\n",
      "[Train Epoch]1/5 [Time]7176.37 [Batch]46000 [Speed]156.01ms/step [Loss]12.2180 [Metrics]{'train_loss:12.2180'}\n",
      "[Train Epoch]1/5 [Time]7206.90 [Batch]46200 [Speed]155.99ms/step [Loss]12.2147 [Metrics]{'train_loss:12.2147'}\n",
      "[Train Epoch]1/5 [Time]7237.46 [Batch]46400 [Speed]155.98ms/step [Loss]12.2111 [Metrics]{'train_loss:12.2111'}\n",
      "[Train Epoch]1/5 [Time]7267.98 [Batch]46600 [Speed]155.97ms/step [Loss]12.2076 [Metrics]{'train_loss:12.2076'}\n",
      "[Train Epoch]1/5 [Time]7298.54 [Batch]46800 [Speed]155.95ms/step [Loss]12.2042 [Metrics]{'train_loss:12.2042'}\n",
      "[Train Epoch]1/5 [Time]7329.05 [Batch]47000 [Speed]155.94ms/step [Loss]12.2011 [Metrics]{'train_loss:12.2011'}\n",
      "[Train Epoch]1/5 [Time]7359.60 [Batch]47200 [Speed]155.92ms/step [Loss]12.1976 [Metrics]{'train_loss:12.1976'}\n",
      "[Train Epoch]1/5 [Time]7390.14 [Batch]47400 [Speed]155.91ms/step [Loss]12.1941 [Metrics]{'train_loss:12.1941'}\n",
      "[Train Epoch]1/5 [Time]7420.68 [Batch]47600 [Speed]155.90ms/step [Loss]12.1905 [Metrics]{'train_loss:12.1905'}\n",
      "[Train Epoch]1/5 [Time]7451.19 [Batch]47800 [Speed]155.88ms/step [Loss]12.1871 [Metrics]{'train_loss:12.1871'}\n",
      "[Train Epoch]1/5 [Time]7481.77 [Batch]48000 [Speed]155.87ms/step [Loss]12.1837 [Metrics]{'train_loss:12.1837'}\n",
      "[Train Epoch]1/5 [Time]7512.29 [Batch]48200 [Speed]155.86ms/step [Loss]12.1805 [Metrics]{'train_loss:12.1805'}\n",
      "[Train Epoch]1/5 [Time]7542.84 [Batch]48400 [Speed]155.84ms/step [Loss]12.1776 [Metrics]{'train_loss:12.1776'}\n",
      "[Train Epoch]1/5 [Time]7573.36 [Batch]48600 [Speed]155.83ms/step [Loss]12.1740 [Metrics]{'train_loss:12.1740'}\n",
      "[Train Epoch]1/5 [Time]7603.89 [Batch]48800 [Speed]155.82ms/step [Loss]12.1706 [Metrics]{'train_loss:12.1706'}\n",
      "[Train Epoch]1/5 [Time]7634.44 [Batch]49000 [Speed]155.80ms/step [Loss]12.1674 [Metrics]{'train_loss:12.1674'}\n",
      "[Train Epoch]1/5 [Time]7664.95 [Batch]49200 [Speed]155.79ms/step [Loss]12.1643 [Metrics]{'train_loss:12.1643'}\n",
      "[Train Epoch]1/5 [Time]7695.50 [Batch]49400 [Speed]155.78ms/step [Loss]12.1611 [Metrics]{'train_loss:12.1611'}\n",
      "[Train Epoch]1/5 [Time]7726.03 [Batch]49600 [Speed]155.77ms/step [Loss]12.1577 [Metrics]{'train_loss:12.1577'}\n",
      "[Train Epoch]1/5 [Time]7756.59 [Batch]49800 [Speed]155.75ms/step [Loss]12.1545 [Metrics]{'train_loss:12.1545'}\n",
      "Saving checkpoint for epoch 1 at step 50000 on path model_bert4rec_complete_0.4.1_v3\n",
      "[Train Epoch]1/5 [Time]7788.92 [Batch]50000 [Speed]155.78ms/step [Loss]12.1511 [Metrics]{'train_loss:12.1511'}\n",
      "[Train Epoch]1/5 [Time]7819.48 [Batch]50200 [Speed]155.77ms/step [Loss]12.1479 [Metrics]{'train_loss:12.1479'}\n",
      "[Train Epoch]1/5 [Time]7850.06 [Batch]50400 [Speed]155.76ms/step [Loss]12.1448 [Metrics]{'train_loss:12.1448'}\n",
      "[Train Epoch]1/5 [Time]7880.61 [Batch]50600 [Speed]155.74ms/step [Loss]12.1416 [Metrics]{'train_loss:12.1416'}\n",
      "[Train Epoch]1/5 [Time]7911.19 [Batch]50800 [Speed]155.73ms/step [Loss]12.1384 [Metrics]{'train_loss:12.1384'}\n",
      "[Train Epoch]1/5 [Time]7941.79 [Batch]51000 [Speed]155.72ms/step [Loss]12.1353 [Metrics]{'train_loss:12.1353'}\n",
      "[Train Epoch]1/5 [Time]7972.36 [Batch]51200 [Speed]155.71ms/step [Loss]12.1321 [Metrics]{'train_loss:12.1321'}\n",
      "[Train Epoch]1/5 [Time]8002.93 [Batch]51400 [Speed]155.70ms/step [Loss]12.1289 [Metrics]{'train_loss:12.1289'}\n",
      "[Train Epoch]1/5 [Time]8033.47 [Batch]51600 [Speed]155.69ms/step [Loss]12.1255 [Metrics]{'train_loss:12.1255'}\n",
      "[Train Epoch]1/5 [Time]8064.02 [Batch]51800 [Speed]155.68ms/step [Loss]12.1225 [Metrics]{'train_loss:12.1225'}\n",
      "[Train Epoch]1/5 [Time]8094.59 [Batch]52000 [Speed]155.67ms/step [Loss]12.1196 [Metrics]{'train_loss:12.1196'}\n",
      "[Train Epoch]1/5 [Time]8125.15 [Batch]52200 [Speed]155.65ms/step [Loss]12.1167 [Metrics]{'train_loss:12.1167'}\n",
      "[Train Epoch]1/5 [Time]8155.73 [Batch]52400 [Speed]155.64ms/step [Loss]12.1139 [Metrics]{'train_loss:12.1139'}\n",
      "[Train Epoch]1/5 [Time]8186.32 [Batch]52600 [Speed]155.63ms/step [Loss]12.1110 [Metrics]{'train_loss:12.1110'}\n",
      "[Train Epoch]1/5 [Time]8216.93 [Batch]52800 [Speed]155.62ms/step [Loss]12.1080 [Metrics]{'train_loss:12.1080'}\n",
      "[Train Epoch]1/5 [Time]8247.48 [Batch]53000 [Speed]155.61ms/step [Loss]12.1050 [Metrics]{'train_loss:12.1050'}\n",
      "[Train Epoch]1/5 [Time]8278.04 [Batch]53200 [Speed]155.60ms/step [Loss]12.1018 [Metrics]{'train_loss:12.1018'}\n",
      "[Train Epoch]1/5 [Time]8308.61 [Batch]53400 [Speed]155.59ms/step [Loss]12.0989 [Metrics]{'train_loss:12.0989'}\n",
      "[Train Epoch]1/5 [Time]8339.21 [Batch]53600 [Speed]155.58ms/step [Loss]12.0959 [Metrics]{'train_loss:12.0959'}\n",
      "[Train Epoch]1/5 [Time]8369.75 [Batch]53800 [Speed]155.57ms/step [Loss]12.0929 [Metrics]{'train_loss:12.0929'}\n",
      "[Train Epoch]1/5 [Time]8400.33 [Batch]54000 [Speed]155.56ms/step [Loss]12.0903 [Metrics]{'train_loss:12.0903'}\n",
      "[Train Epoch]1/5 [Time]8430.86 [Batch]54200 [Speed]155.55ms/step [Loss]12.0872 [Metrics]{'train_loss:12.0872'}\n",
      "[Train Epoch]1/5 [Time]8461.41 [Batch]54400 [Speed]155.54ms/step [Loss]12.0842 [Metrics]{'train_loss:12.0842'}\n",
      "[Train Epoch]1/5 [Time]8491.96 [Batch]54600 [Speed]155.53ms/step [Loss]12.0817 [Metrics]{'train_loss:12.0817'}\n",
      "[Train Epoch]1/5 [Time]8522.54 [Batch]54800 [Speed]155.52ms/step [Loss]12.0786 [Metrics]{'train_loss:12.0786'}\n",
      "[Train Epoch]1/5 [Time]8553.13 [Batch]55000 [Speed]155.51ms/step [Loss]12.0755 [Metrics]{'train_loss:12.0755'}\n",
      "[Train Epoch]1/5 [Time]8583.73 [Batch]55200 [Speed]155.50ms/step [Loss]12.0725 [Metrics]{'train_loss:12.0725'}\n",
      "[Train Epoch]1/5 [Time]8614.24 [Batch]55400 [Speed]155.49ms/step [Loss]12.0694 [Metrics]{'train_loss:12.0694'}\n",
      "[Train Epoch]1/5 [Time]8644.79 [Batch]55600 [Speed]155.48ms/step [Loss]12.0665 [Metrics]{'train_loss:12.0665'}\n",
      "[Train Epoch]1/5 [Time]8675.36 [Batch]55800 [Speed]155.47ms/step [Loss]12.0636 [Metrics]{'train_loss:12.0636'}\n",
      "[Train Epoch]1/5 [Time]8705.91 [Batch]56000 [Speed]155.46ms/step [Loss]12.0607 [Metrics]{'train_loss:12.0607'}\n",
      "[Train Epoch]1/5 [Time]8736.51 [Batch]56200 [Speed]155.45ms/step [Loss]12.0579 [Metrics]{'train_loss:12.0579'}\n",
      "[Train Epoch]1/5 [Time]8767.05 [Batch]56400 [Speed]155.44ms/step [Loss]12.0551 [Metrics]{'train_loss:12.0551'}\n",
      "[Train Epoch]1/5 [Time]8797.68 [Batch]56600 [Speed]155.44ms/step [Loss]12.0524 [Metrics]{'train_loss:12.0524'}\n",
      "[Train Epoch]1/5 [Time]8828.24 [Batch]56800 [Speed]155.43ms/step [Loss]12.0496 [Metrics]{'train_loss:12.0496'}\n",
      "[Train Epoch]1/5 [Time]8858.86 [Batch]57000 [Speed]155.42ms/step [Loss]12.0466 [Metrics]{'train_loss:12.0466'}\n",
      "[Train Epoch]1/5 [Time]8889.46 [Batch]57200 [Speed]155.41ms/step [Loss]12.0439 [Metrics]{'train_loss:12.0439'}\n",
      "[Train Epoch]1/5 [Time]8920.07 [Batch]57400 [Speed]155.40ms/step [Loss]12.0413 [Metrics]{'train_loss:12.0413'}\n",
      "[Train Epoch]1/5 [Time]8950.66 [Batch]57600 [Speed]155.39ms/step [Loss]12.0383 [Metrics]{'train_loss:12.0383'}\n",
      "[Train Epoch]1/5 [Time]8981.27 [Batch]57800 [Speed]155.39ms/step [Loss]12.0353 [Metrics]{'train_loss:12.0353'}\n",
      "[Train Epoch]1/5 [Time]9011.85 [Batch]58000 [Speed]155.38ms/step [Loss]12.0325 [Metrics]{'train_loss:12.0325'}\n",
      "[Train Epoch]1/5 [Time]9042.45 [Batch]58200 [Speed]155.37ms/step [Loss]12.0294 [Metrics]{'train_loss:12.0294'}\n",
      "[Train Epoch]1/5 [Time]9073.11 [Batch]58400 [Speed]155.36ms/step [Loss]12.0262 [Metrics]{'train_loss:12.0262'}\n",
      "[Train Epoch]1/5 [Time]9103.73 [Batch]58600 [Speed]155.35ms/step [Loss]12.0236 [Metrics]{'train_loss:12.0236'}\n",
      "[Train Epoch]1/5 [Time]9134.27 [Batch]58800 [Speed]155.34ms/step [Loss]12.0209 [Metrics]{'train_loss:12.0209'}\n",
      "[Train Epoch]1/5 [Time]9164.88 [Batch]59000 [Speed]155.34ms/step [Loss]12.0182 [Metrics]{'train_loss:12.0182'}\n",
      "[Train Epoch]1/5 [Time]9195.47 [Batch]59200 [Speed]155.33ms/step [Loss]12.0156 [Metrics]{'train_loss:12.0156'}\n",
      "[Train Epoch]1/5 [Time]9226.08 [Batch]59400 [Speed]155.32ms/step [Loss]12.0124 [Metrics]{'train_loss:12.0124'}\n",
      "[Train Epoch]1/5 [Time]9256.71 [Batch]59600 [Speed]155.31ms/step [Loss]12.0096 [Metrics]{'train_loss:12.0096'}\n",
      "[Train Epoch]1/5 [Time]9287.31 [Batch]59800 [Speed]155.31ms/step [Loss]12.0071 [Metrics]{'train_loss:12.0071'}\n",
      "Saving checkpoint for epoch 1 at step 60000 on path model_bert4rec_complete_0.4.1_v3\n",
      "[Train Epoch]1/5 [Time]9319.65 [Batch]60000 [Speed]155.33ms/step [Loss]12.0045 [Metrics]{'train_loss:12.0045'}\n",
      "[Train Epoch]1/5 [Time]9350.29 [Batch]60200 [Speed]155.32ms/step [Loss]12.0019 [Metrics]{'train_loss:12.0019'}\n",
      "[Train Epoch]1/5 [Time]9380.89 [Batch]60400 [Speed]155.31ms/step [Loss]11.9990 [Metrics]{'train_loss:11.9990'}\n",
      "[Train Epoch]1/5 [Time]9411.46 [Batch]60600 [Speed]155.30ms/step [Loss]11.9962 [Metrics]{'train_loss:11.9962'}\n",
      "[Train Epoch]1/5 [Time]9442.04 [Batch]60800 [Speed]155.30ms/step [Loss]11.9933 [Metrics]{'train_loss:11.9933'}\n",
      "[Train Epoch]1/5 [Time]9472.62 [Batch]61000 [Speed]155.29ms/step [Loss]11.9906 [Metrics]{'train_loss:11.9906'}\n",
      "[Train Epoch]1/5 [Time]9503.24 [Batch]61200 [Speed]155.28ms/step [Loss]11.9880 [Metrics]{'train_loss:11.9880'}\n",
      "[Train Epoch]1/5 [Time]9533.90 [Batch]61400 [Speed]155.28ms/step [Loss]11.9854 [Metrics]{'train_loss:11.9854'}\n",
      "[Train Epoch]1/5 [Time]9564.53 [Batch]61600 [Speed]155.27ms/step [Loss]11.9827 [Metrics]{'train_loss:11.9827'}\n",
      "[Train Epoch]1/5 [Time]9595.19 [Batch]61800 [Speed]155.26ms/step [Loss]11.9801 [Metrics]{'train_loss:11.9801'}\n",
      "[Train Epoch]1/5 [Time]9625.86 [Batch]62000 [Speed]155.26ms/step [Loss]11.9776 [Metrics]{'train_loss:11.9776'}\n",
      "[Train Epoch]1/5 [Time]9656.46 [Batch]62200 [Speed]155.25ms/step [Loss]11.9749 [Metrics]{'train_loss:11.9749'}\n",
      "[Train Epoch]1/5 [Time]9687.11 [Batch]62400 [Speed]155.24ms/step [Loss]11.9724 [Metrics]{'train_loss:11.9724'}\n",
      "[Train Epoch]1/5 [Time]9717.65 [Batch]62600 [Speed]155.23ms/step [Loss]11.9698 [Metrics]{'train_loss:11.9698'}\n",
      "[Train Epoch]1/5 [Time]9748.22 [Batch]62800 [Speed]155.23ms/step [Loss]11.9670 [Metrics]{'train_loss:11.9670'}\n",
      "[Train Epoch]1/5 [Time]9778.84 [Batch]63000 [Speed]155.22ms/step [Loss]11.9644 [Metrics]{'train_loss:11.9644'}\n",
      "[Train Epoch]1/5 [Time]9809.46 [Batch]63200 [Speed]155.21ms/step [Loss]11.9617 [Metrics]{'train_loss:11.9617'}\n",
      "[Train Epoch]1/5 [Time]9840.06 [Batch]63400 [Speed]155.21ms/step [Loss]11.9589 [Metrics]{'train_loss:11.9589'}\n",
      "[Train Epoch]1/5 [Time]9870.65 [Batch]63600 [Speed]155.20ms/step [Loss]11.9562 [Metrics]{'train_loss:11.9562'}\n",
      "[Train Epoch]1/5 [Time]9901.23 [Batch]63800 [Speed]155.19ms/step [Loss]11.9536 [Metrics]{'train_loss:11.9536'}\n",
      "[Train Epoch]1/5 [Time]9931.78 [Batch]64000 [Speed]155.18ms/step [Loss]11.9511 [Metrics]{'train_loss:11.9511'}\n",
      "[Train Epoch]1/5 [Time]9962.37 [Batch]64200 [Speed]155.18ms/step [Loss]11.9485 [Metrics]{'train_loss:11.9485'}\n",
      "[Train Epoch]1/5 [Time]9993.00 [Batch]64400 [Speed]155.17ms/step [Loss]11.9460 [Metrics]{'train_loss:11.9460'}\n",
      "[Train Epoch]1/5 [Time]10023.56 [Batch]64600 [Speed]155.16ms/step [Loss]11.9436 [Metrics]{'train_loss:11.9436'}\n",
      "[Train Epoch]1/5 [Time]10054.21 [Batch]64800 [Speed]155.16ms/step [Loss]11.9412 [Metrics]{'train_loss:11.9412'}\n",
      "[Train Epoch]1/5 [Time]10084.82 [Batch]65000 [Speed]155.15ms/step [Loss]11.9386 [Metrics]{'train_loss:11.9386'}\n",
      "[Train Epoch]1/5 [Time]10115.42 [Batch]65200 [Speed]155.14ms/step [Loss]11.9362 [Metrics]{'train_loss:11.9362'}\n",
      "[Train Epoch]1/5 [Time]10146.04 [Batch]65400 [Speed]155.14ms/step [Loss]11.9338 [Metrics]{'train_loss:11.9338'}\n",
      "[Train Epoch]1/5 [Time]10176.66 [Batch]65600 [Speed]155.13ms/step [Loss]11.9311 [Metrics]{'train_loss:11.9311'}\n",
      "[Train Epoch]1/5 [Time]10207.29 [Batch]65800 [Speed]155.13ms/step [Loss]11.9286 [Metrics]{'train_loss:11.9286'}\n",
      "[Train Epoch]1/5 [Time]10237.90 [Batch]66000 [Speed]155.12ms/step [Loss]11.9262 [Metrics]{'train_loss:11.9262'}\n",
      "[Train Epoch]1/5 [Time]10268.52 [Batch]66200 [Speed]155.11ms/step [Loss]11.9235 [Metrics]{'train_loss:11.9235'}\n",
      "[Train Epoch]1/5 [Time]10299.13 [Batch]66400 [Speed]155.11ms/step [Loss]11.9211 [Metrics]{'train_loss:11.9211'}\n",
      "[Train Epoch]1/5 [Time]10329.74 [Batch]66600 [Speed]155.10ms/step [Loss]11.9188 [Metrics]{'train_loss:11.9188'}\n",
      "[Train Epoch]1/5 [Time]10360.36 [Batch]66800 [Speed]155.10ms/step [Loss]11.9162 [Metrics]{'train_loss:11.9162'}\n",
      "[Train Epoch]1/5 [Time]10390.94 [Batch]67000 [Speed]155.09ms/step [Loss]11.9137 [Metrics]{'train_loss:11.9137'}\n",
      "[Train Epoch]1/5 [Time]10421.59 [Batch]67200 [Speed]155.08ms/step [Loss]11.9114 [Metrics]{'train_loss:11.9114'}\n",
      "[Train Epoch]1/5 [Time]10452.20 [Batch]67400 [Speed]155.08ms/step [Loss]11.9090 [Metrics]{'train_loss:11.9090'}\n",
      "[Train Epoch]1/5 [Time]10482.82 [Batch]67600 [Speed]155.07ms/step [Loss]11.9067 [Metrics]{'train_loss:11.9067'}\n",
      "[Train Epoch]1/5 [Time]10513.43 [Batch]67800 [Speed]155.07ms/step [Loss]11.9043 [Metrics]{'train_loss:11.9043'}\n",
      "[Train Epoch]1/5 [Time]10544.06 [Batch]68000 [Speed]155.06ms/step [Loss]11.9019 [Metrics]{'train_loss:11.9019'}\n",
      "[Train Epoch]1/5 [Time]10574.70 [Batch]68200 [Speed]155.05ms/step [Loss]11.8996 [Metrics]{'train_loss:11.8996'}\n",
      "[Train Epoch]1/5 [Time]10605.32 [Batch]68400 [Speed]155.05ms/step [Loss]11.8971 [Metrics]{'train_loss:11.8971'}\n",
      "[Train Epoch]1/5 [Time]10635.92 [Batch]68600 [Speed]155.04ms/step [Loss]11.8947 [Metrics]{'train_loss:11.8947'}\n",
      "[Train Epoch]1/5 [Time]10666.59 [Batch]68800 [Speed]155.04ms/step [Loss]11.8922 [Metrics]{'train_loss:11.8922'}\n",
      "[Train Epoch]1/5 [Time]10697.20 [Batch]69000 [Speed]155.03ms/step [Loss]11.8897 [Metrics]{'train_loss:11.8897'}\n",
      "[Train Epoch]1/5 [Time]10727.81 [Batch]69200 [Speed]155.03ms/step [Loss]11.8874 [Metrics]{'train_loss:11.8874'}\n",
      "[Train Epoch]1/5 [Time]10758.41 [Batch]69400 [Speed]155.02ms/step [Loss]11.8851 [Metrics]{'train_loss:11.8851'}\n",
      "[Train Epoch]1/5 [Time]10789.00 [Batch]69600 [Speed]155.01ms/step [Loss]11.8827 [Metrics]{'train_loss:11.8827'}\n",
      "[Train Epoch]1/5 [Time]10819.62 [Batch]69800 [Speed]155.01ms/step [Loss]11.8804 [Metrics]{'train_loss:11.8804'}\n",
      "Saving checkpoint for epoch 1 at step 70000 on path model_bert4rec_complete_0.4.1_v3\n",
      "[Train Epoch]1/5 [Time]10852.05 [Batch]70000 [Speed]155.03ms/step [Loss]11.8781 [Metrics]{'train_loss:11.8781'}\n",
      "[Train Epoch]1/5 [Time]10882.70 [Batch]70200 [Speed]155.02ms/step [Loss]11.8757 [Metrics]{'train_loss:11.8757'}\n",
      "[Train Epoch]1/5 [Time]10913.36 [Batch]70400 [Speed]155.02ms/step [Loss]11.8735 [Metrics]{'train_loss:11.8735'}\n",
      "[Train Epoch]1/5 [Time]10944.02 [Batch]70600 [Speed]155.01ms/step [Loss]11.8712 [Metrics]{'train_loss:11.8712'}\n",
      "[Train Epoch]1/5 [Time]10974.62 [Batch]70800 [Speed]155.01ms/step [Loss]11.8689 [Metrics]{'train_loss:11.8689'}\n",
      "[Train Epoch]1/5 [Time]11005.22 [Batch]71000 [Speed]155.00ms/step [Loss]11.8664 [Metrics]{'train_loss:11.8664'}\n",
      "[Train Epoch]1/5 [Time]11035.78 [Batch]71200 [Speed]155.00ms/step [Loss]11.8641 [Metrics]{'train_loss:11.8641'}\n",
      "[Train Epoch]1/5 [Time]11066.40 [Batch]71400 [Speed]154.99ms/step [Loss]11.8618 [Metrics]{'train_loss:11.8618'}\n",
      "[Train Epoch]1/5 [Time]11097.00 [Batch]71600 [Speed]154.99ms/step [Loss]11.8597 [Metrics]{'train_loss:11.8597'}\n",
      "[Train Epoch]1/5 [Time]11127.56 [Batch]71800 [Speed]154.98ms/step [Loss]11.8573 [Metrics]{'train_loss:11.8573'}\n",
      "[Train Epoch]1/5 [Time]11158.18 [Batch]72000 [Speed]154.97ms/step [Loss]11.8552 [Metrics]{'train_loss:11.8552'}\n",
      "[Train Epoch]1/5 [Time]11188.75 [Batch]72200 [Speed]154.97ms/step [Loss]11.8528 [Metrics]{'train_loss:11.8528'}\n",
      "[Train Epoch]1/5 [Time]11219.29 [Batch]72400 [Speed]154.96ms/step [Loss]11.8504 [Metrics]{'train_loss:11.8504'}\n",
      "[Train Epoch]1/5 [Time]11249.90 [Batch]72600 [Speed]154.96ms/step [Loss]11.8481 [Metrics]{'train_loss:11.8481'}\n",
      "[Train Epoch]1/5 [Time]11280.50 [Batch]72800 [Speed]154.95ms/step [Loss]11.8459 [Metrics]{'train_loss:11.8459'}\n",
      "[Train Epoch]1/5 [Time]11311.13 [Batch]73000 [Speed]154.95ms/step [Loss]11.8436 [Metrics]{'train_loss:11.8436'}\n",
      "[Train Epoch]1/5 [Time]11341.77 [Batch]73200 [Speed]154.94ms/step [Loss]11.8412 [Metrics]{'train_loss:11.8412'}\n",
      "[Train Epoch]1/5 [Time]11372.39 [Batch]73400 [Speed]154.94ms/step [Loss]11.8390 [Metrics]{'train_loss:11.8390'}\n",
      "[Train Epoch]1/5 [Time]11402.97 [Batch]73600 [Speed]154.93ms/step [Loss]11.8368 [Metrics]{'train_loss:11.8368'}\n",
      "[Train Epoch]1/5 [Time]11433.59 [Batch]73800 [Speed]154.93ms/step [Loss]11.8346 [Metrics]{'train_loss:11.8346'}\n",
      "[Train Epoch]1/5 [Time]11464.15 [Batch]74000 [Speed]154.92ms/step [Loss]11.8325 [Metrics]{'train_loss:11.8325'}\n",
      "[Train Epoch]1/5 [Time]11494.77 [Batch]74200 [Speed]154.92ms/step [Loss]11.8302 [Metrics]{'train_loss:11.8302'}\n",
      "[Train Epoch]1/5 [Time]11525.40 [Batch]74400 [Speed]154.91ms/step [Loss]11.8281 [Metrics]{'train_loss:11.8281'}\n",
      "[Train Epoch]1/5 [Time]11556.03 [Batch]74600 [Speed]154.91ms/step [Loss]11.8258 [Metrics]{'train_loss:11.8258'}\n",
      "[Train Epoch]1/5 [Time]11586.66 [Batch]74800 [Speed]154.90ms/step [Loss]11.8236 [Metrics]{'train_loss:11.8236'}\n",
      "[Train Epoch]1/5 [Time]11617.26 [Batch]75000 [Speed]154.90ms/step [Loss]11.8214 [Metrics]{'train_loss:11.8214'}\n",
      "[Train Epoch]1/5 [Time]11647.88 [Batch]75200 [Speed]154.89ms/step [Loss]11.8191 [Metrics]{'train_loss:11.8191'}\n",
      "[Train Epoch]1/5 [Time]11678.51 [Batch]75400 [Speed]154.89ms/step [Loss]11.8169 [Metrics]{'train_loss:11.8169'}\n",
      "[Train Epoch]1/5 [Time]11709.12 [Batch]75600 [Speed]154.88ms/step [Loss]11.8147 [Metrics]{'train_loss:11.8147'}\n",
      "[Train Epoch]1/5 [Time]11739.76 [Batch]75800 [Speed]154.88ms/step [Loss]11.8125 [Metrics]{'train_loss:11.8125'}\n",
      "[Train Epoch]1/5 [Time]11770.37 [Batch]76000 [Speed]154.87ms/step [Loss]11.8105 [Metrics]{'train_loss:11.8105'}\n",
      "[Train Epoch]1/5 [Time]11800.98 [Batch]76200 [Speed]154.87ms/step [Loss]11.8083 [Metrics]{'train_loss:11.8083'}\n",
      "[Train Epoch]1/5 [Time]11831.59 [Batch]76400 [Speed]154.86ms/step [Loss]11.8060 [Metrics]{'train_loss:11.8060'}\n",
      "[Train Epoch]1/5 [Time]11862.18 [Batch]76600 [Speed]154.86ms/step [Loss]11.8040 [Metrics]{'train_loss:11.8040'}\n",
      "[Train Epoch]1/5 [Time]11892.77 [Batch]76800 [Speed]154.85ms/step [Loss]11.8019 [Metrics]{'train_loss:11.8019'}\n",
      "[Train Epoch]1/5 [Time]11923.41 [Batch]77000 [Speed]154.85ms/step [Loss]11.7997 [Metrics]{'train_loss:11.7997'}\n",
      "[Train Epoch]1/5 [Time]11954.05 [Batch]77200 [Speed]154.85ms/step [Loss]11.7976 [Metrics]{'train_loss:11.7976'}\n",
      "[Train Epoch]1/5 [Time]11984.70 [Batch]77400 [Speed]154.84ms/step [Loss]11.7956 [Metrics]{'train_loss:11.7956'}\n",
      "[Train Epoch]1/5 [Time]12015.37 [Batch]77600 [Speed]154.84ms/step [Loss]11.7936 [Metrics]{'train_loss:11.7936'}\n",
      "[Train Epoch]1/5 [Time]12045.98 [Batch]77800 [Speed]154.83ms/step [Loss]11.7914 [Metrics]{'train_loss:11.7914'}\n",
      "[Train Epoch]1/5 [Time]12077.49 [Batch]78000 [Speed]154.84ms/step [Loss]11.7894 [Metrics]{'train_loss:11.7894'}\n",
      "[Train Epoch]1/5 [Time]12109.55 [Batch]78200 [Speed]154.85ms/step [Loss]11.7874 [Metrics]{'train_loss:11.7874'}\n",
      "[Train Epoch]1/5 [Time]12141.35 [Batch]78400 [Speed]154.86ms/step [Loss]11.7852 [Metrics]{'train_loss:11.7852'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 100\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m batch_num, batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     99\u001b[0m     inputs, target \u001b[39m=\u001b[39m batch_data\n\u001b[1;32m--> 100\u001b[0m     step_gradients \u001b[39m=\u001b[39m train_step(inputs, target\u001b[39m=\u001b[39;49mtarget, loss\u001b[39m=\u001b[39;49mtrain_loss, num_accum_steps\u001b[39m=\u001b[39;49mBERT4REC_CONFIG\u001b[39m.\u001b[39;49mnum_grad_accum_steps)\n\u001b[0;32m    101\u001b[0m     global_gradients \u001b[39m=\u001b[39m backward_optimization(BERT4REC_CONFIG\u001b[39m.\u001b[39mnum_grad_accum_steps, global_gradients, step_gradients, total_step, model, optimizer)\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m batch_num \u001b[39m%\u001b[39m BERT4REC_CONFIG\u001b[39m.\u001b[39mbatch_num_printer_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Enric\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "\n",
    "class BERT4REC_CONFIG:\n",
    "    num_items = NUM_ITEMS\n",
    "    path_tfrecords = '../tfrecords/tfrecords_v0.3/'\n",
    "    restore_last_chekpoint = (False, 'model_bert4rec_complete_v0.4_finetuned')\n",
    "    model_name = 'model_bert4rec_complete_0.4.1'\n",
    "    checkpoint_filepath = f'../2_Models/'\n",
    "    num_records_dataset = 12_000_000\n",
    "    batch_size = 32\n",
    "    num_grad_accum_steps = 2\n",
    "    seq_len = 10\n",
    "    mask_prob = 0.35\n",
    "    reverse_prob = 0.25\n",
    "    emb_dim = 32\n",
    "    trf_dim = 32\n",
    "    num_heads = 2\n",
    "    num_layers = 1\n",
    "    ff_dim = trf_dim*4\n",
    "    drop_rate = 0.1\n",
    "    att_drop_rate = 0.1\n",
    "    epochs = 5\n",
    "    early_stopping = 5\n",
    "    batch_num_printer_train = 200\n",
    "    batch_num_printer_val = 100\n",
    "    clipnorm = 1\n",
    "    num_iters_save_checkpoint = 10_000\n",
    "    scheduler_scaler = 128 \n",
    "    warmup_steps = 10_000\n",
    "    log_wandb = False\n",
    "    \n",
    "\n",
    "list_paths_train = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=train/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=train')]\n",
    "np.random.shuffle(list_paths_train)\n",
    "list_paths_val = [f'{BERT4REC_CONFIG.path_tfrecords}na_split=val/' + x for x in os.listdir(f'{BERT4REC_CONFIG.path_tfrecords}na_split=val')]\n",
    "\n",
    "train_dataloader = Bert4RecDataLoader(list_paths_train, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len, \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=BERT4REC_CONFIG.mask_prob, \n",
    "                                     reverse_prob=BERT4REC_CONFIG.reverse_prob, \n",
    "                                     is_test=False,\n",
    "                                     is_val=False,\n",
    "                                     shuffle=True,\n",
    "                                     drop_remainder=True).get_generator()\n",
    "\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=BERT4REC_CONFIG.num_items, \n",
    "                                     seq_len=BERT4REC_CONFIG.seq_len,  \n",
    "                                     batch_size=BERT4REC_CONFIG.batch_size, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     get_session=False,\n",
    "                                     is_val=True,\n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = build_model_bert4Rec(num_items=BERT4REC_CONFIG.num_items, model_cfg=BERT4REC_CONFIG)\n",
    "# model = tf.keras.models.load_model(f'../2_Models/seq_len{BERT4REC_CONFIG.seq_len}_{BERT4REC_CONFIG.restore_last_chekpoint[1]}/', compile=False)\n",
    "optimizer = optimizers.Adam(learning_rate=CustomSchedule(BERT4REC_CONFIG.scheduler_scaler, \n",
    "                            warmup_steps=BERT4REC_CONFIG.warmup_steps),\n",
    "                            clipnorm=BERT4REC_CONFIG.clipnorm)\n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)                           \n",
    "\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "                            \n",
    "# Build utils\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "checkpoint_path = create_folder_with_version(BERT4REC_CONFIG.model_name, BERT4REC_CONFIG.checkpoint_filepath)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, os.path.join(BERT4REC_CONFIG.checkpoint_filepath, checkpoint_path, 'checkpoints'), \n",
    "                                          max_to_keep=10)\n",
    "if BERT4REC_CONFIG.restore_last_chekpoint[0]:\n",
    "    ckpt.restore(BERT4REC_CONFIG.restore_last_chekpoint[1])\n",
    "    print('Latest checkpoint restored!!')\n",
    "\n",
    "# Loss function\n",
    "loss_function = custom_loss_bert4rec()\n",
    "\n",
    "# Trackers\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "##############################################\n",
    "\n",
    "total_step, val_step = 0, 0\n",
    "global_gradients = []\n",
    "for epoch in range(BERT4REC_CONFIG.epochs):\n",
    "    start = time.time()\n",
    "    print('===='*20)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    metrics_reset_states(train_loss, val_loss)\n",
    "    \n",
    "    for batch_num, batch_data in enumerate(train_dataloader):\n",
    "        inputs, target = batch_data\n",
    "        step_gradients = train_step(inputs, target=target, loss=train_loss, num_accum_steps=BERT4REC_CONFIG.num_grad_accum_steps)\n",
    "        global_gradients = backward_optimization(BERT4REC_CONFIG.num_grad_accum_steps, global_gradients, step_gradients, total_step, model, optimizer)\n",
    "        if batch_num % BERT4REC_CONFIG.batch_num_printer_train == 0:\n",
    "            train_dict_metrics = {x.name : x.result() for x in [train_loss]}\n",
    "            fancy_printer(train_loss, epoch, batch_num, start, step='Train', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=train_dict_metrics)\n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                log_wandb_metrics(step='train', num_step=total_step, gradients=global_gradients, dict_metrics=train_dict_metrics) \n",
    "        \n",
    "        total_step += 1  \n",
    "\n",
    "        if total_step % BERT4REC_CONFIG.num_iters_save_checkpoint==0:\n",
    "            print(f'Saving checkpoint for epoch {epoch+1} at step {total_step} on path {checkpoint_path}')        \n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            \n",
    "    for val_batch_num, val_batch_data in enumerate(val_dataloader):\n",
    "        inputs, target = val_batch_data\n",
    "        test_step(inputs, target=target, loss=val_loss)\n",
    "        val_step += 1\n",
    "        if val_batch_num % BERT4REC_CONFIG.batch_num_printer_val == 0:\n",
    "            val_dict_metrics = {x.name : x.result() for x in [val_loss]}\n",
    "            fancy_printer(val_loss, epoch, val_batch_num, start, step='Val', num_epochs=BERT4REC_CONFIG.epochs, dict_metrics=val_dict_metrics)    \n",
    "            if BERT4REC_CONFIG.log_wandb:\n",
    "                log_wandb_metrics(step='val', num_step=val_step, dict_metrics=val_dict_metrics) \n",
    "                # if val_batch_num==0:\n",
    "                #     log_wandb_metrics(step=None, plot_image=True, \n",
    "                #                       model=model, inputs=inputs, epoch=epoch, target=target, stats=stats)\n",
    "\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {checkpoint_path}')        \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    \n",
    "    epoch_dict_metrics = {x.name : x.result() for x in [train_loss, val_loss]}\n",
    "    printer = fancy_printer(None, epoch, epoch, start, step='epoch', dict_metrics=epoch_dict_metrics, \n",
    "                            train_loss=train_loss, val_loss=val_loss)\n",
    "    if BERT4REC_CONFIG.log_wandb:\n",
    "        log_wandb_metrics(step='epoch', num_step=total_step, dict_metrics=epoch_dict_metrics)\n",
    "\n",
    "if BERT4REC_CONFIG.log_wandb:\n",
    "    # wandb.save(checkpoint_path)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [02:34,  6.48it/s]\n",
      "100%|██████████| 96096/96096 [00:01<00:00, 72908.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.609600e+04</td>\n",
       "      <td>40454.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.358616e+06</td>\n",
       "      <td>0.117692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.721860e+06</td>\n",
       "      <td>0.316083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.200000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.118926e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.336554e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.541742e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.289966e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            session         score\n",
       "count  9.609600e+04  40454.000000\n",
       "mean   6.358616e+06      0.117692\n",
       "std    3.721860e+06      0.316083\n",
       "min    2.200000e+02      0.000000\n",
       "25%    3.118926e+06      0.000000\n",
       "50%    6.336554e+06      0.000000\n",
       "75%    9.541742e+06      0.000000\n",
       "max    1.289966e+07      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'carts': 0.14735728765090225,\n",
       " 'clicks': 0.10318137094129223,\n",
       " 'orders': 0.19717041401234073}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Metric: 0.1728\n"
     ]
    }
   ],
   "source": [
    "def get_score_metric(y_true, y_pred, type_target, k=20):\n",
    "    score = 0\n",
    "    if len(y_true)==0:\n",
    "        return None\n",
    "    if type_target=='clicks':\n",
    "        num_targets = 1\n",
    "        hits = len([x for x in y_pred if x==y_true[0]])\n",
    "    else:\n",
    "        num_targets = min(k, len(y_true))\n",
    "        hits = len([x for x in y_pred if x in y_true])\n",
    "    score = hits / num_targets\n",
    "    return score\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "# model = models.load_model('../2_Models/seq_len10_model_bert4rec_complete_v0.4_finetuned/', compile=False)\n",
    "list_paths_val = ['../tfrecords/tfrecords_v0.3/na_split=val/' + x for x in os.listdir('../tfrecords/tfrecords_v0.3/na_split=val')]\n",
    "val_dataloader = Bert4RecDataLoader(list_paths_val, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=10, \n",
    "                                     seq_len_target=20, \n",
    "                                     batch_size=32, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0, \n",
    "                                     is_val=True,\n",
    "                                     get_session=True, \n",
    "                                     is_test=False,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "\n",
    "list_sessions, list_predictions, list_trues, list_types = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(val_dataloader)):\n",
    "    features, targets, session = batch\n",
    "    seq_items, seq_type, seq_time = features\n",
    "    target, type_target = targets\n",
    "    idxs = tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[x for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        labels = [list(set([_target for _type, _target in zip(type_target.numpy()[i], target.numpy()[i]) if dict_map_type[type_]==_type and _target!=0])) for i in range(target.shape[0])]\n",
    "        ###\n",
    "        list_sessions.append(session.numpy())\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_trues = list_trues + labels\n",
    "    if num_batch==1_000:\n",
    "        break\n",
    "\n",
    "df_val = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'trues' : list_trues,\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_val['score'] = df_val.progress_apply(lambda x: get_score_metric(x['trues'], x['predictions'], x['type']), axis=1)\n",
    "\n",
    "display(df_val.describe())\n",
    "dict_scores = df_val.groupby('type')['score'].mean().to_dict()\n",
    "display(dict_scores)\n",
    "kaggle_metric = 0.1*dict_scores['clicks'] + 0.3*dict_scores['carts'] + 0.6*dict_scores['orders']\n",
    "print(f'Kaggle Metric: {kaggle_metric:.4f}')\n",
    "\n",
    "# v0.4 seqlen=10\n",
    "# {'carts': 0.222143,\n",
    "#  'clicks': 0.163726,\n",
    "#  'orders': 0.301489}\n",
    "# Kaggle Metric: 0.2639089\n",
    "\n",
    "# v0.4_finetuned seqlen=10\n",
    "# {'carts': 0.23272587826464677,\n",
    "#  'clicks': 0.16818629058707774,\n",
    "#  'orders': 0.31957377011651095}\n",
    "# Kaggle Metric: 0.2783808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "tf.keras.backend.clear_session()\n",
    "model = models.load_model('../2_Models/seq_len10_model_bert4rec_complete_v0.4/', compile=False)\n",
    "\n",
    "\n",
    "list_paths_test = ['../tfrecords/tfrecords_v0.3/na_split=test/' + x for x in os.listdir('../tfrecords/tfrecords_v0.3/na_split=test')]\n",
    "test_dataloader = Bert4RecDataLoader(list_paths_test, \n",
    "                                     num_items=NUM_ITEMS, \n",
    "                                     seq_len=10,  \n",
    "                                     batch_size=64, \n",
    "                                     mask_prob=0.0, \n",
    "                                     reverse_prob=0.0,  \n",
    "                                     is_val=False,\n",
    "                                     is_test=True,\n",
    "                                     get_session=True,\n",
    "                                     shuffle=False).get_generator()\n",
    "\n",
    "list_predictions, list_sessions, list_types, list_scores = [], [], [], []\n",
    "for num_batch, batch in enumerate(tqdm(test_dataloader)):\n",
    "    features, target, session = batch\n",
    "    seq_items, seq_type, seq_time = features\n",
    "    idxs = tf.argmin(seq_items[:, :, 0], 1).numpy()\n",
    "    ###\n",
    "    for type_ in ['clicks', 'carts', 'orders']:\n",
    "        seq_type_new = [tf.concat([\n",
    "                        seq_type[i, :ix],\n",
    "                        tf.constant([[dict_map_type[type_]]], tf.int64),\n",
    "                        seq_type[i, ix+1:]], axis=0)\n",
    "                    for i, ix in enumerate(idxs)]\n",
    "        features = (seq_items, tf.stack(seq_type_new, axis=0), seq_time)\n",
    "        preds = model(features, training=False)\n",
    "        preds = tf.gather(preds, indices=idxs, axis=1, batch_dims=1)\n",
    "        topk_scores, topk_idxs = tf.math.top_k(preds, k=20)\n",
    "        topk_idxs = np.asarray([[dict_map[x] for x in topk_idxs.numpy()[i, :]] for i in range(topk_idxs.numpy().shape[0])])\n",
    "        topk_idxs = topk_idxs - 1\n",
    "        list_predictions.append(topk_idxs)\n",
    "        list_types.append([type_ for _ in range(seq_items.shape[0])])\n",
    "        list_sessions.append(session.numpy())\n",
    "    # if num_batch==100:\n",
    "    #     break\n",
    "    \n",
    "\n",
    "# 52244it [2:47:45,  5.19it/s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_submission = f\"submission_{datetime.now().__str__().split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_')}\"\n",
    "\n",
    "df_inference = pd.DataFrame({\n",
    "    'session' : np.concatenate(list_sessions),\n",
    "    'predictions' : np.concatenate(list_predictions).tolist(),\n",
    "    'type' : np.concatenate(list_types)\n",
    "})\n",
    "\n",
    "df_inference['session_type'] = df_inference['session'].astype(str) + '_' + df_inference['type']\n",
    "df_inference['labels'] = df_inference['predictions'].apply(lambda x : ' '.join([str(y) for y in x]))\n",
    "df_inference[['session_type', 'labels']].to_csv(f'../3_Submissions/{name_submission}.csv', index=False)\n",
    "\n",
    "print(df_inference.shape)\n",
    "display(\n",
    "    df_inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c4b929e2472036a63dc2b4145b104daea13432f82a7dbc65e279332da4f8b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
